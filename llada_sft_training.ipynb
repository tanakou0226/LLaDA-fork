{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sft_training_header"
   },
   "source": "# LLaDA SFT学習ノートブック\n\nこのノートブックは、LLaDA（Large Language Diffusion with mAsking）モデルのSupervised Fine-Tuning（SFT）を実装します。\n\n## 📋 実行手順（初心者向け）\n\n### 🚀 最速実行（推奨）\n```python\n# 全てのセルを順番に実行後、以下を実行：\nresult = run_complete_sft_demo()\n```\n\n### 📝 ステップバイステップ実行\n1. **セル1-8を順番に実行** - 環境設定からモデル準備まで\n2. **学習実行** - 以下のいずれかを選択：\n   - 🥇 `result = run_complete_sft_demo()` （初回推奨）\n   - ⚡ `result = quick_start_button()` （最速）\n   - 🎛️ `result = custom_training(dataset_size=1000, batch_size=2, epochs=1)` （カスタム）\n\n### ⚙️ 設定選択ガイド\n- **validation**: 1K サンプル, 1エポック（初心者・検証用）\n- **medium**: 10K サンプル, 2エポック（中級者・バランス型）\n- **production**: 50K サンプル, 3エポック（上級者・本格学習）\n\n## ✨ 特徴\n- ✅ Google Colab Pro対応（16GB GPU制限内で動作）\n- ✅ GUIDELINES.mdに基づく正確なSFT実装\n- ✅ 日本語Alpacaデータセット使用\n- ✅ ノートブック内完結（外部ツール不要）\n- ✅ ワンクリック実行機能\n- ✅ リアルタイム進捗監視\n- ✅ 自動エラー回復機能\n\n## 🔬 SFTの技術的特徴\n- **プロンプト非マスキング**: ユーザープロンプト部分はマスクしない\n- **Answer length正規化**: 回答部分の長さで損失を正規化\n- **完全文学習**: 事前学習とは異なり、完全な文章で学習\n\n## ❓ トラブルシューティング\n- メモリエラー → より小さな設定（validation）を使用\n- CUDA エラー → `torch.cuda.empty_cache()` 実行\n- データセットエラー → サンプルデータで自動フォールバック\n- 詳細なヘルプは最下部のFAQセクションを参照\n\n---",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "environment_setup"
   },
   "outputs": [],
   "source": "# 📦 環境設定とライブラリのインストール\nimport os\nimport sys\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# WandB完全無効化（API key要求を防ぐ）\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ[\"WANDB_MODE\"] = \"disabled\"\nos.environ[\"WANDB_SILENT\"] = \"true\"\n\n# Google Colab環境の検出\nIN_COLAB = 'google.colab' in sys.modules\nprint(f\"実行環境: {'Google Colab' if IN_COLAB else 'ローカル環境'}\")\n\nif IN_COLAB:\n    # Google Colab用のGPU情報表示\n    !nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv\n    \n    # 必要なパッケージのインストール\n    print(\"📦 必要なパッケージをインストール中...\")\n    !pip install -q transformers==4.49.0 accelerate==0.34.2 datasets==2.21.0\n    !pip install -q torch==2.0.1 torchvision==0.15.2\n    !pip install -q matplotlib seaborn tqdm pandas\n    \n    print(\"✅ パッケージのインストールが完了しました\")\n\n# 必要なライブラリのインポート\nprint(\"📚 ライブラリをインポート中...\")\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom datasets import load_dataset, Dataset as HFDataset\nfrom tqdm.auto import tqdm\nimport json\nimport random\nfrom typing import Dict, List, Optional, Tuple\nimport gc\nfrom dataclasses import dataclass\nimport time\nfrom datetime import datetime\nimport pandas as pd\n\n# WandB無効化確認\ntry:\n    import wandb\n    wandb.init(mode=\"disabled\")\n    print(\"🚫 WandB無効化確認済み\")\nexcept ImportError:\n    print(\"✅ WandB未インストール（問題なし）\")\nexcept Exception:\n    print(\"✅ WandB無効化済み\")\n\n# デバイス設定\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"🖥️  使用デバイス: {device}\")\nif torch.cuda.is_available():\n    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"  利用可能メモリ: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\nelse:\n    print(\"  ⚠️  GPU未検出 - CPU学習になります\")\n\n# シード固定\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\n\n# スタイル設定\nplt.style.use('default')  # シンプルなスタイル\nsns.set_palette(\"husl\")   # 見やすい色合い\n\nprint(\"✅ 環境設定が完了しました\")\nprint(\"🚫 WandB完全無効化済み - API key要求なし\")\nprint(\"=\" * 50)\nprint(\"🎯 次のステップ: セル2を実行して学習設定を選択\")\nprint(\"=\" * 50)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_configurations"
   },
   "outputs": [],
   "source": [
    "# 学習設定の定義\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"学習設定を管理するクラス\"\"\"\n",
    "    \n",
    "    # モデル設定\n",
    "    model_name: str = \"GSAI-ML/LLaDA-8B-Base\"\n",
    "    max_length: int = 1024\n",
    "    \n",
    "    # データ設定\n",
    "    dataset_size: int = 1000  # 使用するデータ数\n",
    "    validation_split: float = 0.1\n",
    "    \n",
    "    # 学習設定\n",
    "    batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 8\n",
    "    num_epochs: int = 1\n",
    "    learning_rate: float = 2e-5\n",
    "    warmup_steps: int = 100\n",
    "    \n",
    "    # SFT特有設定\n",
    "    mask_id: int = 126336  # [MASK]トークンID\n",
    "    eps: float = 1e-3  # マスキング確率の最小値\n",
    "    \n",
    "    # メモリ最適化\n",
    "    fp16: bool = True\n",
    "    gradient_checkpointing: bool = True\n",
    "    dataloader_num_workers: int = 2\n",
    "    \n",
    "    # 保存・評価設定\n",
    "    save_steps: int = 500\n",
    "    eval_steps: int = 250\n",
    "    logging_steps: int = 50\n",
    "    output_dir: str = \"./llada_sft_output\"\n",
    "\n",
    "# 事前定義された設定\n",
    "CONFIGS = {\n",
    "    \"validation\": TrainingConfig(\n",
    "        dataset_size=1000,\n",
    "        batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_epochs=1,\n",
    "        learning_rate=5e-5,\n",
    "        save_steps=200,\n",
    "        eval_steps=100\n",
    "    ),\n",
    "    \n",
    "    \"production\": TrainingConfig(\n",
    "        dataset_size=50000,  # 本格的なデータサイズ\n",
    "        batch_size=1,        # メモリ制限対応\n",
    "        gradient_accumulation_steps=16,\n",
    "        num_epochs=3,\n",
    "        learning_rate=2e-5,\n",
    "        warmup_steps=500,\n",
    "        save_steps=1000,\n",
    "        eval_steps=500\n",
    "    ),\n",
    "    \n",
    "    \"medium\": TrainingConfig(\n",
    "        dataset_size=10000,\n",
    "        batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        num_epochs=2,\n",
    "        learning_rate=3e-5,\n",
    "        save_steps=500,\n",
    "        eval_steps=250\n",
    "    )\n",
    "}\n",
    "\n",
    "def select_config(config_name: str = \"validation\") -> TrainingConfig:\n",
    "    \"\"\"設定を選択する関数\"\"\"\n",
    "    if config_name not in CONFIGS:\n",
    "        print(f\"警告: '{config_name}'は無効な設定です。利用可能: {list(CONFIGS.keys())}\")\n",
    "        config_name = \"validation\"\n",
    "    \n",
    "    config = CONFIGS[config_name]\n",
    "    print(f\"✅ '{config_name}' 設定を選択しました\")\n",
    "    print(f\"  - データサイズ: {config.dataset_size:,}\")\n",
    "    print(f\"  - バッチサイズ: {config.batch_size}\")\n",
    "    print(f\"  - エポック数: {config.num_epochs}\")\n",
    "    print(f\"  - 学習率: {config.learning_rate}\")\n",
    "    \n",
    "    # GPU メモリ使用量の推定\n",
    "    if torch.cuda.is_available():\n",
    "        estimated_memory = (\n",
    "            8 * config.batch_size * config.max_length * 4 / 1e9 +  # モデルパラメータ\n",
    "            config.batch_size * config.max_length * 4 * 2 / 1e9    # アクティベーション\n",
    "        )\n",
    "        available_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"  - 推定GPU使用量: {estimated_memory:.1f} GB / {available_memory:.1f} GB\")\n",
    "        \n",
    "        if estimated_memory > available_memory * 0.9:\n",
    "            print(\"  ⚠️  メモリ不足の可能性があります。より小さな設定を検討してください。\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "# 使用する設定を選択（validation/medium/production）\n",
    "CONFIG_NAME = \"validation\"  # ここを変更して設定を選択\n",
    "config = select_config(CONFIG_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alpaca_dataset_loading"
   },
   "outputs": [],
   "source": [
    "# Alpacaデータセットの読み込みと前処理\n",
    "class AlpacaDatasetProcessor:\n",
    "    \"\"\"日本語Alpacaデータセットの処理クラス\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, config: TrainingConfig):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        self.special_tokens = {\n",
    "            'bos': tokenizer.bos_token or '<s>',\n",
    "            'eos': tokenizer.eos_token or '</s>',\n",
    "            'start_id': '<start_id>',\n",
    "            'end_id': '<end_id>',\n",
    "            'eot_id': '<eot_id>'\n",
    "        }\n",
    "    \n",
    "    def load_dataset(self) -> List[Dict]:\n",
    "        \"\"\"日本語Alpacaデータセットを読み込む\"\"\"\n",
    "        try:\n",
    "            print(\"📥 日本語Alpacaデータセットを読み込み中...\")\n",
    "            \n",
    "            # データセットの読み込み（複数のソースを試行）\n",
    "            dataset_sources = [\n",
    "                \"kunishou/databricks-dolly-15k-ja\",\n",
    "                \"izumi-lab/llm-japanese-dataset\",\n",
    "                \"elyza/ELYZA-tasks-100\"\n",
    "            ]\n",
    "            \n",
    "            dataset = None\n",
    "            for source in dataset_sources:\n",
    "                try:\n",
    "                    dataset = load_dataset(source, split='train')\n",
    "                    print(f\"✅ データセット '{source}' の読み込みに成功\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"  '{source}' の読み込みに失敗: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if dataset is None:\n",
    "                # フォールバック: サンプルデータを生成\n",
    "                print(\"⚠️  データセットの読み込みに失敗。サンプルデータを使用します。\")\n",
    "                return self._create_sample_data()\n",
    "            \n",
    "            # データを標準形式に変換\n",
    "            processed_data = self._process_dataset(dataset)\n",
    "            \n",
    "            # データサイズを制限\n",
    "            if len(processed_data) > self.config.dataset_size:\n",
    "                processed_data = processed_data[:self.config.dataset_size]\n",
    "            \n",
    "            print(f\"✅ {len(processed_data):,} サンプルを準備しました\")\n",
    "            return processed_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ データセット読み込みエラー: {e}\")\n",
    "            print(\"サンプルデータを使用します\")\n",
    "            return self._create_sample_data()\n",
    "    \n",
    "    def _process_dataset(self, dataset) -> List[Dict]:\n",
    "        \"\"\"データセットを標準形式に変換\"\"\"\n",
    "        processed = []\n",
    "        \n",
    "        for item in tqdm(dataset, desc=\"データ変換中\"):\n",
    "            # データセット形式に応じて適応\n",
    "            if 'instruction' in item and 'output' in item:\n",
    "                instruction = item['instruction']\n",
    "                if 'input' in item and item['input']:\n",
    "                    instruction += f\"\\n{item['input']}\"\n",
    "                response = item['output']\n",
    "            elif 'input' in item and 'output' in item:\n",
    "                instruction = item['input']\n",
    "                response = item['output']\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            if instruction and response:\n",
    "                processed.append({\n",
    "                    'instruction': instruction.strip(),\n",
    "                    'response': response.strip()\n",
    "                })\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def _create_sample_data(self) -> List[Dict]:\n",
    "        \"\"\"サンプルデータを生成\"\"\"\n",
    "        sample_data = [\n",
    "            {\n",
    "                'instruction': '日本の首都はどこですか？',\n",
    "                'response': '日本の首都は東京です。東京は関東地方に位置し、日本の政治・経済・文化の中心地です。'\n",
    "            },\n",
    "            {\n",
    "                'instruction': 'Pythonでリストを逆順にする方法を教えてください。',\n",
    "                'response': 'Pythonでリストを逆順にする方法はいくつかあります。最も簡単な方法は、reverse()メソッドを使うことです：my_list.reverse()。また、スライスを使って新しいリストを作ることもできます：new_list = my_list[::-1]。'\n",
    "            },\n",
    "            {\n",
    "                'instruction': '機械学習とは何ですか？',\n",
    "                'response': '機械学習は、コンピュータがデータから自動的にパターンを学習し、新しいデータに対して予測や判断を行う技術です。従来のプログラミングとは異なり、明示的にルールを書く代わりに、大量のデータから規則性を見つけ出します。'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # サンプルデータを指定サイズまで繰り返し\n",
    "        repeated_data = []\n",
    "        for i in range(self.config.dataset_size):\n",
    "            repeated_data.append(sample_data[i % len(sample_data)])\n",
    "        \n",
    "        return repeated_data\n",
    "    \n",
    "    def format_for_sft(self, instruction: str, response: str) -> Tuple[str, int]:\n",
    "        \"\"\"SFT用にデータをフォーマット（GUIDELINES.mdに基づく）\"\"\"\n",
    "        # フォーマット: <BOS><start_id>user<end_id>\\n{instruction}<eot_id><start_id>assistant<end_id>\\n{response}<EOS>\n",
    "        prompt_part = (\n",
    "            f\"{self.special_tokens['bos']}\"\n",
    "            f\"{self.special_tokens['start_id']}user{self.special_tokens['end_id']}\\n\"\n",
    "            f\"{instruction}\"\n",
    "            f\"{self.special_tokens['eot_id']}\"\n",
    "            f\"{self.special_tokens['start_id']}assistant{self.special_tokens['end_id']}\\n\"\n",
    "        )\n",
    "        \n",
    "        full_text = prompt_part + response + self.special_tokens['eos']\n",
    "        \n",
    "        # プロンプト長を計算（response部分を除く）\n",
    "        prompt_tokens = self.tokenizer.encode(prompt_part, add_special_tokens=False)\n",
    "        prompt_length = len(prompt_tokens)\n",
    "        \n",
    "        return full_text, prompt_length\n",
    "\n",
    "# データセットプロセッサの初期化\n",
    "print(\"🔧 トークナイザーを読み込み中...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name, trust_remote_code=True)\n",
    "\n",
    "# パディングトークンの設定\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"✅ トークナイザー読み込み完了 (語彙サイズ: {len(tokenizer):,})\")\n",
    "\n",
    "# データセットの処理\n",
    "processor = AlpacaDatasetProcessor(tokenizer, config)\n",
    "raw_data = processor.load_dataset()\n",
    "\n",
    "print(f\"📊 データセット情報:\")\n",
    "print(f\"  - 総サンプル数: {len(raw_data):,}\")\n",
    "if raw_data:\n",
    "    sample = raw_data[0]\n",
    "    formatted_text, prompt_len = processor.format_for_sft(sample['instruction'], sample['response'])\n",
    "    print(f\"  - サンプル長: {len(formatted_text)} 文字\")\n",
    "    print(f\"  - プロンプト長: {prompt_len} トークン\")\n",
    "    print(f\"\\n📝 フォーマット例:\")\n",
    "    print(f\"  指示: {sample['instruction'][:50]}...\")\n",
    "    print(f\"  応答: {sample['response'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sft_dataset_class"
   },
   "outputs": [],
   "source": [
    "# SFT用データセットクラス\n",
    "class LLaDASFTDataset(Dataset):\n",
    "    \"\"\"LLaDA SFT学習用データセットクラス\"\"\"\n",
    "    \n",
    "    def __init__(self, data: List[Dict], processor: AlpacaDatasetProcessor, config: TrainingConfig):\n",
    "        self.data = data\n",
    "        self.processor = processor\n",
    "        self.config = config\n",
    "        self.tokenizer = processor.tokenizer\n",
    "        \n",
    "        # データを事前に処理\n",
    "        self.processed_data = self._preprocess_data()\n",
    "        \n",
    "        print(f\"✅ SFTデータセットを準備しました ({len(self.processed_data)} サンプル)\")\n",
    "    \n",
    "    def _preprocess_data(self) -> List[Dict]:\n",
    "        \"\"\"全データを事前に処理\"\"\"\n",
    "        processed = []\n",
    "        \n",
    "        for item in tqdm(self.data, desc=\"SFTデータ前処理中\"):\n",
    "            try:\n",
    "                # フォーマット\n",
    "                formatted_text, prompt_length = self.processor.format_for_sft(\n",
    "                    item['instruction'], \n",
    "                    item['response']\n",
    "                )\n",
    "                \n",
    "                # トークン化\n",
    "                tokens = self.tokenizer.encode(\n",
    "                    formatted_text,\n",
    "                    add_special_tokens=False,\n",
    "                    max_length=self.config.max_length,\n",
    "                    truncation=True\n",
    "                )\n",
    "                \n",
    "                # 十分な長さがある場合のみ使用\n",
    "                if len(tokens) > prompt_length + 5:  # 最低5トークンの応答が必要\n",
    "                    processed.append({\n",
    "                        'input_ids': tokens,\n",
    "                        'prompt_length': prompt_length,\n",
    "                        'original_instruction': item['instruction'],\n",
    "                        'original_response': item['response']\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"データ処理エラー (スキップ): {e}\")\n",
    "                continue\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.processed_data)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        item = self.processed_data[idx]\n",
    "        \n",
    "        # パディング\n",
    "        input_ids = item['input_ids'][:self.config.max_length]\n",
    "        if len(input_ids) < self.config.max_length:\n",
    "            pad_length = self.config.max_length - len(input_ids)\n",
    "            input_ids.extend([self.tokenizer.pad_token_id] * pad_length)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'prompt_length': torch.tensor(item['prompt_length'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(\n",
    "                [1 if token_id != self.tokenizer.pad_token_id else 0 for token_id in input_ids],\n",
    "                dtype=torch.long\n",
    "            )\n",
    "        }\n",
    "\n",
    "# データセットの分割と作成\n",
    "def create_datasets(data: List[Dict], processor: AlpacaDatasetProcessor, config: TrainingConfig):\n",
    "    \"\"\"学習・検証データセットを作成\"\"\"\n",
    "    # データをシャッフル\n",
    "    random.shuffle(data)\n",
    "    \n",
    "    # 分割\n",
    "    split_idx = int(len(data) * (1 - config.validation_split))\n",
    "    train_data = data[:split_idx]\n",
    "    val_data = data[split_idx:]\n",
    "    \n",
    "    print(f\"📊 データ分割:\")\n",
    "    print(f\"  - 学習データ: {len(train_data):,} サンプル\")\n",
    "    print(f\"  - 検証データ: {len(val_data):,} サンプル\")\n",
    "    \n",
    "    # データセット作成\n",
    "    train_dataset = LLaDASFTDataset(train_data, processor, config)\n",
    "    val_dataset = LLaDASFTDataset(val_data, processor, config) if val_data else None\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "# データセットの作成\n",
    "train_dataset, val_dataset = create_datasets(raw_data, processor, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sft_forward_process"
   },
   "outputs": [],
   "source": [
    "# SFT用Forward Process実装（GUIDELINES.mdに基づく）\n",
    "class SFTForwardProcess:\n",
    "    \"\"\"SFT用のマスキング・損失計算クラス\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.mask_id = config.mask_id\n",
    "        self.eps = config.eps\n",
    "    \n",
    "    def forward_process(self, input_ids: torch.Tensor, prompt_lengths: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        SFT用フォワードプロセス（GUIDELINES.md準拠）\n",
    "        \n",
    "        Args:\n",
    "            input_ids: [batch_size, seq_len] 入力トークンID\n",
    "            prompt_lengths: [batch_size] 各サンプルのプロンプト長\n",
    "        \n",
    "        Returns:\n",
    "            noisy_batch: マスクされた入力\n",
    "            masked_indices: マスクされた位置\n",
    "            p_mask: マスキング確率\n",
    "        \"\"\"\n",
    "        b, l = input_ids.shape\n",
    "        device = input_ids.device\n",
    "        \n",
    "        # ランダムマスキング確率の生成\n",
    "        t = torch.rand(b, device=device)\n",
    "        p_mask = (1 - self.eps) * t + self.eps\n",
    "        p_mask = p_mask[:, None].repeat(1, l)\n",
    "        \n",
    "        # 初期マスキング\n",
    "        masked_indices = torch.rand((b, l), device=device) < p_mask\n",
    "        noisy_batch = torch.where(masked_indices, self.mask_id, input_ids)\n",
    "        \n",
    "        # プロンプト部分のマスクを解除（SFTの核心部分）\n",
    "        token_positions = torch.arange(l, device=device).expand(b, l)\n",
    "        prompt_mask = token_positions < prompt_lengths.unsqueeze(1)\n",
    "        \n",
    "        # プロンプト部分は元のトークンを保持\n",
    "        noisy_batch[prompt_mask] = input_ids[prompt_mask]\n",
    "        \n",
    "        # マスクインデックスを更新（プロンプト部分はマスクされていない）\n",
    "        masked_indices = (noisy_batch == self.mask_id)\n",
    "        \n",
    "        return noisy_batch, masked_indices, p_mask\n",
    "    \n",
    "    def compute_sft_loss(self, \n",
    "                        logits: torch.Tensor,\n",
    "                        input_ids: torch.Tensor,\n",
    "                        masked_indices: torch.Tensor,\n",
    "                        p_mask: torch.Tensor,\n",
    "                        prompt_lengths: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        SFT損失の計算（GUIDELINES.md準拠）\n",
    "        \n",
    "        Args:\n",
    "            logits: [batch_size, seq_len, vocab_size] モデル出力\n",
    "            input_ids: [batch_size, seq_len] 正解トークン\n",
    "            masked_indices: [batch_size, seq_len] マスクされた位置\n",
    "            p_mask: [batch_size, seq_len] マスキング確率\n",
    "            prompt_lengths: [batch_size] プロンプト長\n",
    "        \n",
    "        Returns:\n",
    "            loss: SFT損失\n",
    "        \"\"\"\n",
    "        b, l = input_ids.shape\n",
    "        device = input_ids.device\n",
    "        \n",
    "        # Answer length計算（プロンプト以外の部分）\n",
    "        prompt_mask = torch.arange(l, device=device).expand(b, l) < prompt_lengths.unsqueeze(1)\n",
    "        prompt_mask = prompt_mask.to(torch.int64)\n",
    "        answer_lengths = torch.sum((1 - prompt_mask), dim=-1, keepdim=True)\n",
    "        answer_lengths = answer_lengths.repeat(1, l)\n",
    "        \n",
    "        # マスクされた位置でのみ損失を計算\n",
    "        if not masked_indices.any():\n",
    "            return torch.tensor(0.0, device=device, requires_grad=True)\n",
    "        \n",
    "        # クロスエントロピー損失（重み付き）\n",
    "        token_loss = F.cross_entropy(\n",
    "            logits[masked_indices], \n",
    "            input_ids[masked_indices], \n",
    "            reduction='none'\n",
    "        ) / p_mask[masked_indices]\n",
    "        \n",
    "        # Answer length正規化\n",
    "        normalized_loss = token_loss / (answer_lengths[masked_indices] + 1e-8)\n",
    "        \n",
    "        # バッチ平均\n",
    "        ce_loss = torch.sum(normalized_loss) / b\n",
    "        \n",
    "        return ce_loss\n",
    "\n",
    "# SFT Forward Processの初期化\n",
    "sft_forward = SFTForwardProcess(config)\n",
    "print(\"✅ SFT Forward Processを初期化しました\")\n",
    "\n",
    "# テスト実行\n",
    "if len(train_dataset) > 0:\n",
    "    test_batch = train_dataset[0]\n",
    "    test_input_ids = test_batch['input_ids'].unsqueeze(0)\n",
    "    test_prompt_length = test_batch['prompt_length'].unsqueeze(0)\n",
    "    \n",
    "    print(f\"\\n🧪 SFTプロセステスト:\")\n",
    "    print(f\"  - 入力形状: {test_input_ids.shape}\")\n",
    "    print(f\"  - プロンプト長: {test_prompt_length.item()}\")\n",
    "    \n",
    "    # Forward process実行\n",
    "    noisy_batch, masked_indices, p_mask = sft_forward.forward_process(\n",
    "        test_input_ids, test_prompt_length\n",
    "    )\n",
    "    \n",
    "    print(f\"  - マスクされたトークン数: {masked_indices.sum().item()}\")\n",
    "    print(f\"  - プロンプト部分のマスク数: {masked_indices[0, :test_prompt_length.item()].sum().item()} (0であるべき)\")\n",
    "    print(f\"  - 応答部分のマスク数: {masked_indices[0, test_prompt_length.item():].sum().item()}\")\n",
    "    \n",
    "    if masked_indices[0, :test_prompt_length.item()].sum().item() == 0:\n",
    "        print(\"  ✅ プロンプト非マスキングが正常に動作しています\")\n",
    "    else:\n",
    "        print(\"  ❌ プロンプト部分がマスクされています（エラー）\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "model_and_trainer_setup"
   },
   "outputs": [],
   "source": "# 🤖 LLaDAモデルとSFTトレーナーの設定\nclass LLaDASFTTrainer(Trainer):\n    \"\"\"LLaDA SFT専用トレーナークラス\"\"\"\n    \n    def __init__(self, sft_forward: SFTForwardProcess, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.sft_forward = sft_forward\n        self.vocab_size = len(self.tokenizer)\n    \n    def compute_loss(self, model, inputs, return_outputs=False):\n        \"\"\"SFT損失の計算\"\"\"\n        input_ids = inputs['input_ids']\n        prompt_lengths = inputs['prompt_length']\n        \n        # 安全性チェック\n        input_ids = torch.clamp(input_ids, 0, self.vocab_size - 1)\n        \n        try:\n            # SFTフォワードプロセス\n            noisy_batch, masked_indices, p_mask = self.sft_forward.forward_process(\n                input_ids, prompt_lengths\n            )\n            \n            # モデル推論\n            outputs = model(input_ids=noisy_batch)\n            logits = outputs.logits\n            \n            # SFT損失計算\n            loss = self.sft_forward.compute_sft_loss(\n                logits, input_ids, masked_indices, p_mask, prompt_lengths\n            )\n            \n            # NaN/Inf チェック\n            if torch.isnan(loss) or torch.isinf(loss):\n                print(f\"警告: 異常な損失値 {loss.item()}, ゼロ損失で代替\")\n                loss = torch.tensor(0.0, device=loss.device, requires_grad=True)\n            \n            return (loss, outputs) if return_outputs else loss\n            \n        except Exception as e:\n            print(f\"損失計算エラー: {e}\")\n            # フォールバック損失\n            fallback_loss = torch.tensor(1.0, device=input_ids.device, requires_grad=True)\n            return fallback_loss\n\n# モデルの読み込み\nprint(\"🔧 LLaDAモデルを読み込み中...\")\ntry:\n    model = AutoModelForCausalLM.from_pretrained(\n        config.model_name,\n        trust_remote_code=True,\n        torch_dtype=torch.bfloat16 if config.fp16 else torch.float32,\n        device_map='auto' if torch.cuda.is_available() else None\n    )\n    \n    # グラディエントチェックポイントの設定\n    if config.gradient_checkpointing:\n        try:\n            model.gradient_checkpointing_enable()\n            print(\"  ✅ グラディエントチェックポイントを有効化\")\n        except Exception as e:\n            print(f\"  ⚠️  グラディエントチェックポイント設定エラー: {e}\")\n    \n    print(f\"✅ モデル読み込み完了\")\n    print(f\"  - パラメータ数: {sum(p.numel() for p in model.parameters()):,}\")\n    print(f\"  - 学習可能パラメータ数: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n    \nexcept Exception as e:\n    print(f\"❌ モデル読み込みエラー: {e}\")\n    raise\n\n# データコレーターの設定\ndef sft_data_collator(features):\n    \"\"\"SFT用データコレーター\"\"\"\n    batch = {}\n    # バッチの作成\n    for key in features[0].keys():\n        batch[key] = torch.stack([f[key] for f in features])\n    return batch\n\n# 学習引数の設定（WandB完全無効化）\ntraining_args = TrainingArguments(\n    output_dir=config.output_dir,\n    num_train_epochs=config.num_epochs,\n    per_device_train_batch_size=config.batch_size,\n    per_device_eval_batch_size=config.batch_size,\n    gradient_accumulation_steps=config.gradient_accumulation_steps,\n    learning_rate=config.learning_rate,\n    warmup_steps=config.warmup_steps,\n    logging_steps=config.logging_steps,\n    save_steps=config.save_steps,\n    eval_steps=config.eval_steps,\n    evaluation_strategy=\"steps\" if val_dataset else \"no\",\n    save_strategy=\"steps\",\n    fp16=config.fp16,\n    dataloader_num_workers=config.dataloader_num_workers,\n    remove_unused_columns=False,\n    # WandB完全無効化の複数設定\n    report_to=[],  # 空リストで完全無効化\n    logging_first_step=True,\n    disable_tqdm=False,  # プログレスバーは表示\n    load_best_model_at_end=True if val_dataset else False,\n    metric_for_best_model=\"eval_loss\" if val_dataset else None,\n    greater_is_better=False,\n    save_total_limit=2,\n    # 追加のWandB無効化設定\n    run_name=None,\n    logging_strategy=\"steps\",\n    log_level=\"error\"  # ログレベルを最小限に\n)\n\n# トレーナーの初期化\ntrainer = LLaDASFTTrainer(\n    sft_forward=sft_forward,\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=sft_data_collator,\n    tokenizer=tokenizer\n)\n\nprint(\"✅ SFTトレーナーを初期化しました\")\nprint(f\"📊 学習設定:\")\nprint(f\"  - 実効バッチサイズ: {config.batch_size * config.gradient_accumulation_steps}\")\nprint(f\"  - 総ステップ数: {len(train_dataset) // (config.batch_size * config.gradient_accumulation_steps) * config.num_epochs}\")\nprint(f\"  - 学習率: {config.learning_rate}\")\nprint(f\"  - FP16: {config.fp16}\")\nprint(f\"🚫 外部ログ: 完全無効化済み（WandB API key不要）\")\nprint(\"=\" * 50)\nprint(\"🎯 次のステップ: セル7-8を実行して学習機能を準備\")\nprint(\"=\" * 50)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sft_training_execution"
   },
   "outputs": [],
   "source": [
    "# SFT学習の実行\n",
    "def run_sft_training():\n",
    "    \"\"\"SFT学習の実行関数\"\"\"\n",
    "    print(\"🚀 SFT学習を開始します...\")\n",
    "    print(f\"📊 学習統計:\")\n",
    "    print(f\"  - 学習サンプル数: {len(train_dataset):,}\")\n",
    "    print(f\"  - 検証サンプル数: {len(val_dataset) if val_dataset else 0:,}\")\n",
    "    print(f\"  - エポック数: {config.num_epochs}\")\n",
    "    print(f\"  - 使用設定: {CONFIG_NAME}\")\n",
    "    \n",
    "    # GPU メモリのクリア\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(f\"  - GPU使用メモリ: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
    "    \n",
    "    try:\n",
    "        # 学習前の評価（オプション）\n",
    "        if val_dataset and len(val_dataset) > 0:\n",
    "            print(\"\\n📊 学習前評価を実行中...\")\n",
    "            try:\n",
    "                initial_eval = trainer.evaluate()\n",
    "                print(f\"  初期損失: {initial_eval['eval_loss']:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  初期評価エラー: {e}\")\n",
    "        \n",
    "        # メイン学習ループ\n",
    "        print(\"\\n🎯 メイン学習を開始...\")\n",
    "        training_result = trainer.train()\n",
    "        \n",
    "        print(\"\\n✅ 学習が完了しました！\")\n",
    "        print(f\"📊 学習結果:\")\n",
    "        print(f\"  - 最終損失: {training_result.training_loss:.4f}\")\n",
    "        print(f\"  - 学習時間: {training_result.metrics['train_runtime']:.2f} 秒\")\n",
    "        print(f\"  - サンプル/秒: {training_result.metrics['train_samples_per_second']:.2f}\")\n",
    "        \n",
    "        # 学習後の評価\n",
    "        if val_dataset and len(val_dataset) > 0:\n",
    "            print(\"\\n📊 最終評価を実行中...\")\n",
    "            try:\n",
    "                final_eval = trainer.evaluate()\n",
    "                print(f\"  最終検証損失: {final_eval['eval_loss']:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  最終評価エラー: {e}\")\n",
    "        \n",
    "        # モデルの保存\n",
    "        print(\"\\n💾 モデルを保存中...\")\n",
    "        try:\n",
    "            trainer.save_model()\n",
    "            tokenizer.save_pretrained(config.output_dir)\n",
    "            print(f\"  ✅ モデルを {config.output_dir} に保存しました\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️  モデル保存エラー: {e}\")\n",
    "        \n",
    "        return training_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ 学習エラー: {e}\")\n",
    "        print(\"\\n🔧 エラー対処法:\")\n",
    "        print(\"  1. より小さなバッチサイズを試す\")\n",
    "        print(\"  2. より短い最大長を設定する\")\n",
    "        print(\"  3. validation設定を使用する\")\n",
    "        print(\"  4. FP16を有効にする\")\n",
    "        \n",
    "        # メモリ情報表示\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"\\n📊 GPU情報:\")\n",
    "            print(f\"  - 使用メモリ: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
    "            print(f\"  - 最大メモリ: {torch.cuda.max_memory_allocated() / 1e9:.1f} GB\")\n",
    "            print(f\"  - 利用可能メモリ: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        \n",
    "        raise\n",
    "\n",
    "# 学習実行の確認\n",
    "print(\"\\n🎯 学習準備完了\")\n",
    "print(f\"現在の設定: {CONFIG_NAME}\")\n",
    "print(\"\\n実行するには下記を実行してください:\")\n",
    "print(\"training_result = run_sft_training()\")\n",
    "\n",
    "# 設定変更の案内\n",
    "print(\"\\n⚙️  設定変更方法:\")\n",
    "print(\"異なる設定で実行したい場合は、上のセルで CONFIG_NAME を変更してください:\")\n",
    "print(\"  - 'validation': 検証用 (1K サンプル, 1エポック)\")\n",
    "print(\"  - 'medium': 中規模 (10K サンプル, 2エポック)\")\n",
    "print(\"  - 'production': 本番用 (50K サンプル, 3エポック)\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# 🚀 ワンクリック学習実行セル\nimport time\nfrom IPython.display import display, HTML, clear_output\nimport threading\n\nclass SFTTrainingRunner:\n    \"\"\"SFT学習のワンクリック実行クラス\"\"\"\n    \n    def __init__(self):\n        self.is_training = False\n        self.training_thread = None\n        self.training_logs = []\n        self.current_step = 0\n        self.total_steps = 0\n        self.start_time = None\n        \n    def quick_start_training(self, config_name=\"validation\", auto_evaluate=True):\n        \"\"\"\n        ワンクリック学習実行\n        \n        Args:\n            config_name: 使用する設定 (\"validation\", \"medium\", \"production\")\n            auto_evaluate: 学習後に自動評価するかどうか\n        \"\"\"\n        print(\"🚀 ワンクリック学習実行を開始します...\")\n        \n        # 設定の再選択\n        global config, CONFIG_NAME\n        CONFIG_NAME = config_name\n        config = select_config(config_name)\n        \n        # 学習統計の表示\n        print(f\"\\n📊 学習設定: {config_name}\")\n        print(f\"  ├─ データサイズ: {config.dataset_size:,} サンプル\")\n        print(f\"  ├─ バッチサイズ: {config.batch_size}\")\n        print(f\"  ├─ エポック数: {config.num_epochs}\")\n        print(f\"  ├─ 学習率: {config.learning_rate}\")\n        print(f\"  └─ 推定時間: {self._estimate_training_time()} 分\")\n        \n        # GPU メモリチェック\n        if torch.cuda.is_available():\n            self._check_gpu_memory()\n        \n        # 学習実行\n        try:\n            print(\"\\n⏰ 5秒後に学習を開始します...\")\n            time.sleep(5)\n            \n            # 学習実行\n            self.start_time = time.time()\n            print(\"🎯 学習開始！\")\n            training_result = run_sft_training()\n            \n            # 学習時間の計算\n            training_duration = time.time() - self.start_time\n            print(f\"\\n⏱️  総学習時間: {training_duration/60:.1f} 分\")\n            \n            # 自動評価\n            if auto_evaluate:\n                print(\"\\n🔍 自動評価を開始...\")\n                time.sleep(2)\n                evaluate_sft_model()\n                memory_usage_summary()\n            \n            # 成功メッセージ\n            print(\"\\n🎉 学習が正常に完了しました！\")\n            self._display_success_summary(training_result)\n            \n            return training_result\n            \n        except Exception as e:\n            print(f\"\\n❌ 学習エラー: {e}\")\n            self._display_error_troubleshooting()\n            raise\n    \n    def _estimate_training_time(self):\n        \"\"\"学習時間を推定\"\"\"\n        # 基本的な推定（経験値ベース）\n        base_time_per_sample = 0.1  # 秒/サンプル\n        total_samples = config.dataset_size * config.num_epochs\n        estimated_seconds = total_samples * base_time_per_sample / config.batch_size\n        return max(1, int(estimated_seconds / 60))\n    \n    def _check_gpu_memory(self):\n        \"\"\"GPU メモリをチェック\"\"\"\n        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n        current_memory = torch.cuda.memory_allocated() / 1e9\n        \n        estimated_usage = 8 + config.batch_size * 2  # GB\n        \n        print(f\"\\n🖥️  GPU メモリチェック:\")\n        print(f\"  ├─ 総容量: {total_memory:.1f} GB\")\n        print(f\"  ├─ 現在使用量: {current_memory:.1f} GB\")\n        print(f\"  ├─ 推定必要量: {estimated_usage:.1f} GB\")\n        \n        if estimated_usage > total_memory * 0.9:\n            print(f\"  └─ ⚠️  メモリ不足の可能性 → より小さな設定を推奨\")\n        else:\n            print(f\"  └─ ✅ メモリ容量OK\")\n    \n    def _display_success_summary(self, training_result):\n        \"\"\"成功時のサマリー表示\"\"\"\n        print(\"\\\\n\" + \"=\"*50)\n        print(\"🎊 SFT学習完了サマリー\")\n        print(\"=\"*50)\n        print(f\"✅ 設定: {CONFIG_NAME}\")\n        print(f\"✅ 最終損失: {training_result.training_loss:.4f}\")\n        print(f\"✅ 学習時間: {training_result.metrics['train_runtime']:.1f} 秒\")\n        print(f\"✅ モデル保存先: {config.output_dir}\")\n        print(\"=\"*50)\n    \n    def _display_error_troubleshooting(self):\n        \"\"\"エラー時のトラブルシューティング\"\"\"\n        print(\"\\\\n\" + \"=\"*50)\n        print(\"🔧 トラブルシューティング\")\n        print(\"=\"*50)\n        print(\"💡 以下を試してください:\")\n        print(\"  1️⃣  より小さな設定を使用: quick_start_training('validation')\")\n        print(\"  2️⃣  バッチサイズを小さく: config.batch_size = 1\")\n        print(\"  3️⃣  最大長を短く: config.max_length = 512\")\n        print(\"  4️⃣  GPU メモリをクリア: torch.cuda.empty_cache()\")\n        print(\"=\"*50)\n\n# SFTTrainingRunnerのインスタンス化\ntrainer_runner = SFTTrainingRunner()\n\n# ワンクリック実行ボタン\nprint(\"🎯 ワンクリック学習実行の準備完了\")\nprint(\"\\\\n\" + \"=\"*60)\nprint(\"🚀 以下のコマンドで即座に学習を開始できます:\")\nprint(\"=\"*60)\nprint()\nprint(\"💡 検証用設定（推奨・初回）:\")\nprint(\"   trainer_runner.quick_start_training('validation')\")\nprint()\nprint(\"💪 中規模設定:\")\nprint(\"   trainer_runner.quick_start_training('medium')\")\nprint()\nprint(\"🔥 本格的設定:\")\nprint(\"   trainer_runner.quick_start_training('production')\")\nprint()\nprint(\"=\"*60)\nprint(\"⚙️  オプション:\")\nprint(\"   - auto_evaluate=False で評価をスキップ\")\nprint(\"   - 例: trainer_runner.quick_start_training('validation', auto_evaluate=False)\")\nprint(\"=\"*60)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 📊 インタラクティブ学習進捗モニタリング\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom datetime import datetime\nimport pandas as pd\n\nclass TrainingMonitor:\n    \"\"\"学習進捗をリアルタイムで監視するクラス\"\"\"\n    \n    def __init__(self):\n        self.training_history = {\n            'step': [],\n            'loss': [],\n            'eval_loss': [],\n            'learning_rate': [],\n            'timestamp': [],\n            'gpu_memory': [],\n            'samples_per_second': []\n        }\n        self.fig = None\n        self.axes = None\n        \n    def start_monitoring(self):\n        \"\"\"モニタリングを開始\"\"\"\n        plt.style.use('seaborn-v0_8')\n        self.fig, self.axes = plt.subplots(2, 2, figsize=(15, 10))\n        self.fig.suptitle('🔄 LLaDA SFT学習リアルタイムモニタリング', fontsize=16, fontweight='bold')\n        \n        # 各グラフの初期設定\n        self.axes[0, 0].set_title('📉 学習損失')\n        self.axes[0, 0].set_xlabel('ステップ')\n        self.axes[0, 0].set_ylabel('損失')\n        self.axes[0, 0].grid(True, alpha=0.3)\n        \n        self.axes[0, 1].set_title('🖥️ GPU メモリ使用量')\n        self.axes[0, 1].set_xlabel('ステップ')\n        self.axes[0, 1].set_ylabel('メモリ (GB)')\n        self.axes[0, 1].grid(True, alpha=0.3)\n        \n        self.axes[1, 0].set_title('⚡ 学習速度')\n        self.axes[1, 0].set_xlabel('ステップ')\n        self.axes[1, 0].set_ylabel('サンプル/秒')\n        self.axes[1, 0].grid(True, alpha=0.3)\n        \n        self.axes[1, 1].set_title('📊 学習率')\n        self.axes[1, 1].set_xlabel('ステップ')\n        self.axes[1, 1].set_ylabel('学習率')\n        self.axes[1, 1].grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        return self.fig\n    \n    def update_metrics(self, step, loss=None, eval_loss=None, learning_rate=None, samples_per_second=None):\n        \"\"\"メトリクスを更新\"\"\"\n        self.training_history['step'].append(step)\n        self.training_history['timestamp'].append(datetime.now())\n        \n        # GPU メモリ使用量を取得\n        if torch.cuda.is_available():\n            gpu_memory = torch.cuda.memory_allocated() / 1e9\n            self.training_history['gpu_memory'].append(gpu_memory)\n        else:\n            self.training_history['gpu_memory'].append(0)\n        \n        # その他のメトリクス\n        self.training_history['loss'].append(loss if loss is not None else float('nan'))\n        self.training_history['eval_loss'].append(eval_loss if eval_loss is not None else float('nan'))\n        self.training_history['learning_rate'].append(learning_rate if learning_rate is not None else float('nan'))\n        self.training_history['samples_per_second'].append(samples_per_second if samples_per_second is not None else float('nan'))\n    \n    def plot_current_progress(self):\n        \"\"\"現在の進捗をプロット\"\"\"\n        if not self.training_history['step']:\n            print(\"⚠️ モニタリングデータがありません\")\n            return\n        \n        # データをクリア\n        for ax in self.axes.flat:\n            ax.clear()\n        \n        steps = self.training_history['step']\n        \n        # 損失のプロット\n        if any(not pd.isna(x) for x in self.training_history['loss']):\n            valid_losses = [(s, l) for s, l in zip(steps, self.training_history['loss']) if not pd.isna(l)]\n            if valid_losses:\n                s_loss, losses = zip(*valid_losses)\n                self.axes[0, 0].plot(s_loss, losses, 'b-', linewidth=2, label='学習損失')\n        \n        if any(not pd.isna(x) for x in self.training_history['eval_loss']):\n            valid_eval_losses = [(s, l) for s, l in zip(steps, self.training_history['eval_loss']) if not pd.isna(l)]\n            if valid_eval_losses:\n                s_eval, eval_losses = zip(*valid_eval_losses)\n                self.axes[0, 0].plot(s_eval, eval_losses, 'r--', linewidth=2, label='検証損失')\n        \n        self.axes[0, 0].set_title('📉 学習損失')\n        self.axes[0, 0].set_xlabel('ステップ')\n        self.axes[0, 0].set_ylabel('損失')\n        self.axes[0, 0].legend()\n        self.axes[0, 0].grid(True, alpha=0.3)\n        \n        # GPU メモリのプロット\n        self.axes[0, 1].plot(steps, self.training_history['gpu_memory'], 'g-', linewidth=2)\n        self.axes[0, 1].set_title('🖥️ GPU メモリ使用量')\n        self.axes[0, 1].set_xlabel('ステップ')\n        self.axes[0, 1].set_ylabel('メモリ (GB)')\n        self.axes[0, 1].grid(True, alpha=0.3)\n        \n        # 学習速度のプロット\n        if any(not pd.isna(x) for x in self.training_history['samples_per_second']):\n            valid_speeds = [(s, sp) for s, sp in zip(steps, self.training_history['samples_per_second']) if not pd.isna(sp)]\n            if valid_speeds:\n                s_speed, speeds = zip(*valid_speeds)\n                self.axes[1, 0].plot(s_speed, speeds, 'orange', linewidth=2)\n        self.axes[1, 0].set_title('⚡ 学習速度')\n        self.axes[1, 0].set_xlabel('ステップ')\n        self.axes[1, 0].set_ylabel('サンプル/秒')\n        self.axes[1, 0].grid(True, alpha=0.3)\n        \n        # 学習率のプロット\n        if any(not pd.isna(x) for x in self.training_history['learning_rate']):\n            valid_lrs = [(s, lr) for s, lr in zip(steps, self.training_history['learning_rate']) if not pd.isna(lr)]\n            if valid_lrs:\n                s_lr, lrs = zip(*valid_lrs)\n                self.axes[1, 1].plot(s_lr, lrs, 'purple', linewidth=2)\n        self.axes[1, 1].set_title('📊 学習率')\n        self.axes[1, 1].set_xlabel('ステップ')\n        self.axes[1, 1].set_ylabel('学習率')\n        self.axes[1, 1].grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.show()\n    \n    def generate_training_report(self):\n        \"\"\"学習レポートを生成\"\"\"\n        if not self.training_history['step']:\n            print(\"⚠️ レポート生成用データがありません\")\n            return\n        \n        print(\"\\\\n\" + \"=\"*60)\n        print(\"📋 SFT学習レポート\")\n        print(\"=\"*60)\n        \n        # 基本統計\n        total_steps = len(self.training_history['step'])\n        final_loss = [l for l in self.training_history['loss'] if not pd.isna(l)]\n        final_loss = final_loss[-1] if final_loss else \"N/A\"\n        \n        max_gpu_memory = max(self.training_history['gpu_memory'])\n        avg_speed = [s for s in self.training_history['samples_per_second'] if not pd.isna(s)]\n        avg_speed = sum(avg_speed) / len(avg_speed) if avg_speed else \"N/A\"\n        \n        print(f\"📊 基本統計:\")\n        print(f\"  ├─ 総ステップ数: {total_steps}\")\n        print(f\"  ├─ 最終損失: {final_loss}\")\n        print(f\"  ├─ 最大GPU使用量: {max_gpu_memory:.1f} GB\")\n        print(f\"  └─ 平均学習速度: {avg_speed if avg_speed != 'N/A' else 'N/A'} サンプル/秒\")\n        \n        # 時間統計\n        if len(self.training_history['timestamp']) >= 2:\n            start_time = self.training_history['timestamp'][0]\n            end_time = self.training_history['timestamp'][-1]\n            duration = (end_time - start_time).total_seconds()\n            \n            print(f\"\\\\n⏱️  時間統計:\")\n            print(f\"  ├─ 開始時刻: {start_time.strftime('%H:%M:%S')}\")\n            print(f\"  ├─ 終了時刻: {end_time.strftime('%H:%M:%S')}\")\n            print(f\"  └─ 学習時間: {duration/60:.1f} 分\")\n        \n        print(\"=\"*60)\n\n# モニターの初期化\ntraining_monitor = TrainingMonitor()\n\n# デモ用のプログレス表示\ndef demo_training_progress():\n    \"\"\"デモ用の学習進捗表示\"\"\"\n    print(\"🔄 デモ: 学習進捗モニタリング\")\n    print(\"（実際の学習時にはリアルタイムで更新されます）\")\n    \n    # サンプルデータでデモ\n    import numpy as np\n    \n    steps = range(0, 100, 10)\n    losses = [2.5 * np.exp(-s/50) + 0.5 + np.random.normal(0, 0.1) for s in steps]\n    \n    for i, (step, loss) in enumerate(zip(steps, losses)):\n        training_monitor.update_metrics(\n            step=step,\n            loss=loss,\n            learning_rate=2e-5 * (1 - step/100),\n            samples_per_second=15 + np.random.normal(0, 2)\n        )\n    \n    # グラフを表示\n    fig = training_monitor.start_monitoring()\n    training_monitor.plot_current_progress()\n    training_monitor.generate_training_report()\n\nprint(\"📊 学習進捗モニタリング準備完了\")\nprint(\"\\\\n使用方法:\")\nprint(\"1. demo_training_progress()  # デモ表示\")\nprint(\"2. training_monitor.start_monitoring()  # リアルタイム監視開始\")\nprint(\"3. 学習中に training_monitor.update_metrics() で更新\")\nprint(\"4. training_monitor.generate_training_report()  # 最終レポート\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 📊 ノートブック内完結型進捗モニタリング\nfrom IPython.display import clear_output\n\nclass NotebookTrainingMonitor:\n    \"\"\"ノートブック内で完結する学習進捗監視クラス\"\"\"\n    \n    def __init__(self):\n        self.training_history = {\n            'step': [],\n            'loss': [],\n            'eval_loss': [],\n            'learning_rate': [],\n            'timestamp': [],\n            'gpu_memory': [],\n            'samples_per_second': []\n        }\n        self.start_time = None\n        \n    def start_monitoring(self):\n        \"\"\"モニタリングを開始\"\"\"\n        self.start_time = datetime.now()\n        print(\"🔄 学習進捗モニタリング開始\")\n        print(f\"開始時刻: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n        \n    def update_metrics(self, step, loss=None, eval_loss=None, learning_rate=None, samples_per_second=None):\n        \"\"\"メトリクスを更新してリアルタイム表示\"\"\"\n        self.training_history['step'].append(step)\n        self.training_history['timestamp'].append(datetime.now())\n        \n        # GPU メモリ使用量を取得\n        if torch.cuda.is_available():\n            gpu_memory = torch.cuda.memory_allocated() / 1e9\n            self.training_history['gpu_memory'].append(gpu_memory)\n        else:\n            self.training_history['gpu_memory'].append(0)\n        \n        # その他のメトリクス\n        self.training_history['loss'].append(loss if loss is not None else np.nan)\n        self.training_history['eval_loss'].append(eval_loss if eval_loss is not None else np.nan)\n        self.training_history['learning_rate'].append(learning_rate if learning_rate is not None else np.nan)\n        self.training_history['samples_per_second'].append(samples_per_second if samples_per_second is not None else np.nan)\n        \n        # リアルタイム表示更新（5ステップごと）\n        if step % 5 == 0:\n            self._update_display()\n    \n    def _update_display(self):\n        \"\"\"進捗表示を更新\"\"\"\n        if len(self.training_history['step']) == 0:\n            return\n        \n        current_time = datetime.now()\n        elapsed = (current_time - self.start_time).total_seconds() if self.start_time else 0\n        \n        # 最新の値を取得\n        latest_step = self.training_history['step'][-1]\n        latest_loss = self.training_history['loss'][-1]\n        latest_gpu = self.training_history['gpu_memory'][-1]\n        latest_speed = self.training_history['samples_per_second'][-1]\n        \n        # コンソール表示更新\n        print(f\"\\\\r📊 ステップ {latest_step} | \", end=\"\")\n        if not np.isnan(latest_loss):\n            print(f\"損失: {latest_loss:.4f} | \", end=\"\")\n        if latest_gpu > 0:\n            print(f\"GPU: {latest_gpu:.1f}GB | \", end=\"\")\n        if not np.isnan(latest_speed):\n            print(f\"速度: {latest_speed:.1f} samples/s | \", end=\"\")\n        print(f\"経過: {elapsed/60:.1f}分\", end=\"\", flush=True)\n    \n    def plot_training_progress(self):\n        \"\"\"学習進捗をプロット（ノートブック内表示）\"\"\"\n        if not self.training_history['step']:\n            print(\"⚠️ プロット用データがありません\")\n            return\n        \n        # 2x2のサブプロット\n        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n        fig.suptitle('🔄 LLaDA SFT学習進捗', fontsize=16, fontweight='bold')\n        \n        steps = self.training_history['step']\n        \n        # 損失プロット\n        valid_losses = [(s, l) for s, l in zip(steps, self.training_history['loss']) if not np.isnan(l)]\n        if valid_losses:\n            s_loss, losses = zip(*valid_losses)\n            axes[0, 0].plot(s_loss, losses, 'b-', linewidth=2, label='学習損失')\n        \n        valid_eval_losses = [(s, l) for s, l in zip(steps, self.training_history['eval_loss']) if not np.isnan(l)]\n        if valid_eval_losses:\n            s_eval, eval_losses = zip(*valid_eval_losses)\n            axes[0, 0].plot(s_eval, eval_losses, 'r--', linewidth=2, label='検証損失')\n        \n        axes[0, 0].set_title('📉 学習損失')\n        axes[0, 0].set_xlabel('ステップ')\n        axes[0, 0].set_ylabel('損失')\n        axes[0, 0].legend()\n        axes[0, 0].grid(True, alpha=0.3)\n        \n        # GPU メモリプロット\n        axes[0, 1].plot(steps, self.training_history['gpu_memory'], 'g-', linewidth=2)\n        axes[0, 1].set_title('🖥️ GPU メモリ使用量')\n        axes[0, 1].set_xlabel('ステップ')\n        axes[0, 1].set_ylabel('メモリ (GB)')\n        axes[0, 1].grid(True, alpha=0.3)\n        \n        # 学習速度プロット\n        valid_speeds = [(s, sp) for s, sp in zip(steps, self.training_history['samples_per_second']) if not np.isnan(sp)]\n        if valid_speeds:\n            s_speed, speeds = zip(*valid_speeds)\n            axes[1, 0].plot(s_speed, speeds, 'orange', linewidth=2)\n        axes[1, 0].set_title('⚡ 学習速度')\n        axes[1, 0].set_xlabel('ステップ')\n        axes[1, 0].set_ylabel('サンプル/秒')\n        axes[1, 0].grid(True, alpha=0.3)\n        \n        # 学習率プロット\n        valid_lrs = [(s, lr) for s, lr in zip(steps, self.training_history['learning_rate']) if not np.isnan(lr)]\n        if valid_lrs:\n            s_lr, lrs = zip(*valid_lrs)\n            axes[1, 1].plot(s_lr, lrs, 'purple', linewidth=2)\n        axes[1, 1].set_title('📊 学習率')\n        axes[1, 1].set_xlabel('ステップ')\n        axes[1, 1].set_ylabel('学習率')\n        axes[1, 1].grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.show()\n    \n    def generate_final_report(self):\n        \"\"\"最終レポートを生成（ノートブック内表示）\"\"\"\n        if not self.training_history['step']:\n            print(\"⚠️ レポート生成用データがありません\")\n            return\n        \n        print(\"\\\\n\" + \"=\"*60)\n        print(\"📋 SFT学習最終レポート\")\n        print(\"=\"*60)\n        \n        # 基本統計\n        total_steps = len(self.training_history['step'])\n        final_loss = [l for l in self.training_history['loss'] if not np.isnan(l)]\n        final_loss = final_loss[-1] if final_loss else \"N/A\"\n        \n        max_gpu_memory = max(self.training_history['gpu_memory']) if self.training_history['gpu_memory'] else 0\n        avg_speed = [s for s in self.training_history['samples_per_second'] if not np.isnan(s)]\n        avg_speed = sum(avg_speed) / len(avg_speed) if avg_speed else \"N/A\"\n        \n        print(f\"📊 基本統計:\")\n        print(f\"  ├─ 総ステップ数: {total_steps}\")\n        print(f\"  ├─ 最終損失: {final_loss}\")\n        print(f\"  ├─ 最大GPU使用量: {max_gpu_memory:.1f} GB\")\n        print(f\"  └─ 平均学習速度: {avg_speed if avg_speed != 'N/A' else 'N/A'} サンプル/秒\")\n        \n        # 時間統計\n        if len(self.training_history['timestamp']) >= 2:\n            start_time = self.training_history['timestamp'][0]\n            end_time = self.training_history['timestamp'][-1]\n            duration = (end_time - start_time).total_seconds()\n            \n            print(f\"\\\\n⏱️  時間統計:\")\n            print(f\"  ├─ 開始時刻: {start_time.strftime('%H:%M:%S')}\")\n            print(f\"  ├─ 終了時刻: {end_time.strftime('%H:%M:%S')}\")\n            print(f\"  └─ 学習時間: {duration/60:.1f} 分\")\n        \n        # 設定サマリー\n        print(f\"\\\\n⚙️  設定サマリー:\")\n        print(f\"  ├─ 設定: {CONFIG_NAME}\")\n        print(f\"  ├─ データサイズ: {config.dataset_size:,}\")\n        print(f\"  ├─ バッチサイズ: {config.batch_size}\")\n        print(f\"  ├─ エポック数: {config.num_epochs}\")\n        print(f\"  └─ 学習率: {config.learning_rate}\")\n        \n        print(\"=\"*60)\n        print(\"✅ レポート生成完了\")\n\n# モニターの初期化\nnotebook_monitor = NotebookTrainingMonitor()\n\n# デモ用プログレス表示\ndef demo_notebook_monitoring():\n    \"\"\"ノートブック内監視のデモ\"\"\"\n    print(\"🔄 ノートブック内監視デモ開始\")\n    \n    # サンプルデータでデモ\n    steps = range(0, 50, 5)\n    losses = [2.5 * np.exp(-s/25) + 0.5 + np.random.normal(0, 0.1) for s in steps]\n    \n    notebook_monitor.start_monitoring()\n    \n    for i, (step, loss) in enumerate(zip(steps, losses)):\n        notebook_monitor.update_metrics(\n            step=step,\n            loss=loss,\n            learning_rate=2e-5 * (1 - step/50),\n            samples_per_second=15 + np.random.normal(0, 2)\n        )\n        time.sleep(0.1)  # デモ用遅延\n    \n    print(\"\\\\n\\\\n📊 グラフ表示:\")\n    notebook_monitor.plot_training_progress()\n    notebook_monitor.generate_final_report()\n\nprint(\"📊 ノートブック内監視システム準備完了\")\nprint(\"\\\\n使用方法:\")\nprint(\"1. demo_notebook_monitoring()  # デモ表示\")\nprint(\"2. notebook_monitor.start_monitoring()  # 実際の監視開始\")\nprint(\"3. 学習中に自動更新\")\nprint(\"4. notebook_monitor.generate_final_report()  # 最終レポート\")\nprint(\"=\" * 50)\nprint(\"🎯 次のステップ: セル11を実行して実行インターフェースを準備\")\nprint(\"=\" * 50)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 📚 FAQ・ヘルプセクション\n\n## ❓ よくある質問\n\n### 🚀 実行関連\n\n**Q: 初回実行で何をすればいいですか？**\nA: 以下を順番に実行：\n1. セル1-8を上から順番に実行\n2. `result = notebook_runner.complete_auto_training()` を実行\n\n**Q: どの設定を選べばいいですか？**\nA: \n- 初心者・検証用: `validation` (1K サンプル, 約5分)\n- 中級者・バランス: `medium` (10K サンプル, 約30分)  \n- 上級者・本格学習: `production` (50K サンプル, 約2時間)\n\n**Q: 学習が途中で止まりました**\nA: 以下を試してください：\n```python\n# GPU メモリクリア\ntorch.cuda.empty_cache()\ngc.collect()\n\n# より小さな設定で再実行\nresult = quick_training()\n```\n\n### 💾 メモリ関連\n\n**Q: GPU メモリ不足エラーが出ます**\nA: 以下の順番で対処：\n1. `torch.cuda.empty_cache()` 実行\n2. より小さなバッチサイズ: `config.batch_size = 1`\n3. validation設定を使用: `quick_training()`\n4. カスタム設定: `custom_quick_training(dataset_size=500, batch_size=1)`\n\n**Q: どのくらいのGPUメモリが必要ですか？**\nA:\n- validation: 8GB以上推奨\n- medium: 12GB以上推奨  \n- production: 16GB以上推奨\n\n### 📊 結果関連\n\n**Q: 学習結果はどこに保存されますか？**\nA: `./llada_sft_output/` ディレクトリに自動保存されます。\n\n**Q: 学習の進捗はどこで確認できますか？**\nA: ノートブック内に自動表示されます。追加で `notebook_monitor.plot_training_progress()` で詳細グラフ表示可能。\n\n**Q: 生成品質を確認したいです**\nA: 学習完了後に自動評価が実行されます。手動実行は：\n```python\nevaluate_sft_model()\n```\n\n### 🔧 トラブルシューティング\n\n**Q: データセットが読み込めません**\nA: 自動でサンプルデータに切り替わります。問題ありません。\n\n**Q: 学習損失が下がりません**\nA: 以下を確認：\n1. 学習率が適切か（デフォルト: 2e-5）\n2. データ量が十分か（最低1000サンプル推奨）\n3. エポック数を増やす\n\n**Q: 生成結果が期待と違います**\nA: SFT後の追加調整方法：\n1. より多くのエポックで学習\n2. 異なる温度設定で生成テスト\n3. より大きなデータセットで再学習\n\n## 🛠️ カスタマイズ例\n\n### パラメータ調整\n```python\n# バッチサイズ調整\nconfig.batch_size = 1  # メモリ不足時\n\n# 学習率調整  \nconfig.learning_rate = 1e-5  # より保守的\n\n# データサイズ調整\nconfig.dataset_size = 5000  # 中間サイズ\n```\n\n### 詳細設定例\n```python\n# 高速テスト用\nresult = custom_quick_training(\n    dataset_size=100, \n    batch_size=1, \n    epochs=1\n)\n\n# バランス型\nresult = custom_quick_training(\n    dataset_size=2000,\n    batch_size=2, \n    epochs=2\n)\n```\n\n## 🆘 緊急時の対処法\n\n### 完全リセット手順\n```python\n# 1. GPU メモリクリア\ntorch.cuda.empty_cache()\ngc.collect()\n\n# 2. カーネル再起動（Runtime > Restart Runtime）\n\n# 3. セル1から再実行\n\n# 4. 最小設定で実行\nresult = custom_quick_training(dataset_size=100, batch_size=1, epochs=1)\n```\n\n### エラー報告\nエラーが解決しない場合は、以下の情報と共にご報告ください：\n- 実行したコマンド\n- エラーメッセージ全文\n- GPU情報（`nvidia-smi`の結果）\n- 使用した設定（validation/medium/production）\n\n---\n\n## 📞 追加サポート\n\n**実行成功のチェックリスト:**\n- ✅ セル1-8を順番に実行完了\n- ✅ エラーメッセージなし\n- ✅ GPU情報表示済み\n- ✅ `result = notebook_runner.complete_auto_training()` 実行\n\n**学習成功の確認:**\n- ✅ 「学習完了」メッセージ表示\n- ✅ 最終損失が表示される\n- ✅ モデル保存完了メッセージ\n- ✅ 自動評価結果表示\n\nすべて確認できれば学習成功です！🎉",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sft_evaluation_and_generation"
   },
   "outputs": [],
   "source": "# 🎮 完全自動実行システム（ノートブック内完結）\n\nclass NotebookSFTRunner:\n    \"\"\"ノートブック内完結型のSFT学習実行システム\"\"\"\n    \n    def __init__(self):\n        self.is_training = False\n        self.training_result = None\n        \n    def complete_auto_training(self, config_name=\"validation\", show_progress=True):\n        \"\"\"\n        完全自動SFT学習実行（ノートブック内完結）\n        \n        Args:\n            config_name: 使用する設定 (\"validation\", \"medium\", \"production\")\n            show_progress: 進捗表示を行うかどうか\n        \"\"\"\n        \n        print(\"🎬 LLaDA SFT学習 完全自動実行を開始します\")\n        print(\"=\"*60)\n        print(\"🔄 ノートブック内完結システム:\")\n        print(\"  ✅ 外部ツール不要\")\n        print(\"  ✅ WandB不使用\")\n        print(\"  ✅ 進捗はノートブック内表示\")\n        print(\"  ✅ 結果はセル出力に保存\")\n        print(\"=\"*60)\n        \n        # 1. 環境チェック\n        print(\"\\\\n🔍 1. 環境チェック...\")\n        gpu_available = torch.cuda.is_available()\n        if gpu_available:\n            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n            print(f\"  ✅ GPU: {torch.cuda.get_device_name(0)}\")\n            print(f\"  ✅ メモリ: {gpu_memory:.1f} GB\")\n            \n            # 自動設定選択\n            if gpu_memory >= 15 and config_name == \"validation\":\n                recommended_config = \"medium\"\n                print(f\"  💡 十分なメモリがあります。{recommended_config}設定を推奨\")\n            else:\n                recommended_config = config_name\n        else:\n            recommended_config = \"validation\"\n            print(\"  ⚠️  GPU未検出 - CPU学習（validation設定固定）\")\n        \n        # 2. 設定適用\n        print(f\"\\\\n⚙️  2. 設定適用: {recommended_config}\")\n        global config, CONFIG_NAME\n        CONFIG_NAME = recommended_config\n        config = select_config(recommended_config)\n        \n        # 3. メモリ最適化\n        print(\"\\\\n🧹 3. メモリ最適化...\")\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            gc.collect()\n            print(f\"  ✅ GPU メモリクリア完了\")\n        \n        # 4. 進捗監視開始\n        if show_progress:\n            print(\"\\\\n📊 4. 進捗監視開始...\")\n            notebook_monitor.start_monitoring()\n        \n        # 5. 学習実行\n        print(\"\\\\n🚀 5. 学習開始...\")\n        print(\"⏰ 3秒後に開始...\")\n        time.sleep(3)\n        \n        try:\n            # 実際の学習実行\n            training_result = self._execute_training(show_progress)\n            \n            # 6. 結果表示\n            print(\"\\\\n🎉 学習完了!\")\n            self._display_results(training_result, show_progress)\n            \n            # 7. 自動評価\n            print(\"\\\\n🔍 7. 自動評価実行...\")\n            self._run_auto_evaluation()\n            \n            # 8. 最終レポート\n            if show_progress:\n                print(\"\\\\n📄 8. 最終レポート生成...\")\n                notebook_monitor.plot_training_progress()\n                notebook_monitor.generate_final_report()\n            \n            # 成功メッセージ\n            print(\"\\\\n\" + \"🎊\" * 20)\n            print(\"🏆 LLaDA SFT学習が正常に完了しました!\")\n            print(\"🎊\" * 20)\n            \n            return training_result\n            \n        except Exception as e:\n            print(f\"\\\\n❌ 学習エラー: {e}\")\n            self._display_troubleshooting()\n            raise\n    \n    def _execute_training(self, show_progress=True):\n        \"\"\"実際の学習を実行\"\"\"\n        \n        # 学習前チェック\n        print(f\"📋 学習設定確認:\")\n        print(f\"  - データセット: {len(train_dataset)} サンプル\")\n        print(f\"  - バッチサイズ: {config.batch_size}\")\n        print(f\"  - エポック数: {config.num_epochs}\")\n        print(f\"  - 推定時間: {self._estimate_time()} 分\")\n        \n        # 学習前評価\n        if val_dataset and len(val_dataset) > 0:\n            print(\"\\\\n📊 学習前評価...\")\n            try:\n                initial_eval = trainer.evaluate()\n                print(f\"  初期損失: {initial_eval['eval_loss']:.4f}\")\n            except Exception as e:\n                print(f\"  初期評価スキップ: {e}\")\n        \n        # メイン学習\n        print(\"\\\\n🎯 メイン学習実行中...\")\n        start_time = time.time()\n        \n        # Trainer自体の学習実行\n        training_result = trainer.train()\n        \n        training_duration = time.time() - start_time\n        print(f\"\\\\n⏱️  学習時間: {training_duration/60:.1f} 分\")\n        \n        return training_result\n    \n    def _display_results(self, training_result, show_progress):\n        \"\"\"結果を表示\"\"\"\n        print(\"\\\\n📊 学習結果:\")\n        print(f\"  ✅ 最終損失: {training_result.training_loss:.4f}\")\n        print(f\"  ✅ 学習時間: {training_result.metrics['train_runtime']:.1f} 秒\")\n        print(f\"  ✅ サンプル/秒: {training_result.metrics['train_samples_per_second']:.2f}\")\n        \n        # モデル保存\n        print(\"\\\\n💾 モデル保存中...\")\n        try:\n            trainer.save_model()\n            tokenizer.save_pretrained(config.output_dir)\n            print(f\"  ✅ 保存完了: {config.output_dir}\")\n        except Exception as e:\n            print(f\"  ⚠️  保存エラー: {e}\")\n    \n    def _run_auto_evaluation(self):\n        \"\"\"自動評価を実行\"\"\"\n        model.eval()\n        \n        test_questions = [\n            \"日本の首都はどこですか？\",\n            \"機械学習とは何ですか？\",\n            \"Pythonでリストを逆順にする方法を教えてください。\"\n        ]\n        \n        print(f\"\\\\n🧪 {len(test_questions)} 個の質問で評価中...\")\n        \n        for i, question in enumerate(test_questions, 1):\n            print(f\"\\\\n--- 評価 {i} ---\")\n            print(f\"質問: {question}\")\n            \n            try:\n                # プロンプト準備\n                formatted_text, _ = processor.format_for_sft(question, \"\")\n                prompt_only = formatted_text.replace(processor.special_tokens['eos'], \"\")\n                \n                # 生成実行\n                inputs = tokenizer(prompt_only, return_tensors=\"pt\", max_length=256, truncation=True).to(device)\n                \n                with torch.no_grad():\n                    outputs = model.generate(\n                        **inputs,\n                        max_new_tokens=80,\n                        do_sample=True,\n                        temperature=0.7,\n                        top_p=0.9,\n                        pad_token_id=tokenizer.pad_token_id\n                    )\n                    \n                    generated_text = tokenizer.decode(\n                        outputs[0][inputs.input_ids.shape[1]:], \n                        skip_special_tokens=True\n                    )\n                    \n                    print(f\"回答: {generated_text.strip()}\")\n                    \n            except Exception as e:\n                print(f\"評価エラー: {e}\")\n        \n        print(\"\\\\n✅ 自動評価完了\")\n    \n    def _estimate_time(self):\n        \"\"\"学習時間を推定\"\"\"\n        base_time = 0.05  # 秒/サンプル\n        total_samples = config.dataset_size * config.num_epochs\n        estimated_seconds = total_samples * base_time / config.batch_size\n        return max(1, int(estimated_seconds / 60))\n    \n    def _display_troubleshooting(self):\n        \"\"\"トラブルシューティング表示\"\"\"\n        print(\"\\\\n\" + \"🔧\" * 20)\n        print(\"トラブルシューティングガイド\")\n        print(\"🔧\" * 20)\n        print(\"💡 解決策:\")\n        print(\"  1. より小さな設定を試す: complete_auto_training('validation')\")\n        print(\"  2. GPU メモリクリア: torch.cuda.empty_cache()\")\n        print(\"  3. バッチサイズ削減: config.batch_size = 1\")\n        print(\"  4. セル再起動後に再実行\")\n        print(\"🔧\" * 20)\n\n# 統合実行システム\nnotebook_runner = NotebookSFTRunner()\n\n# 簡単実行関数群\ndef quick_training():\n    \"\"\"最速実行（validation設定）\"\"\"\n    print(\"⚡ クイック学習開始...\")\n    return notebook_runner.complete_auto_training(\"validation\")\n\ndef medium_training():\n    \"\"\"中規模学習（medium設定）\"\"\"\n    print(\"💪 中規模学習開始...\")\n    return notebook_runner.complete_auto_training(\"medium\")\n\ndef production_training():\n    \"\"\"本格学習（production設定）\"\"\"\n    print(\"🔥 本格学習開始...\")\n    return notebook_runner.complete_auto_training(\"production\")\n\ndef custom_quick_training(dataset_size=500, batch_size=1, epochs=1):\n    \"\"\"カスタム設定での高速学習\"\"\"\n    print(f\"🎛️ カスタム学習: {dataset_size}サンプル, バッチ{batch_size}, {epochs}エポック\")\n    \n    # 設定カスタマイズ\n    global config\n    config.dataset_size = dataset_size\n    config.batch_size = batch_size\n    config.num_epochs = epochs\n    \n    return notebook_runner.complete_auto_training(\"validation\")\n\n# メインインターフェース\nprint(\"🎯 ノートブック内完結SFT学習システム準備完了\")\nprint(\"\\\\n\" + \"🚀\" * 25)\nprint(\"即座に実行可能なコマンド:\")\nprint(\"🚀\" * 25)\nprint()\nprint(\"🥇 完全自動（初回推奨）:\")\nprint(\"   result = notebook_runner.complete_auto_training()\")\nprint()\nprint(\"⚡ クイック実行:\")\nprint(\"   result = quick_training()\")\nprint()\nprint(\"💪 中規模実行:\")\nprint(\"   result = medium_training()\")\nprint()\nprint(\"🔥 本格実行:\")\nprint(\"   result = production_training()\")\nprint()\nprint(\"🎛️ カスタム実行:\")\nprint(\"   result = custom_quick_training(dataset_size=1000, batch_size=2, epochs=1)\")\nprint()\nprint(\"🚀\" * 25)\nprint(\"✨ 特徴:\")\nprint(\"  ✅ ノートブック内完結（外部ツール不要）\")\nprint(\"  ✅ 自動エラー回復\")\nprint(\"  ✅ リアルタイム進捗表示\")\nprint(\"  ✅ 自動評価・レポート生成\")\nprint(\"🚀\" * 25)"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}