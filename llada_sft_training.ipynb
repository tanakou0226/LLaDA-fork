{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sft_training_header"
   },
   "source": "# LLaDA SFTå­¦ç¿’ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯\n\nã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€LLaDAï¼ˆLarge Language Diffusion with mAskingï¼‰ãƒ¢ãƒ‡ãƒ«ã®Supervised Fine-Tuningï¼ˆSFTï¼‰ã‚’å®Ÿè£…ã—ã¾ã™ã€‚\n\n## ğŸ“‹ å®Ÿè¡Œæ‰‹é †ï¼ˆåˆå¿ƒè€…å‘ã‘ï¼‰\n\n### ğŸš€ æœ€é€Ÿå®Ÿè¡Œï¼ˆæ¨å¥¨ï¼‰\n```python\n# å…¨ã¦ã®ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œå¾Œã€ä»¥ä¸‹ã‚’å®Ÿè¡Œï¼š\nresult = run_complete_sft_demo()\n```\n\n### ğŸ“ ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ\n1. **ã‚»ãƒ«1-8ã‚’é †ç•ªã«å®Ÿè¡Œ** - ç’°å¢ƒè¨­å®šã‹ã‚‰ãƒ¢ãƒ‡ãƒ«æº–å‚™ã¾ã§\n2. **å­¦ç¿’å®Ÿè¡Œ** - ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã‚’é¸æŠï¼š\n   - ğŸ¥‡ `result = run_complete_sft_demo()` ï¼ˆåˆå›æ¨å¥¨ï¼‰\n   - âš¡ `result = quick_start_button()` ï¼ˆæœ€é€Ÿï¼‰\n   - ğŸ›ï¸ `result = custom_training(dataset_size=1000, batch_size=2, epochs=1)` ï¼ˆã‚«ã‚¹ã‚¿ãƒ ï¼‰\n\n### âš™ï¸ è¨­å®šé¸æŠã‚¬ã‚¤ãƒ‰\n- **validation**: 1K ã‚µãƒ³ãƒ—ãƒ«, 1ã‚¨ãƒãƒƒã‚¯ï¼ˆåˆå¿ƒè€…ãƒ»æ¤œè¨¼ç”¨ï¼‰\n- **medium**: 10K ã‚µãƒ³ãƒ—ãƒ«, 2ã‚¨ãƒãƒƒã‚¯ï¼ˆä¸­ç´šè€…ãƒ»ãƒãƒ©ãƒ³ã‚¹å‹ï¼‰\n- **production**: 50K ã‚µãƒ³ãƒ—ãƒ«, 3ã‚¨ãƒãƒƒã‚¯ï¼ˆä¸Šç´šè€…ãƒ»æœ¬æ ¼å­¦ç¿’ï¼‰\n\n## âœ¨ ç‰¹å¾´\n- âœ… Google Colab Proå¯¾å¿œï¼ˆ16GB GPUåˆ¶é™å†…ã§å‹•ä½œï¼‰\n- âœ… GUIDELINES.mdã«åŸºã¥ãæ­£ç¢ºãªSFTå®Ÿè£…\n- âœ… æ—¥æœ¬èªAlpacaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½¿ç”¨\n- âœ… ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…å®Œçµï¼ˆå¤–éƒ¨ãƒ„ãƒ¼ãƒ«ä¸è¦ï¼‰\n- âœ… ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯å®Ÿè¡Œæ©Ÿèƒ½\n- âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—ç›£è¦–\n- âœ… è‡ªå‹•ã‚¨ãƒ©ãƒ¼å›å¾©æ©Ÿèƒ½\n\n## ğŸ”¬ SFTã®æŠ€è¡“çš„ç‰¹å¾´\n- **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéãƒã‚¹ã‚­ãƒ³ã‚°**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã¯ãƒã‚¹ã‚¯ã—ãªã„\n- **Answer lengthæ­£è¦åŒ–**: å›ç­”éƒ¨åˆ†ã®é•·ã•ã§æå¤±ã‚’æ­£è¦åŒ–\n- **å®Œå…¨æ–‡å­¦ç¿’**: äº‹å‰å­¦ç¿’ã¨ã¯ç•°ãªã‚Šã€å®Œå…¨ãªæ–‡ç« ã§å­¦ç¿’\n\n## â“ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n- ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼ â†’ ã‚ˆã‚Šå°ã•ãªè¨­å®šï¼ˆvalidationï¼‰ã‚’ä½¿ç”¨\n- CUDA ã‚¨ãƒ©ãƒ¼ â†’ `torch.cuda.empty_cache()` å®Ÿè¡Œ\n- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¨ãƒ©ãƒ¼ â†’ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\n- è©³ç´°ãªãƒ˜ãƒ«ãƒ—ã¯æœ€ä¸‹éƒ¨ã®FAQã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‚ç…§\n\n---",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "environment_setup"
   },
   "outputs": [],
   "source": "# ğŸ“¦ ç’°å¢ƒè¨­å®šã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\nimport os\nimport sys\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# WandBå®Œå…¨ç„¡åŠ¹åŒ–ï¼ˆAPI keyè¦æ±‚ã‚’é˜²ãï¼‰\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ[\"WANDB_MODE\"] = \"disabled\"\nos.environ[\"WANDB_SILENT\"] = \"true\"\n\n# Google Colabç’°å¢ƒã®æ¤œå‡º\nIN_COLAB = 'google.colab' in sys.modules\nprint(f\"å®Ÿè¡Œç’°å¢ƒ: {'Google Colab' if IN_COLAB else 'ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ'}\")\n\nif IN_COLAB:\n    # Google Colabç”¨ã®GPUæƒ…å ±è¡¨ç¤º\n    !nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv\n    \n    # å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n    print(\"ğŸ“¦ å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n    !pip install -q transformers==4.49.0 accelerate==0.34.2 datasets==2.21.0\n    !pip install -q torch==2.0.1 torchvision==0.15.2\n    !pip install -q matplotlib seaborn tqdm pandas\n    \n    print(\"âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå®Œäº†ã—ã¾ã—ãŸ\")\n\n# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\nprint(\"ğŸ“š ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆä¸­...\")\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom datasets import load_dataset, Dataset as HFDataset\nfrom tqdm.auto import tqdm\nimport json\nimport random\nfrom typing import Dict, List, Optional, Tuple\nimport gc\nfrom dataclasses import dataclass\nimport time\nfrom datetime import datetime\nimport pandas as pd\n\n# WandBç„¡åŠ¹åŒ–ç¢ºèª\ntry:\n    import wandb\n    wandb.init(mode=\"disabled\")\n    print(\"ğŸš« WandBç„¡åŠ¹åŒ–ç¢ºèªæ¸ˆã¿\")\nexcept ImportError:\n    print(\"âœ… WandBæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆå•é¡Œãªã—ï¼‰\")\nexcept Exception:\n    print(\"âœ… WandBç„¡åŠ¹åŒ–æ¸ˆã¿\")\n\n# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"ğŸ–¥ï¸  ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\nif torch.cuda.is_available():\n    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"  åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\nelse:\n    print(\"  âš ï¸  GPUæœªæ¤œå‡º - CPUå­¦ç¿’ã«ãªã‚Šã¾ã™\")\n\n# ã‚·ãƒ¼ãƒ‰å›ºå®š\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\n\n# ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š\nplt.style.use('default')  # ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¹ã‚¿ã‚¤ãƒ«\nsns.set_palette(\"husl\")   # è¦‹ã‚„ã™ã„è‰²åˆã„\n\nprint(\"âœ… ç’°å¢ƒè¨­å®šãŒå®Œäº†ã—ã¾ã—ãŸ\")\nprint(\"ğŸš« WandBå®Œå…¨ç„¡åŠ¹åŒ–æ¸ˆã¿ - API keyè¦æ±‚ãªã—\")\nprint(\"=\" * 50)\nprint(\"ğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: ã‚»ãƒ«2ã‚’å®Ÿè¡Œã—ã¦å­¦ç¿’è¨­å®šã‚’é¸æŠ\")\nprint(\"=\" * 50)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_configurations"
   },
   "outputs": [],
   "source": [
    "# å­¦ç¿’è¨­å®šã®å®šç¾©\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"å­¦ç¿’è¨­å®šã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹\"\"\"\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«è¨­å®š\n",
    "    model_name: str = \"GSAI-ML/LLaDA-8B-Base\"\n",
    "    max_length: int = 1024\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿è¨­å®š\n",
    "    dataset_size: int = 1000  # ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿æ•°\n",
    "    validation_split: float = 0.1\n",
    "    \n",
    "    # å­¦ç¿’è¨­å®š\n",
    "    batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 8\n",
    "    num_epochs: int = 1\n",
    "    learning_rate: float = 2e-5\n",
    "    warmup_steps: int = 100\n",
    "    \n",
    "    # SFTç‰¹æœ‰è¨­å®š\n",
    "    mask_id: int = 126336  # [MASK]ãƒˆãƒ¼ã‚¯ãƒ³ID\n",
    "    eps: float = 1e-3  # ãƒã‚¹ã‚­ãƒ³ã‚°ç¢ºç‡ã®æœ€å°å€¤\n",
    "    \n",
    "    # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–\n",
    "    fp16: bool = True\n",
    "    gradient_checkpointing: bool = True\n",
    "    dataloader_num_workers: int = 2\n",
    "    \n",
    "    # ä¿å­˜ãƒ»è©•ä¾¡è¨­å®š\n",
    "    save_steps: int = 500\n",
    "    eval_steps: int = 250\n",
    "    logging_steps: int = 50\n",
    "    output_dir: str = \"./llada_sft_output\"\n",
    "\n",
    "# äº‹å‰å®šç¾©ã•ã‚ŒãŸè¨­å®š\n",
    "CONFIGS = {\n",
    "    \"validation\": TrainingConfig(\n",
    "        dataset_size=1000,\n",
    "        batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_epochs=1,\n",
    "        learning_rate=5e-5,\n",
    "        save_steps=200,\n",
    "        eval_steps=100\n",
    "    ),\n",
    "    \n",
    "    \"production\": TrainingConfig(\n",
    "        dataset_size=50000,  # æœ¬æ ¼çš„ãªãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º\n",
    "        batch_size=1,        # ãƒ¡ãƒ¢ãƒªåˆ¶é™å¯¾å¿œ\n",
    "        gradient_accumulation_steps=16,\n",
    "        num_epochs=3,\n",
    "        learning_rate=2e-5,\n",
    "        warmup_steps=500,\n",
    "        save_steps=1000,\n",
    "        eval_steps=500\n",
    "    ),\n",
    "    \n",
    "    \"medium\": TrainingConfig(\n",
    "        dataset_size=10000,\n",
    "        batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        num_epochs=2,\n",
    "        learning_rate=3e-5,\n",
    "        save_steps=500,\n",
    "        eval_steps=250\n",
    "    )\n",
    "}\n",
    "\n",
    "def select_config(config_name: str = \"validation\") -> TrainingConfig:\n",
    "    \"\"\"è¨­å®šã‚’é¸æŠã™ã‚‹é–¢æ•°\"\"\"\n",
    "    if config_name not in CONFIGS:\n",
    "        print(f\"è­¦å‘Š: '{config_name}'ã¯ç„¡åŠ¹ãªè¨­å®šã§ã™ã€‚åˆ©ç”¨å¯èƒ½: {list(CONFIGS.keys())}\")\n",
    "        config_name = \"validation\"\n",
    "    \n",
    "    config = CONFIGS[config_name]\n",
    "    print(f\"âœ… '{config_name}' è¨­å®šã‚’é¸æŠã—ã¾ã—ãŸ\")\n",
    "    print(f\"  - ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {config.dataset_size:,}\")\n",
    "    print(f\"  - ãƒãƒƒãƒã‚µã‚¤ã‚º: {config.batch_size}\")\n",
    "    print(f\"  - ã‚¨ãƒãƒƒã‚¯æ•°: {config.num_epochs}\")\n",
    "    print(f\"  - å­¦ç¿’ç‡: {config.learning_rate}\")\n",
    "    \n",
    "    # GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¨å®š\n",
    "    if torch.cuda.is_available():\n",
    "        estimated_memory = (\n",
    "            8 * config.batch_size * config.max_length * 4 / 1e9 +  # ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "            config.batch_size * config.max_length * 4 * 2 / 1e9    # ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "        )\n",
    "        available_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"  - æ¨å®šGPUä½¿ç”¨é‡: {estimated_memory:.1f} GB / {available_memory:.1f} GB\")\n",
    "        \n",
    "        if estimated_memory > available_memory * 0.9:\n",
    "            print(\"  âš ï¸  ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ã‚ˆã‚Šå°ã•ãªè¨­å®šã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "# ä½¿ç”¨ã™ã‚‹è¨­å®šã‚’é¸æŠï¼ˆvalidation/medium/productionï¼‰\n",
    "CONFIG_NAME = \"validation\"  # ã“ã“ã‚’å¤‰æ›´ã—ã¦è¨­å®šã‚’é¸æŠ\n",
    "config = select_config(CONFIG_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alpaca_dataset_loading"
   },
   "outputs": [],
   "source": [
    "# Alpacaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†\n",
    "class AlpacaDatasetProcessor:\n",
    "    \"\"\"æ—¥æœ¬èªAlpacaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å‡¦ç†ã‚¯ãƒ©ã‚¹\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, config: TrainingConfig):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        self.special_tokens = {\n",
    "            'bos': tokenizer.bos_token or '<s>',\n",
    "            'eos': tokenizer.eos_token or '</s>',\n",
    "            'start_id': '<start_id>',\n",
    "            'end_id': '<end_id>',\n",
    "            'eot_id': '<eot_id>'\n",
    "        }\n",
    "    \n",
    "    def load_dataset(self) -> List[Dict]:\n",
    "        \"\"\"æ—¥æœ¬èªAlpacaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€\"\"\"\n",
    "        try:\n",
    "            print(\"ğŸ“¥ æ—¥æœ¬èªAlpacaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "            \n",
    "            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ï¼ˆè¤‡æ•°ã®ã‚½ãƒ¼ã‚¹ã‚’è©¦è¡Œï¼‰\n",
    "            dataset_sources = [\n",
    "                \"kunishou/databricks-dolly-15k-ja\",\n",
    "                \"izumi-lab/llm-japanese-dataset\",\n",
    "                \"elyza/ELYZA-tasks-100\"\n",
    "            ]\n",
    "            \n",
    "            dataset = None\n",
    "            for source in dataset_sources:\n",
    "                try:\n",
    "                    dataset = load_dataset(source, split='train')\n",
    "                    print(f\"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ '{source}' ã®èª­ã¿è¾¼ã¿ã«æˆåŠŸ\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"  '{source}' ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if dataset is None:\n",
    "                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ\n",
    "                print(\"âš ï¸  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã€‚ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\")\n",
    "                return self._create_sample_data()\n",
    "            \n",
    "            # ãƒ‡ãƒ¼ã‚¿ã‚’æ¨™æº–å½¢å¼ã«å¤‰æ›\n",
    "            processed_data = self._process_dataset(dataset)\n",
    "            \n",
    "            # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚’åˆ¶é™\n",
    "            if len(processed_data) > self.config.dataset_size:\n",
    "                processed_data = processed_data[:self.config.dataset_size]\n",
    "            \n",
    "            print(f\"âœ… {len(processed_data):,} ã‚µãƒ³ãƒ—ãƒ«ã‚’æº–å‚™ã—ã¾ã—ãŸ\")\n",
    "            return processed_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "            print(\"ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™\")\n",
    "            return self._create_sample_data()\n",
    "    \n",
    "    def _process_dataset(self, dataset) -> List[Dict]:\n",
    "        \"\"\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ¨™æº–å½¢å¼ã«å¤‰æ›\"\"\"\n",
    "        processed = []\n",
    "        \n",
    "        for item in tqdm(dataset, desc=\"ãƒ‡ãƒ¼ã‚¿å¤‰æ›ä¸­\"):\n",
    "            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå½¢å¼ã«å¿œã˜ã¦é©å¿œ\n",
    "            if 'instruction' in item and 'output' in item:\n",
    "                instruction = item['instruction']\n",
    "                if 'input' in item and item['input']:\n",
    "                    instruction += f\"\\n{item['input']}\"\n",
    "                response = item['output']\n",
    "            elif 'input' in item and 'output' in item:\n",
    "                instruction = item['input']\n",
    "                response = item['output']\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            if instruction and response:\n",
    "                processed.append({\n",
    "                    'instruction': instruction.strip(),\n",
    "                    'response': response.strip()\n",
    "                })\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def _create_sample_data(self) -> List[Dict]:\n",
    "        \"\"\"ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ\"\"\"\n",
    "        sample_data = [\n",
    "            {\n",
    "                'instruction': 'æ—¥æœ¬ã®é¦–éƒ½ã¯ã©ã“ã§ã™ã‹ï¼Ÿ',\n",
    "                'response': 'æ—¥æœ¬ã®é¦–éƒ½ã¯æ±äº¬ã§ã™ã€‚æ±äº¬ã¯é–¢æ±åœ°æ–¹ã«ä½ç½®ã—ã€æ—¥æœ¬ã®æ”¿æ²»ãƒ»çµŒæ¸ˆãƒ»æ–‡åŒ–ã®ä¸­å¿ƒåœ°ã§ã™ã€‚'\n",
    "            },\n",
    "            {\n",
    "                'instruction': 'Pythonã§ãƒªã‚¹ãƒˆã‚’é€†é †ã«ã™ã‚‹æ–¹æ³•ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚',\n",
    "                'response': 'Pythonã§ãƒªã‚¹ãƒˆã‚’é€†é †ã«ã™ã‚‹æ–¹æ³•ã¯ã„ãã¤ã‹ã‚ã‚Šã¾ã™ã€‚æœ€ã‚‚ç°¡å˜ãªæ–¹æ³•ã¯ã€reverse()ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ã†ã“ã¨ã§ã™ï¼šmy_list.reverse()ã€‚ã¾ãŸã€ã‚¹ãƒ©ã‚¤ã‚¹ã‚’ä½¿ã£ã¦æ–°ã—ã„ãƒªã‚¹ãƒˆã‚’ä½œã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ï¼šnew_list = my_list[::-1]ã€‚'\n",
    "            },\n",
    "            {\n",
    "                'instruction': 'æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ',\n",
    "                'response': 'æ©Ÿæ¢°å­¦ç¿’ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è‡ªå‹•çš„ã«ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦äºˆæ¸¬ã‚„åˆ¤æ–­ã‚’è¡Œã†æŠ€è¡“ã§ã™ã€‚å¾“æ¥ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¨ã¯ç•°ãªã‚Šã€æ˜ç¤ºçš„ã«ãƒ«ãƒ¼ãƒ«ã‚’æ›¸ãä»£ã‚ã‚Šã«ã€å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¦å‰‡æ€§ã‚’è¦‹ã¤ã‘å‡ºã—ã¾ã™ã€‚'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æŒ‡å®šã‚µã‚¤ã‚ºã¾ã§ç¹°ã‚Šè¿”ã—\n",
    "        repeated_data = []\n",
    "        for i in range(self.config.dataset_size):\n",
    "            repeated_data.append(sample_data[i % len(sample_data)])\n",
    "        \n",
    "        return repeated_data\n",
    "    \n",
    "    def format_for_sft(self, instruction: str, response: str) -> Tuple[str, int]:\n",
    "        \"\"\"SFTç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆGUIDELINES.mdã«åŸºã¥ãï¼‰\"\"\"\n",
    "        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: <BOS><start_id>user<end_id>\\n{instruction}<eot_id><start_id>assistant<end_id>\\n{response}<EOS>\n",
    "        prompt_part = (\n",
    "            f\"{self.special_tokens['bos']}\"\n",
    "            f\"{self.special_tokens['start_id']}user{self.special_tokens['end_id']}\\n\"\n",
    "            f\"{instruction}\"\n",
    "            f\"{self.special_tokens['eot_id']}\"\n",
    "            f\"{self.special_tokens['start_id']}assistant{self.special_tokens['end_id']}\\n\"\n",
    "        )\n",
    "        \n",
    "        full_text = prompt_part + response + self.special_tokens['eos']\n",
    "        \n",
    "        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·ã‚’è¨ˆç®—ï¼ˆresponseéƒ¨åˆ†ã‚’é™¤ãï¼‰\n",
    "        prompt_tokens = self.tokenizer.encode(prompt_part, add_special_tokens=False)\n",
    "        prompt_length = len(prompt_tokens)\n",
    "        \n",
    "        return full_text, prompt_length\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ—ãƒ­ã‚»ãƒƒã‚µã®åˆæœŸåŒ–\n",
    "print(\"ğŸ”§ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name, trust_remote_code=True)\n",
    "\n",
    "# ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®š\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å®Œäº† (èªå½™ã‚µã‚¤ã‚º: {len(tokenizer):,})\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å‡¦ç†\n",
    "processor = AlpacaDatasetProcessor(tokenizer, config)\n",
    "raw_data = processor.load_dataset()\n",
    "\n",
    "print(f\"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±:\")\n",
    "print(f\"  - ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(raw_data):,}\")\n",
    "if raw_data:\n",
    "    sample = raw_data[0]\n",
    "    formatted_text, prompt_len = processor.format_for_sft(sample['instruction'], sample['response'])\n",
    "    print(f\"  - ã‚µãƒ³ãƒ—ãƒ«é•·: {len(formatted_text)} æ–‡å­—\")\n",
    "    print(f\"  - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {prompt_len} ãƒˆãƒ¼ã‚¯ãƒ³\")\n",
    "    print(f\"\\nğŸ“ ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¾‹:\")\n",
    "    print(f\"  æŒ‡ç¤º: {sample['instruction'][:50]}...\")\n",
    "    print(f\"  å¿œç­”: {sample['response'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sft_dataset_class"
   },
   "outputs": [],
   "source": [
    "# SFTç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹\n",
    "class LLaDASFTDataset(Dataset):\n",
    "    \"\"\"LLaDA SFTå­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹\"\"\"\n",
    "    \n",
    "    def __init__(self, data: List[Dict], processor: AlpacaDatasetProcessor, config: TrainingConfig):\n",
    "        self.data = data\n",
    "        self.processor = processor\n",
    "        self.config = config\n",
    "        self.tokenizer = processor.tokenizer\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚’äº‹å‰ã«å‡¦ç†\n",
    "        self.processed_data = self._preprocess_data()\n",
    "        \n",
    "        print(f\"âœ… SFTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™ã—ã¾ã—ãŸ ({len(self.processed_data)} ã‚µãƒ³ãƒ—ãƒ«)\")\n",
    "    \n",
    "    def _preprocess_data(self) -> List[Dict]:\n",
    "        \"\"\"å…¨ãƒ‡ãƒ¼ã‚¿ã‚’äº‹å‰ã«å‡¦ç†\"\"\"\n",
    "        processed = []\n",
    "        \n",
    "        for item in tqdm(self.data, desc=\"SFTãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ä¸­\"):\n",
    "            try:\n",
    "                # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ\n",
    "                formatted_text, prompt_length = self.processor.format_for_sft(\n",
    "                    item['instruction'], \n",
    "                    item['response']\n",
    "                )\n",
    "                \n",
    "                # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "                tokens = self.tokenizer.encode(\n",
    "                    formatted_text,\n",
    "                    add_special_tokens=False,\n",
    "                    max_length=self.config.max_length,\n",
    "                    truncation=True\n",
    "                )\n",
    "                \n",
    "                # ååˆ†ãªé•·ã•ãŒã‚ã‚‹å ´åˆã®ã¿ä½¿ç”¨\n",
    "                if len(tokens) > prompt_length + 5:  # æœ€ä½5ãƒˆãƒ¼ã‚¯ãƒ³ã®å¿œç­”ãŒå¿…è¦\n",
    "                    processed.append({\n",
    "                        'input_ids': tokens,\n",
    "                        'prompt_length': prompt_length,\n",
    "                        'original_instruction': item['instruction'],\n",
    "                        'original_response': item['response']\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼ (ã‚¹ã‚­ãƒƒãƒ—): {e}\")\n",
    "                continue\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.processed_data)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        item = self.processed_data[idx]\n",
    "        \n",
    "        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°\n",
    "        input_ids = item['input_ids'][:self.config.max_length]\n",
    "        if len(input_ids) < self.config.max_length:\n",
    "            pad_length = self.config.max_length - len(input_ids)\n",
    "            input_ids.extend([self.tokenizer.pad_token_id] * pad_length)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'prompt_length': torch.tensor(item['prompt_length'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(\n",
    "                [1 if token_id != self.tokenizer.pad_token_id else 0 for token_id in input_ids],\n",
    "                dtype=torch.long\n",
    "            )\n",
    "        }\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ†å‰²ã¨ä½œæˆ\n",
    "def create_datasets(data: List[Dict], processor: AlpacaDatasetProcessor, config: TrainingConfig):\n",
    "    \"\"\"å­¦ç¿’ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ\"\"\"\n",
    "    # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
    "    random.shuffle(data)\n",
    "    \n",
    "    # åˆ†å‰²\n",
    "    split_idx = int(len(data) * (1 - config.validation_split))\n",
    "    train_data = data[:split_idx]\n",
    "    val_data = data[split_idx:]\n",
    "    \n",
    "    print(f\"ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²:\")\n",
    "    print(f\"  - å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(train_data):,} ã‚µãƒ³ãƒ—ãƒ«\")\n",
    "    print(f\"  - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(val_data):,} ã‚µãƒ³ãƒ—ãƒ«\")\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ\n",
    "    train_dataset = LLaDASFTDataset(train_data, processor, config)\n",
    "    val_dataset = LLaDASFTDataset(val_data, processor, config) if val_data else None\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ\n",
    "train_dataset, val_dataset = create_datasets(raw_data, processor, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sft_forward_process"
   },
   "outputs": [],
   "source": [
    "# SFTç”¨Forward Processå®Ÿè£…ï¼ˆGUIDELINES.mdã«åŸºã¥ãï¼‰\n",
    "class SFTForwardProcess:\n",
    "    \"\"\"SFTç”¨ã®ãƒã‚¹ã‚­ãƒ³ã‚°ãƒ»æå¤±è¨ˆç®—ã‚¯ãƒ©ã‚¹\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.mask_id = config.mask_id\n",
    "        self.eps = config.eps\n",
    "    \n",
    "    def forward_process(self, input_ids: torch.Tensor, prompt_lengths: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        SFTç”¨ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ—ãƒ­ã‚»ã‚¹ï¼ˆGUIDELINES.mdæº–æ‹ ï¼‰\n",
    "        \n",
    "        Args:\n",
    "            input_ids: [batch_size, seq_len] å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ID\n",
    "            prompt_lengths: [batch_size] å„ã‚µãƒ³ãƒ—ãƒ«ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·\n",
    "        \n",
    "        Returns:\n",
    "            noisy_batch: ãƒã‚¹ã‚¯ã•ã‚ŒãŸå…¥åŠ›\n",
    "            masked_indices: ãƒã‚¹ã‚¯ã•ã‚ŒãŸä½ç½®\n",
    "            p_mask: ãƒã‚¹ã‚­ãƒ³ã‚°ç¢ºç‡\n",
    "        \"\"\"\n",
    "        b, l = input_ids.shape\n",
    "        device = input_ids.device\n",
    "        \n",
    "        # ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¹ã‚­ãƒ³ã‚°ç¢ºç‡ã®ç”Ÿæˆ\n",
    "        t = torch.rand(b, device=device)\n",
    "        p_mask = (1 - self.eps) * t + self.eps\n",
    "        p_mask = p_mask[:, None].repeat(1, l)\n",
    "        \n",
    "        # åˆæœŸãƒã‚¹ã‚­ãƒ³ã‚°\n",
    "        masked_indices = torch.rand((b, l), device=device) < p_mask\n",
    "        noisy_batch = torch.where(masked_indices, self.mask_id, input_ids)\n",
    "        \n",
    "        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã®ãƒã‚¹ã‚¯ã‚’è§£é™¤ï¼ˆSFTã®æ ¸å¿ƒéƒ¨åˆ†ï¼‰\n",
    "        token_positions = torch.arange(l, device=device).expand(b, l)\n",
    "        prompt_mask = token_positions < prompt_lengths.unsqueeze(1)\n",
    "        \n",
    "        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã¯å…ƒã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¿æŒ\n",
    "        noisy_batch[prompt_mask] = input_ids[prompt_mask]\n",
    "        \n",
    "        # ãƒã‚¹ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ›´æ–°ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã¯ãƒã‚¹ã‚¯ã•ã‚Œã¦ã„ãªã„ï¼‰\n",
    "        masked_indices = (noisy_batch == self.mask_id)\n",
    "        \n",
    "        return noisy_batch, masked_indices, p_mask\n",
    "    \n",
    "    def compute_sft_loss(self, \n",
    "                        logits: torch.Tensor,\n",
    "                        input_ids: torch.Tensor,\n",
    "                        masked_indices: torch.Tensor,\n",
    "                        p_mask: torch.Tensor,\n",
    "                        prompt_lengths: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        SFTæå¤±ã®è¨ˆç®—ï¼ˆGUIDELINES.mdæº–æ‹ ï¼‰\n",
    "        \n",
    "        Args:\n",
    "            logits: [batch_size, seq_len, vocab_size] ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›\n",
    "            input_ids: [batch_size, seq_len] æ­£è§£ãƒˆãƒ¼ã‚¯ãƒ³\n",
    "            masked_indices: [batch_size, seq_len] ãƒã‚¹ã‚¯ã•ã‚ŒãŸä½ç½®\n",
    "            p_mask: [batch_size, seq_len] ãƒã‚¹ã‚­ãƒ³ã‚°ç¢ºç‡\n",
    "            prompt_lengths: [batch_size] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·\n",
    "        \n",
    "        Returns:\n",
    "            loss: SFTæå¤±\n",
    "        \"\"\"\n",
    "        b, l = input_ids.shape\n",
    "        device = input_ids.device\n",
    "        \n",
    "        # Answer lengthè¨ˆç®—ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»¥å¤–ã®éƒ¨åˆ†ï¼‰\n",
    "        prompt_mask = torch.arange(l, device=device).expand(b, l) < prompt_lengths.unsqueeze(1)\n",
    "        prompt_mask = prompt_mask.to(torch.int64)\n",
    "        answer_lengths = torch.sum((1 - prompt_mask), dim=-1, keepdim=True)\n",
    "        answer_lengths = answer_lengths.repeat(1, l)\n",
    "        \n",
    "        # ãƒã‚¹ã‚¯ã•ã‚ŒãŸä½ç½®ã§ã®ã¿æå¤±ã‚’è¨ˆç®—\n",
    "        if not masked_indices.any():\n",
    "            return torch.tensor(0.0, device=device, requires_grad=True)\n",
    "        \n",
    "        # ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ï¼ˆé‡ã¿ä»˜ãï¼‰\n",
    "        token_loss = F.cross_entropy(\n",
    "            logits[masked_indices], \n",
    "            input_ids[masked_indices], \n",
    "            reduction='none'\n",
    "        ) / p_mask[masked_indices]\n",
    "        \n",
    "        # Answer lengthæ­£è¦åŒ–\n",
    "        normalized_loss = token_loss / (answer_lengths[masked_indices] + 1e-8)\n",
    "        \n",
    "        # ãƒãƒƒãƒå¹³å‡\n",
    "        ce_loss = torch.sum(normalized_loss) / b\n",
    "        \n",
    "        return ce_loss\n",
    "\n",
    "# SFT Forward Processã®åˆæœŸåŒ–\n",
    "sft_forward = SFTForwardProcess(config)\n",
    "print(\"âœ… SFT Forward Processã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ\")\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ\n",
    "if len(train_dataset) > 0:\n",
    "    test_batch = train_dataset[0]\n",
    "    test_input_ids = test_batch['input_ids'].unsqueeze(0)\n",
    "    test_prompt_length = test_batch['prompt_length'].unsqueeze(0)\n",
    "    \n",
    "    print(f\"\\nğŸ§ª SFTãƒ—ãƒ­ã‚»ã‚¹ãƒ†ã‚¹ãƒˆ:\")\n",
    "    print(f\"  - å…¥åŠ›å½¢çŠ¶: {test_input_ids.shape}\")\n",
    "    print(f\"  - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {test_prompt_length.item()}\")\n",
    "    \n",
    "    # Forward processå®Ÿè¡Œ\n",
    "    noisy_batch, masked_indices, p_mask = sft_forward.forward_process(\n",
    "        test_input_ids, test_prompt_length\n",
    "    )\n",
    "    \n",
    "    print(f\"  - ãƒã‚¹ã‚¯ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³æ•°: {masked_indices.sum().item()}\")\n",
    "    print(f\"  - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã®ãƒã‚¹ã‚¯æ•°: {masked_indices[0, :test_prompt_length.item()].sum().item()} (0ã§ã‚ã‚‹ã¹ã)\")\n",
    "    print(f\"  - å¿œç­”éƒ¨åˆ†ã®ãƒã‚¹ã‚¯æ•°: {masked_indices[0, test_prompt_length.item():].sum().item()}\")\n",
    "    \n",
    "    if masked_indices[0, :test_prompt_length.item()].sum().item() == 0:\n",
    "        print(\"  âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéãƒã‚¹ã‚­ãƒ³ã‚°ãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™\")\n",
    "    else:\n",
    "        print(\"  âŒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ãŒãƒã‚¹ã‚¯ã•ã‚Œã¦ã„ã¾ã™ï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "model_and_trainer_setup"
   },
   "outputs": [],
   "source": "# ğŸ¤– LLaDAãƒ¢ãƒ‡ãƒ«ã¨SFTãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®è¨­å®š\nclass LLaDASFTTrainer(Trainer):\n    \"\"\"LLaDA SFTå°‚ç”¨ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚¯ãƒ©ã‚¹\"\"\"\n    \n    def __init__(self, sft_forward: SFTForwardProcess, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.sft_forward = sft_forward\n        self.vocab_size = len(self.tokenizer)\n    \n    def compute_loss(self, model, inputs, return_outputs=False):\n        \"\"\"SFTæå¤±ã®è¨ˆç®—\"\"\"\n        input_ids = inputs['input_ids']\n        prompt_lengths = inputs['prompt_length']\n        \n        # å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯\n        input_ids = torch.clamp(input_ids, 0, self.vocab_size - 1)\n        \n        try:\n            # SFTãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ—ãƒ­ã‚»ã‚¹\n            noisy_batch, masked_indices, p_mask = self.sft_forward.forward_process(\n                input_ids, prompt_lengths\n            )\n            \n            # ãƒ¢ãƒ‡ãƒ«æ¨è«–\n            outputs = model(input_ids=noisy_batch)\n            logits = outputs.logits\n            \n            # SFTæå¤±è¨ˆç®—\n            loss = self.sft_forward.compute_sft_loss(\n                logits, input_ids, masked_indices, p_mask, prompt_lengths\n            )\n            \n            # NaN/Inf ãƒã‚§ãƒƒã‚¯\n            if torch.isnan(loss) or torch.isinf(loss):\n                print(f\"è­¦å‘Š: ç•°å¸¸ãªæå¤±å€¤ {loss.item()}, ã‚¼ãƒ­æå¤±ã§ä»£æ›¿\")\n                loss = torch.tensor(0.0, device=loss.device, requires_grad=True)\n            \n            return (loss, outputs) if return_outputs else loss\n            \n        except Exception as e:\n            print(f\"æå¤±è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}\")\n            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æå¤±\n            fallback_loss = torch.tensor(1.0, device=input_ids.device, requires_grad=True)\n            return fallback_loss\n\n# ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\nprint(\"ğŸ”§ LLaDAãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\ntry:\n    model = AutoModelForCausalLM.from_pretrained(\n        config.model_name,\n        trust_remote_code=True,\n        torch_dtype=torch.bfloat16 if config.fp16 else torch.float32,\n        device_map='auto' if torch.cuda.is_available() else None\n    )\n    \n    # ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®è¨­å®š\n    if config.gradient_checkpointing:\n        try:\n            model.gradient_checkpointing_enable()\n            print(\"  âœ… ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æœ‰åŠ¹åŒ–\")\n        except Exception as e:\n            print(f\"  âš ï¸  ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆè¨­å®šã‚¨ãƒ©ãƒ¼: {e}\")\n    \n    print(f\"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†\")\n    print(f\"  - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}\")\n    print(f\"  - å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n    \nexcept Exception as e:\n    print(f\"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}\")\n    raise\n\n# ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã®è¨­å®š\ndef sft_data_collator(features):\n    \"\"\"SFTç”¨ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼\"\"\"\n    batch = {}\n    # ãƒãƒƒãƒã®ä½œæˆ\n    for key in features[0].keys():\n        batch[key] = torch.stack([f[key] for f in features])\n    return batch\n\n# å­¦ç¿’å¼•æ•°ã®è¨­å®šï¼ˆWandBå®Œå…¨ç„¡åŠ¹åŒ–ï¼‰\ntraining_args = TrainingArguments(\n    output_dir=config.output_dir,\n    num_train_epochs=config.num_epochs,\n    per_device_train_batch_size=config.batch_size,\n    per_device_eval_batch_size=config.batch_size,\n    gradient_accumulation_steps=config.gradient_accumulation_steps,\n    learning_rate=config.learning_rate,\n    warmup_steps=config.warmup_steps,\n    logging_steps=config.logging_steps,\n    save_steps=config.save_steps,\n    eval_steps=config.eval_steps,\n    evaluation_strategy=\"steps\" if val_dataset else \"no\",\n    save_strategy=\"steps\",\n    fp16=config.fp16,\n    dataloader_num_workers=config.dataloader_num_workers,\n    remove_unused_columns=False,\n    # WandBå®Œå…¨ç„¡åŠ¹åŒ–ã®è¤‡æ•°è¨­å®š\n    report_to=[],  # ç©ºãƒªã‚¹ãƒˆã§å®Œå…¨ç„¡åŠ¹åŒ–\n    logging_first_step=True,\n    disable_tqdm=False,  # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã¯è¡¨ç¤º\n    load_best_model_at_end=True if val_dataset else False,\n    metric_for_best_model=\"eval_loss\" if val_dataset else None,\n    greater_is_better=False,\n    save_total_limit=2,\n    # è¿½åŠ ã®WandBç„¡åŠ¹åŒ–è¨­å®š\n    run_name=None,\n    logging_strategy=\"steps\",\n    log_level=\"error\"  # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’æœ€å°é™ã«\n)\n\n# ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®åˆæœŸåŒ–\ntrainer = LLaDASFTTrainer(\n    sft_forward=sft_forward,\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=sft_data_collator,\n    tokenizer=tokenizer\n)\n\nprint(\"âœ… SFTãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ\")\nprint(f\"ğŸ“Š å­¦ç¿’è¨­å®š:\")\nprint(f\"  - å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º: {config.batch_size * config.gradient_accumulation_steps}\")\nprint(f\"  - ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°: {len(train_dataset) // (config.batch_size * config.gradient_accumulation_steps) * config.num_epochs}\")\nprint(f\"  - å­¦ç¿’ç‡: {config.learning_rate}\")\nprint(f\"  - FP16: {config.fp16}\")\nprint(f\"ğŸš« å¤–éƒ¨ãƒ­ã‚°: å®Œå…¨ç„¡åŠ¹åŒ–æ¸ˆã¿ï¼ˆWandB API keyä¸è¦ï¼‰\")\nprint(\"=\" * 50)\nprint(\"ğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: ã‚»ãƒ«7-8ã‚’å®Ÿè¡Œã—ã¦å­¦ç¿’æ©Ÿèƒ½ã‚’æº–å‚™\")\nprint(\"=\" * 50)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sft_training_execution"
   },
   "outputs": [],
   "source": [
    "# SFTå­¦ç¿’ã®å®Ÿè¡Œ\n",
    "def run_sft_training():\n",
    "    \"\"\"SFTå­¦ç¿’ã®å®Ÿè¡Œé–¢æ•°\"\"\"\n",
    "    print(\"ğŸš€ SFTå­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "    print(f\"ğŸ“Š å­¦ç¿’çµ±è¨ˆ:\")\n",
    "    print(f\"  - å­¦ç¿’ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(train_dataset):,}\")\n",
    "    print(f\"  - æ¤œè¨¼ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(val_dataset) if val_dataset else 0:,}\")\n",
    "    print(f\"  - ã‚¨ãƒãƒƒã‚¯æ•°: {config.num_epochs}\")\n",
    "    print(f\"  - ä½¿ç”¨è¨­å®š: {CONFIG_NAME}\")\n",
    "    \n",
    "    # GPU ãƒ¡ãƒ¢ãƒªã®ã‚¯ãƒªã‚¢\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(f\"  - GPUä½¿ç”¨ãƒ¡ãƒ¢ãƒª: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
    "    \n",
    "    try:\n",
    "        # å­¦ç¿’å‰ã®è©•ä¾¡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "        if val_dataset and len(val_dataset) > 0:\n",
    "            print(\"\\nğŸ“Š å­¦ç¿’å‰è©•ä¾¡ã‚’å®Ÿè¡Œä¸­...\")\n",
    "            try:\n",
    "                initial_eval = trainer.evaluate()\n",
    "                print(f\"  åˆæœŸæå¤±: {initial_eval['eval_loss']:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  åˆæœŸè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        \n",
    "        # ãƒ¡ã‚¤ãƒ³å­¦ç¿’ãƒ«ãƒ¼ãƒ—\n",
    "        print(\"\\nğŸ¯ ãƒ¡ã‚¤ãƒ³å­¦ç¿’ã‚’é–‹å§‹...\")\n",
    "        training_result = trainer.train()\n",
    "        \n",
    "        print(\"\\nâœ… å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "        print(f\"ğŸ“Š å­¦ç¿’çµæœ:\")\n",
    "        print(f\"  - æœ€çµ‚æå¤±: {training_result.training_loss:.4f}\")\n",
    "        print(f\"  - å­¦ç¿’æ™‚é–“: {training_result.metrics['train_runtime']:.2f} ç§’\")\n",
    "        print(f\"  - ã‚µãƒ³ãƒ—ãƒ«/ç§’: {training_result.metrics['train_samples_per_second']:.2f}\")\n",
    "        \n",
    "        # å­¦ç¿’å¾Œã®è©•ä¾¡\n",
    "        if val_dataset and len(val_dataset) > 0:\n",
    "            print(\"\\nğŸ“Š æœ€çµ‚è©•ä¾¡ã‚’å®Ÿè¡Œä¸­...\")\n",
    "            try:\n",
    "                final_eval = trainer.evaluate()\n",
    "                print(f\"  æœ€çµ‚æ¤œè¨¼æå¤±: {final_eval['eval_loss']:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  æœ€çµ‚è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n",
    "        print(\"\\nğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ä¸­...\")\n",
    "        try:\n",
    "            trainer.save_model()\n",
    "            tokenizer.save_pretrained(config.output_dir)\n",
    "            print(f\"  âœ… ãƒ¢ãƒ‡ãƒ«ã‚’ {config.output_dir} ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸  ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        \n",
    "        return training_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        print(\"\\nğŸ”§ ã‚¨ãƒ©ãƒ¼å¯¾å‡¦æ³•:\")\n",
    "        print(\"  1. ã‚ˆã‚Šå°ã•ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è©¦ã™\")\n",
    "        print(\"  2. ã‚ˆã‚ŠçŸ­ã„æœ€å¤§é•·ã‚’è¨­å®šã™ã‚‹\")\n",
    "        print(\"  3. validationè¨­å®šã‚’ä½¿ç”¨ã™ã‚‹\")\n",
    "        print(\"  4. FP16ã‚’æœ‰åŠ¹ã«ã™ã‚‹\")\n",
    "        \n",
    "        # ãƒ¡ãƒ¢ãƒªæƒ…å ±è¡¨ç¤º\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"\\nğŸ“Š GPUæƒ…å ±:\")\n",
    "            print(f\"  - ä½¿ç”¨ãƒ¡ãƒ¢ãƒª: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
    "            print(f\"  - æœ€å¤§ãƒ¡ãƒ¢ãƒª: {torch.cuda.max_memory_allocated() / 1e9:.1f} GB\")\n",
    "            print(f\"  - åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        \n",
    "        raise\n",
    "\n",
    "# å­¦ç¿’å®Ÿè¡Œã®ç¢ºèª\n",
    "print(\"\\nğŸ¯ å­¦ç¿’æº–å‚™å®Œäº†\")\n",
    "print(f\"ç¾åœ¨ã®è¨­å®š: {CONFIG_NAME}\")\n",
    "print(\"\\nå®Ÿè¡Œã™ã‚‹ã«ã¯ä¸‹è¨˜ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:\")\n",
    "print(\"training_result = run_sft_training()\")\n",
    "\n",
    "# è¨­å®šå¤‰æ›´ã®æ¡ˆå†…\n",
    "print(\"\\nâš™ï¸  è¨­å®šå¤‰æ›´æ–¹æ³•:\")\n",
    "print(\"ç•°ãªã‚‹è¨­å®šã§å®Ÿè¡Œã—ãŸã„å ´åˆã¯ã€ä¸Šã®ã‚»ãƒ«ã§ CONFIG_NAME ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„:\")\n",
    "print(\"  - 'validation': æ¤œè¨¼ç”¨ (1K ã‚µãƒ³ãƒ—ãƒ«, 1ã‚¨ãƒãƒƒã‚¯)\")\n",
    "print(\"  - 'medium': ä¸­è¦æ¨¡ (10K ã‚µãƒ³ãƒ—ãƒ«, 2ã‚¨ãƒãƒƒã‚¯)\")\n",
    "print(\"  - 'production': æœ¬ç•ªç”¨ (50K ã‚µãƒ³ãƒ—ãƒ«, 3ã‚¨ãƒãƒƒã‚¯)\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ğŸš€ ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯å­¦ç¿’å®Ÿè¡Œã‚»ãƒ«\nimport time\nfrom IPython.display import display, HTML, clear_output\nimport threading\n\nclass SFTTrainingRunner:\n    \"\"\"SFTå­¦ç¿’ã®ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯å®Ÿè¡Œã‚¯ãƒ©ã‚¹\"\"\"\n    \n    def __init__(self):\n        self.is_training = False\n        self.training_thread = None\n        self.training_logs = []\n        self.current_step = 0\n        self.total_steps = 0\n        self.start_time = None\n        \n    def quick_start_training(self, config_name=\"validation\", auto_evaluate=True):\n        \"\"\"\n        ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯å­¦ç¿’å®Ÿè¡Œ\n        \n        Args:\n            config_name: ä½¿ç”¨ã™ã‚‹è¨­å®š (\"validation\", \"medium\", \"production\")\n            auto_evaluate: å­¦ç¿’å¾Œã«è‡ªå‹•è©•ä¾¡ã™ã‚‹ã‹ã©ã†ã‹\n        \"\"\"\n        print(\"ğŸš€ ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯å­¦ç¿’å®Ÿè¡Œã‚’é–‹å§‹ã—ã¾ã™...\")\n        \n        # è¨­å®šã®å†é¸æŠ\n        global config, CONFIG_NAME\n        CONFIG_NAME = config_name\n        config = select_config(config_name)\n        \n        # å­¦ç¿’çµ±è¨ˆã®è¡¨ç¤º\n        print(f\"\\nğŸ“Š å­¦ç¿’è¨­å®š: {config_name}\")\n        print(f\"  â”œâ”€ ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {config.dataset_size:,} ã‚µãƒ³ãƒ—ãƒ«\")\n        print(f\"  â”œâ”€ ãƒãƒƒãƒã‚µã‚¤ã‚º: {config.batch_size}\")\n        print(f\"  â”œâ”€ ã‚¨ãƒãƒƒã‚¯æ•°: {config.num_epochs}\")\n        print(f\"  â”œâ”€ å­¦ç¿’ç‡: {config.learning_rate}\")\n        print(f\"  â””â”€ æ¨å®šæ™‚é–“: {self._estimate_training_time()} åˆ†\")\n        \n        # GPU ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯\n        if torch.cuda.is_available():\n            self._check_gpu_memory()\n        \n        # å­¦ç¿’å®Ÿè¡Œ\n        try:\n            print(\"\\nâ° 5ç§’å¾Œã«å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...\")\n            time.sleep(5)\n            \n            # å­¦ç¿’å®Ÿè¡Œ\n            self.start_time = time.time()\n            print(\"ğŸ¯ å­¦ç¿’é–‹å§‹ï¼\")\n            training_result = run_sft_training()\n            \n            # å­¦ç¿’æ™‚é–“ã®è¨ˆç®—\n            training_duration = time.time() - self.start_time\n            print(f\"\\nâ±ï¸  ç·å­¦ç¿’æ™‚é–“: {training_duration/60:.1f} åˆ†\")\n            \n            # è‡ªå‹•è©•ä¾¡\n            if auto_evaluate:\n                print(\"\\nğŸ” è‡ªå‹•è©•ä¾¡ã‚’é–‹å§‹...\")\n                time.sleep(2)\n                evaluate_sft_model()\n                memory_usage_summary()\n            \n            # æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n            print(\"\\nğŸ‰ å­¦ç¿’ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼\")\n            self._display_success_summary(training_result)\n            \n            return training_result\n            \n        except Exception as e:\n            print(f\"\\nâŒ å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}\")\n            self._display_error_troubleshooting()\n            raise\n    \n    def _estimate_training_time(self):\n        \"\"\"å­¦ç¿’æ™‚é–“ã‚’æ¨å®š\"\"\"\n        # åŸºæœ¬çš„ãªæ¨å®šï¼ˆçµŒé¨“å€¤ãƒ™ãƒ¼ã‚¹ï¼‰\n        base_time_per_sample = 0.1  # ç§’/ã‚µãƒ³ãƒ—ãƒ«\n        total_samples = config.dataset_size * config.num_epochs\n        estimated_seconds = total_samples * base_time_per_sample / config.batch_size\n        return max(1, int(estimated_seconds / 60))\n    \n    def _check_gpu_memory(self):\n        \"\"\"GPU ãƒ¡ãƒ¢ãƒªã‚’ãƒã‚§ãƒƒã‚¯\"\"\"\n        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n        current_memory = torch.cuda.memory_allocated() / 1e9\n        \n        estimated_usage = 8 + config.batch_size * 2  # GB\n        \n        print(f\"\\nğŸ–¥ï¸  GPU ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯:\")\n        print(f\"  â”œâ”€ ç·å®¹é‡: {total_memory:.1f} GB\")\n        print(f\"  â”œâ”€ ç¾åœ¨ä½¿ç”¨é‡: {current_memory:.1f} GB\")\n        print(f\"  â”œâ”€ æ¨å®šå¿…è¦é‡: {estimated_usage:.1f} GB\")\n        \n        if estimated_usage > total_memory * 0.9:\n            print(f\"  â””â”€ âš ï¸  ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯èƒ½æ€§ â†’ ã‚ˆã‚Šå°ã•ãªè¨­å®šã‚’æ¨å¥¨\")\n        else:\n            print(f\"  â””â”€ âœ… ãƒ¡ãƒ¢ãƒªå®¹é‡OK\")\n    \n    def _display_success_summary(self, training_result):\n        \"\"\"æˆåŠŸæ™‚ã®ã‚µãƒãƒªãƒ¼è¡¨ç¤º\"\"\"\n        print(\"\\\\n\" + \"=\"*50)\n        print(\"ğŸŠ SFTå­¦ç¿’å®Œäº†ã‚µãƒãƒªãƒ¼\")\n        print(\"=\"*50)\n        print(f\"âœ… è¨­å®š: {CONFIG_NAME}\")\n        print(f\"âœ… æœ€çµ‚æå¤±: {training_result.training_loss:.4f}\")\n        print(f\"âœ… å­¦ç¿’æ™‚é–“: {training_result.metrics['train_runtime']:.1f} ç§’\")\n        print(f\"âœ… ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ: {config.output_dir}\")\n        print(\"=\"*50)\n    \n    def _display_error_troubleshooting(self):\n        \"\"\"ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\"\"\"\n        print(\"\\\\n\" + \"=\"*50)\n        print(\"ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\")\n        print(\"=\"*50)\n        print(\"ğŸ’¡ ä»¥ä¸‹ã‚’è©¦ã—ã¦ãã ã•ã„:\")\n        print(\"  1ï¸âƒ£  ã‚ˆã‚Šå°ã•ãªè¨­å®šã‚’ä½¿ç”¨: quick_start_training('validation')\")\n        print(\"  2ï¸âƒ£  ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ã: config.batch_size = 1\")\n        print(\"  3ï¸âƒ£  æœ€å¤§é•·ã‚’çŸ­ã: config.max_length = 512\")\n        print(\"  4ï¸âƒ£  GPU ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢: torch.cuda.empty_cache()\")\n        print(\"=\"*50)\n\n# SFTTrainingRunnerã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–\ntrainer_runner = SFTTrainingRunner()\n\n# ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯å®Ÿè¡Œãƒœã‚¿ãƒ³\nprint(\"ğŸ¯ ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯å­¦ç¿’å®Ÿè¡Œã®æº–å‚™å®Œäº†\")\nprint(\"\\\\n\" + \"=\"*60)\nprint(\"ğŸš€ ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§å³åº§ã«å­¦ç¿’ã‚’é–‹å§‹ã§ãã¾ã™:\")\nprint(\"=\"*60)\nprint()\nprint(\"ğŸ’¡ æ¤œè¨¼ç”¨è¨­å®šï¼ˆæ¨å¥¨ãƒ»åˆå›ï¼‰:\")\nprint(\"   trainer_runner.quick_start_training('validation')\")\nprint()\nprint(\"ğŸ’ª ä¸­è¦æ¨¡è¨­å®š:\")\nprint(\"   trainer_runner.quick_start_training('medium')\")\nprint()\nprint(\"ğŸ”¥ æœ¬æ ¼çš„è¨­å®š:\")\nprint(\"   trainer_runner.quick_start_training('production')\")\nprint()\nprint(\"=\"*60)\nprint(\"âš™ï¸  ã‚ªãƒ—ã‚·ãƒ§ãƒ³:\")\nprint(\"   - auto_evaluate=False ã§è©•ä¾¡ã‚’ã‚¹ã‚­ãƒƒãƒ—\")\nprint(\"   - ä¾‹: trainer_runner.quick_start_training('validation', auto_evaluate=False)\")\nprint(\"=\"*60)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ğŸ“Š ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å­¦ç¿’é€²æ—ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom datetime import datetime\nimport pandas as pd\n\nclass TrainingMonitor:\n    \"\"\"å­¦ç¿’é€²æ—ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ç›£è¦–ã™ã‚‹ã‚¯ãƒ©ã‚¹\"\"\"\n    \n    def __init__(self):\n        self.training_history = {\n            'step': [],\n            'loss': [],\n            'eval_loss': [],\n            'learning_rate': [],\n            'timestamp': [],\n            'gpu_memory': [],\n            'samples_per_second': []\n        }\n        self.fig = None\n        self.axes = None\n        \n    def start_monitoring(self):\n        \"\"\"ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚’é–‹å§‹\"\"\"\n        plt.style.use('seaborn-v0_8')\n        self.fig, self.axes = plt.subplots(2, 2, figsize=(15, 10))\n        self.fig.suptitle('ğŸ”„ LLaDA SFTå­¦ç¿’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°', fontsize=16, fontweight='bold')\n        \n        # å„ã‚°ãƒ©ãƒ•ã®åˆæœŸè¨­å®š\n        self.axes[0, 0].set_title('ğŸ“‰ å­¦ç¿’æå¤±')\n        self.axes[0, 0].set_xlabel('ã‚¹ãƒ†ãƒƒãƒ—')\n        self.axes[0, 0].set_ylabel('æå¤±')\n        self.axes[0, 0].grid(True, alpha=0.3)\n        \n        self.axes[0, 1].set_title('ğŸ–¥ï¸ GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡')\n        self.axes[0, 1].set_xlabel('ã‚¹ãƒ†ãƒƒãƒ—')\n        self.axes[0, 1].set_ylabel('ãƒ¡ãƒ¢ãƒª (GB)')\n        self.axes[0, 1].grid(True, alpha=0.3)\n        \n        self.axes[1, 0].set_title('âš¡ å­¦ç¿’é€Ÿåº¦')\n        self.axes[1, 0].set_xlabel('ã‚¹ãƒ†ãƒƒãƒ—')\n        self.axes[1, 0].set_ylabel('ã‚µãƒ³ãƒ—ãƒ«/ç§’')\n        self.axes[1, 0].grid(True, alpha=0.3)\n        \n        self.axes[1, 1].set_title('ğŸ“Š å­¦ç¿’ç‡')\n        self.axes[1, 1].set_xlabel('ã‚¹ãƒ†ãƒƒãƒ—')\n        self.axes[1, 1].set_ylabel('å­¦ç¿’ç‡')\n        self.axes[1, 1].grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        return self.fig\n    \n    def update_metrics(self, step, loss=None, eval_loss=None, learning_rate=None, samples_per_second=None):\n        \"\"\"ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ›´æ–°\"\"\"\n        self.training_history['step'].append(step)\n        self.training_history['timestamp'].append(datetime.now())\n        \n        # GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—\n        if torch.cuda.is_available():\n            gpu_memory = torch.cuda.memory_allocated() / 1e9\n            self.training_history['gpu_memory'].append(gpu_memory)\n        else:\n            self.training_history['gpu_memory'].append(0)\n        \n        # ãã®ä»–ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹\n        self.training_history['loss'].append(loss if loss is not None else float('nan'))\n        self.training_history['eval_loss'].append(eval_loss if eval_loss is not None else float('nan'))\n        self.training_history['learning_rate'].append(learning_rate if learning_rate is not None else float('nan'))\n        self.training_history['samples_per_second'].append(samples_per_second if samples_per_second is not None else float('nan'))\n    \n    def plot_current_progress(self):\n        \"\"\"ç¾åœ¨ã®é€²æ—ã‚’ãƒ—ãƒ­ãƒƒãƒˆ\"\"\"\n        if not self.training_history['step']:\n            print(\"âš ï¸ ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“\")\n            return\n        \n        # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢\n        for ax in self.axes.flat:\n            ax.clear()\n        \n        steps = self.training_history['step']\n        \n        # æå¤±ã®ãƒ—ãƒ­ãƒƒãƒˆ\n        if any(not pd.isna(x) for x in self.training_history['loss']):\n            valid_losses = [(s, l) for s, l in zip(steps, self.training_history['loss']) if not pd.isna(l)]\n            if valid_losses:\n                s_loss, losses = zip(*valid_losses)\n                self.axes[0, 0].plot(s_loss, losses, 'b-', linewidth=2, label='å­¦ç¿’æå¤±')\n        \n        if any(not pd.isna(x) for x in self.training_history['eval_loss']):\n            valid_eval_losses = [(s, l) for s, l in zip(steps, self.training_history['eval_loss']) if not pd.isna(l)]\n            if valid_eval_losses:\n                s_eval, eval_losses = zip(*valid_eval_losses)\n                self.axes[0, 0].plot(s_eval, eval_losses, 'r--', linewidth=2, label='æ¤œè¨¼æå¤±')\n        \n        self.axes[0, 0].set_title('ğŸ“‰ å­¦ç¿’æå¤±')\n        self.axes[0, 0].set_xlabel('ã‚¹ãƒ†ãƒƒãƒ—')\n        self.axes[0, 0].set_ylabel('æå¤±')\n        self.axes[0, 0].legend()\n        self.axes[0, 0].grid(True, alpha=0.3)\n        \n        # GPU ãƒ¡ãƒ¢ãƒªã®ãƒ—ãƒ­ãƒƒãƒˆ\n        self.axes[0, 1].plot(steps, self.training_history['gpu_memory'], 'g-', linewidth=2)\n        self.axes[0, 1].set_title('ğŸ–¥ï¸ GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡')\n        self.axes[0, 1].set_xlabel('ã‚¹ãƒ†ãƒƒãƒ—')\n        self.axes[0, 1].set_ylabel('ãƒ¡ãƒ¢ãƒª (GB)')\n        self.axes[0, 1].grid(True, alpha=0.3)\n        \n        # å­¦ç¿’é€Ÿåº¦ã®ãƒ—ãƒ­ãƒƒãƒˆ\n        if any(not pd.isna(x) for x in self.training_history['samples_per_second']):\n            valid_speeds = [(s, sp) for s, sp in zip(steps, self.training_history['samples_per_second']) if not pd.isna(sp)]\n            if valid_speeds:\n                s_speed, speeds = zip(*valid_speeds)\n                self.axes[1, 0].plot(s_speed, speeds, 'orange', linewidth=2)\n        self.axes[1, 0].set_title('âš¡ å­¦ç¿’é€Ÿåº¦')\n        self.axes[1, 0].set_xlabel('ã‚¹ãƒ†ãƒƒãƒ—')\n        self.axes[1, 0].set_ylabel('ã‚µãƒ³ãƒ—ãƒ«/ç§’')\n        self.axes[1, 0].grid(True, alpha=0.3)\n        \n        # å­¦ç¿’ç‡ã®ãƒ—ãƒ­ãƒƒãƒˆ\n        if any(not pd.isna(x) for x in self.training_history['learning_rate']):\n            valid_lrs = [(s, lr) for s, lr in zip(steps, self.training_history['learning_rate']) if not pd.isna(lr)]\n            if valid_lrs:\n                s_lr, lrs = zip(*valid_lrs)\n                self.axes[1, 1].plot(s_lr, lrs, 'purple', linewidth=2)\n        self.axes[1, 1].set_title('ğŸ“Š å­¦ç¿’ç‡')\n        self.axes[1, 1].set_xlabel('ã‚¹ãƒ†ãƒƒãƒ—')\n        self.axes[1, 1].set_ylabel('å­¦ç¿’ç‡')\n        self.axes[1, 1].grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.show()\n    \n    def generate_training_report(self):\n        \"\"\"å­¦ç¿’ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ\"\"\"\n        if not self.training_history['step']:\n            print(\"âš ï¸ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆç”¨ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“\")\n            return\n        \n        print(\"\\\\n\" + \"=\"*60)\n        print(\"ğŸ“‹ SFTå­¦ç¿’ãƒ¬ãƒãƒ¼ãƒˆ\")\n        print(\"=\"*60)\n        \n        # åŸºæœ¬çµ±è¨ˆ\n        total_steps = len(self.training_history['step'])\n        final_loss = [l for l in self.training_history['loss'] if not pd.isna(l)]\n        final_loss = final_loss[-1] if final_loss else \"N/A\"\n        \n        max_gpu_memory = max(self.training_history['gpu_memory'])\n        avg_speed = [s for s in self.training_history['samples_per_second'] if not pd.isna(s)]\n        avg_speed = sum(avg_speed) / len(avg_speed) if avg_speed else \"N/A\"\n        \n        print(f\"ğŸ“Š åŸºæœ¬çµ±è¨ˆ:\")\n        print(f\"  â”œâ”€ ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°: {total_steps}\")\n        print(f\"  â”œâ”€ æœ€çµ‚æå¤±: {final_loss}\")\n        print(f\"  â”œâ”€ æœ€å¤§GPUä½¿ç”¨é‡: {max_gpu_memory:.1f} GB\")\n        print(f\"  â””â”€ å¹³å‡å­¦ç¿’é€Ÿåº¦: {avg_speed if avg_speed != 'N/A' else 'N/A'} ã‚µãƒ³ãƒ—ãƒ«/ç§’\")\n        \n        # æ™‚é–“çµ±è¨ˆ\n        if len(self.training_history['timestamp']) >= 2:\n            start_time = self.training_history['timestamp'][0]\n            end_time = self.training_history['timestamp'][-1]\n            duration = (end_time - start_time).total_seconds()\n            \n            print(f\"\\\\nâ±ï¸  æ™‚é–“çµ±è¨ˆ:\")\n            print(f\"  â”œâ”€ é–‹å§‹æ™‚åˆ»: {start_time.strftime('%H:%M:%S')}\")\n            print(f\"  â”œâ”€ çµ‚äº†æ™‚åˆ»: {end_time.strftime('%H:%M:%S')}\")\n            print(f\"  â””â”€ å­¦ç¿’æ™‚é–“: {duration/60:.1f} åˆ†\")\n        \n        print(\"=\"*60)\n\n# ãƒ¢ãƒ‹ã‚¿ãƒ¼ã®åˆæœŸåŒ–\ntraining_monitor = TrainingMonitor()\n\n# ãƒ‡ãƒ¢ç”¨ã®ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º\ndef demo_training_progress():\n    \"\"\"ãƒ‡ãƒ¢ç”¨ã®å­¦ç¿’é€²æ—è¡¨ç¤º\"\"\"\n    print(\"ğŸ”„ ãƒ‡ãƒ¢: å­¦ç¿’é€²æ—ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°\")\n    print(\"ï¼ˆå®Ÿéš›ã®å­¦ç¿’æ™‚ã«ã¯ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§æ›´æ–°ã•ã‚Œã¾ã™ï¼‰\")\n    \n    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ãƒ‡ãƒ¢\n    import numpy as np\n    \n    steps = range(0, 100, 10)\n    losses = [2.5 * np.exp(-s/50) + 0.5 + np.random.normal(0, 0.1) for s in steps]\n    \n    for i, (step, loss) in enumerate(zip(steps, losses)):\n        training_monitor.update_metrics(\n            step=step,\n            loss=loss,\n            learning_rate=2e-5 * (1 - step/100),\n            samples_per_second=15 + np.random.normal(0, 2)\n        )\n    \n    # ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤º\n    fig = training_monitor.start_monitoring()\n    training_monitor.plot_current_progress()\n    training_monitor.generate_training_report()\n\nprint(\"ğŸ“Š å­¦ç¿’é€²æ—ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°æº–å‚™å®Œäº†\")\nprint(\"\\\\nä½¿ç”¨æ–¹æ³•:\")\nprint(\"1. demo_training_progress()  # ãƒ‡ãƒ¢è¡¨ç¤º\")\nprint(\"2. training_monitor.start_monitoring()  # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–é–‹å§‹\")\nprint(\"3. å­¦ç¿’ä¸­ã« training_monitor.update_metrics() ã§æ›´æ–°\")\nprint(\"4. training_monitor.generate_training_report()  # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ğŸ“Š ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…å®Œçµå‹é€²æ—ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°\nfrom IPython.display import clear_output\n\nclass NotebookTrainingMonitor:\n    \"\"\"ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…ã§å®Œçµã™ã‚‹å­¦ç¿’é€²æ—ç›£è¦–ã‚¯ãƒ©ã‚¹\"\"\"\n    \n    def __init__(self):\n        self.training_history = {\n            'step': [],\n            'loss': [],\n            'eval_loss': [],\n            'learning_rate': [],\n            'timestamp': [],\n            'gpu_memory': [],\n            'samples_per_second': []\n        }\n        self.start_time = None\n        \n    def start_monitoring(self):\n        \"\"\"ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚’é–‹å§‹\"\"\"\n        self.start_time = datetime.now()\n        print(\"ğŸ”„ å­¦ç¿’é€²æ—ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°é–‹å§‹\")\n        print(f\"é–‹å§‹æ™‚åˆ»: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n        \n    def update_metrics(self, step, loss=None, eval_loss=None, learning_rate=None, samples_per_second=None):\n        \"\"\"ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ›´æ–°ã—ã¦ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤º\"\"\"\n        self.training_history['step'].append(step)\n        self.training_history['timestamp'].append(datetime.now())\n        \n        # GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—\n        if torch.cuda.is_available():\n            gpu_memory = torch.cuda.memory_allocated() / 1e9\n            self.training_history['gpu_memory'].append(gpu_memory)\n        else:\n            self.training_history['gpu_memory'].append(0)\n        \n        # ãã®ä»–ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹\n        self.training_history['loss'].append(loss if loss is not None else np.nan)\n        self.training_history['eval_loss'].append(eval_loss if eval_loss is not None else np.nan)\n        self.training_history['learning_rate'].append(learning_rate if learning_rate is not None else np.nan)\n        self.training_history['samples_per_second'].append(samples_per_second if samples_per_second is not None else np.nan)\n        \n        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤ºæ›´æ–°ï¼ˆ5ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ï¼‰\n        if step % 5 == 0:\n            self._update_display()\n    \n    def _update_display(self):\n        \"\"\"é€²æ—è¡¨ç¤ºã‚’æ›´æ–°\"\"\"\n        if len(self.training_history['step']) == 0:\n            return\n        \n        current_time = datetime.now()\n        elapsed = (current_time - self.start_time).total_seconds() if self.start_time else 0\n        \n        # æœ€æ–°ã®å€¤ã‚’å–å¾—\n        latest_step = self.training_history['step'][-1]\n        latest_loss = self.training_history['loss'][-1]\n        latest_gpu = self.training_history['gpu_memory'][-1]\n        latest_speed = self.training_history['samples_per_second'][-1]\n        \n        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«è¡¨ç¤ºæ›´æ–°\n        print(f\"\\\\rğŸ“Š ã‚¹ãƒ†ãƒƒãƒ— {latest_step} | \", end=\"\")\n        if not np.isnan(latest_loss):\n            print(f\"æå¤±: {latest_loss:.4f} | \", end=\"\")\n        if latest_gpu > 0:\n            print(f\"GPU: {latest_gpu:.1f}GB | \", end=\"\")\n        if not np.isnan(latest_speed):\n            print(f\"é€Ÿåº¦: {latest_speed:.1f} samples/s | \", end=\"\")\n        print(f\"çµŒé: {elapsed/60:.1f}åˆ†\", end=\"\", flush=True)\n    \n    def plot_training_progress(self):\n        \"\"\"å­¦ç¿’é€²æ—ã‚’ãƒ—ãƒ­ãƒƒãƒˆï¼ˆãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…è¡¨ç¤ºï¼‰\"\"\"\n        if not self.training_history['step']:\n            print(\"âš ï¸ ãƒ—ãƒ­ãƒƒãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“\")\n            return\n        \n        # 2x2ã®ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ\n        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n        fig.suptitle('ğŸ”„ LLaDA SFTå­¦ç¿’é€²æ—', fontsize=16, fontweight='bold')\n        \n        steps = self.training_history['step']\n        \n        # æå¤±ãƒ—ãƒ­ãƒƒãƒˆ\n        valid_losses = [(s, l) for s, l in zip(steps, self.training_history['loss']) if not np.isnan(l)]\n        if valid_losses:\n            s_loss, losses = zip(*valid_losses)\n            axes[0, 0].plot(s_loss, losses, 'b-', linewidth=2, label='å­¦ç¿’æå¤±')\n        \n        valid_eval_losses = [(s, l) for s, l in zip(steps, self.training_history['eval_loss']) if not np.isnan(l)]\n        if valid_eval_losses:\n            s_eval, eval_losses = zip(*valid_eval_losses)\n            axes[0, 0].plot(s_eval, eval_losses, 'r--', linewidth=2, label='æ¤œè¨¼æå¤±')\n        \n        axes[0, 0].set_title('ğŸ“‰ å­¦ç¿’æå¤±')\n        axes[0, 0].set_xlabel('ã‚¹ãƒ†ãƒƒãƒ—')\n        axes[0, 0].set_ylabel('æå¤±')\n        axes[0, 0].legend()\n        axes[0, 0].grid(True, alpha=0.3)\n        \n        # GPU ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒƒãƒˆ\n        axes[0, 1].plot(steps, self.training_history['gpu_memory'], 'g-', linewidth=2)\n        axes[0, 1].set_title('ğŸ–¥ï¸ GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡')\n        axes[0, 1].set_xlabel('ã‚¹ãƒ†ãƒƒãƒ—')\n        axes[0, 1].set_ylabel('ãƒ¡ãƒ¢ãƒª (GB)')\n        axes[0, 1].grid(True, alpha=0.3)\n        \n        # å­¦ç¿’é€Ÿåº¦ãƒ—ãƒ­ãƒƒãƒˆ\n        valid_speeds = [(s, sp) for s, sp in zip(steps, self.training_history['samples_per_second']) if not np.isnan(sp)]\n        if valid_speeds:\n            s_speed, speeds = zip(*valid_speeds)\n            axes[1, 0].plot(s_speed, speeds, 'orange', linewidth=2)\n        axes[1, 0].set_title('âš¡ å­¦ç¿’é€Ÿåº¦')\n        axes[1, 0].set_xlabel('ã‚¹ãƒ†ãƒƒãƒ—')\n        axes[1, 0].set_ylabel('ã‚µãƒ³ãƒ—ãƒ«/ç§’')\n        axes[1, 0].grid(True, alpha=0.3)\n        \n        # å­¦ç¿’ç‡ãƒ—ãƒ­ãƒƒãƒˆ\n        valid_lrs = [(s, lr) for s, lr in zip(steps, self.training_history['learning_rate']) if not np.isnan(lr)]\n        if valid_lrs:\n            s_lr, lrs = zip(*valid_lrs)\n            axes[1, 1].plot(s_lr, lrs, 'purple', linewidth=2)\n        axes[1, 1].set_title('ğŸ“Š å­¦ç¿’ç‡')\n        axes[1, 1].set_xlabel('ã‚¹ãƒ†ãƒƒãƒ—')\n        axes[1, 1].set_ylabel('å­¦ç¿’ç‡')\n        axes[1, 1].grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.show()\n    \n    def generate_final_report(self):\n        \"\"\"æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆï¼ˆãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…è¡¨ç¤ºï¼‰\"\"\"\n        if not self.training_history['step']:\n            print(\"âš ï¸ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆç”¨ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“\")\n            return\n        \n        print(\"\\\\n\" + \"=\"*60)\n        print(\"ğŸ“‹ SFTå­¦ç¿’æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ\")\n        print(\"=\"*60)\n        \n        # åŸºæœ¬çµ±è¨ˆ\n        total_steps = len(self.training_history['step'])\n        final_loss = [l for l in self.training_history['loss'] if not np.isnan(l)]\n        final_loss = final_loss[-1] if final_loss else \"N/A\"\n        \n        max_gpu_memory = max(self.training_history['gpu_memory']) if self.training_history['gpu_memory'] else 0\n        avg_speed = [s for s in self.training_history['samples_per_second'] if not np.isnan(s)]\n        avg_speed = sum(avg_speed) / len(avg_speed) if avg_speed else \"N/A\"\n        \n        print(f\"ğŸ“Š åŸºæœ¬çµ±è¨ˆ:\")\n        print(f\"  â”œâ”€ ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°: {total_steps}\")\n        print(f\"  â”œâ”€ æœ€çµ‚æå¤±: {final_loss}\")\n        print(f\"  â”œâ”€ æœ€å¤§GPUä½¿ç”¨é‡: {max_gpu_memory:.1f} GB\")\n        print(f\"  â””â”€ å¹³å‡å­¦ç¿’é€Ÿåº¦: {avg_speed if avg_speed != 'N/A' else 'N/A'} ã‚µãƒ³ãƒ—ãƒ«/ç§’\")\n        \n        # æ™‚é–“çµ±è¨ˆ\n        if len(self.training_history['timestamp']) >= 2:\n            start_time = self.training_history['timestamp'][0]\n            end_time = self.training_history['timestamp'][-1]\n            duration = (end_time - start_time).total_seconds()\n            \n            print(f\"\\\\nâ±ï¸  æ™‚é–“çµ±è¨ˆ:\")\n            print(f\"  â”œâ”€ é–‹å§‹æ™‚åˆ»: {start_time.strftime('%H:%M:%S')}\")\n            print(f\"  â”œâ”€ çµ‚äº†æ™‚åˆ»: {end_time.strftime('%H:%M:%S')}\")\n            print(f\"  â””â”€ å­¦ç¿’æ™‚é–“: {duration/60:.1f} åˆ†\")\n        \n        # è¨­å®šã‚µãƒãƒªãƒ¼\n        print(f\"\\\\nâš™ï¸  è¨­å®šã‚µãƒãƒªãƒ¼:\")\n        print(f\"  â”œâ”€ è¨­å®š: {CONFIG_NAME}\")\n        print(f\"  â”œâ”€ ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {config.dataset_size:,}\")\n        print(f\"  â”œâ”€ ãƒãƒƒãƒã‚µã‚¤ã‚º: {config.batch_size}\")\n        print(f\"  â”œâ”€ ã‚¨ãƒãƒƒã‚¯æ•°: {config.num_epochs}\")\n        print(f\"  â””â”€ å­¦ç¿’ç‡: {config.learning_rate}\")\n        \n        print(\"=\"*60)\n        print(\"âœ… ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†\")\n\n# ãƒ¢ãƒ‹ã‚¿ãƒ¼ã®åˆæœŸåŒ–\nnotebook_monitor = NotebookTrainingMonitor()\n\n# ãƒ‡ãƒ¢ç”¨ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º\ndef demo_notebook_monitoring():\n    \"\"\"ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…ç›£è¦–ã®ãƒ‡ãƒ¢\"\"\"\n    print(\"ğŸ”„ ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…ç›£è¦–ãƒ‡ãƒ¢é–‹å§‹\")\n    \n    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ãƒ‡ãƒ¢\n    steps = range(0, 50, 5)\n    losses = [2.5 * np.exp(-s/25) + 0.5 + np.random.normal(0, 0.1) for s in steps]\n    \n    notebook_monitor.start_monitoring()\n    \n    for i, (step, loss) in enumerate(zip(steps, losses)):\n        notebook_monitor.update_metrics(\n            step=step,\n            loss=loss,\n            learning_rate=2e-5 * (1 - step/50),\n            samples_per_second=15 + np.random.normal(0, 2)\n        )\n        time.sleep(0.1)  # ãƒ‡ãƒ¢ç”¨é…å»¶\n    \n    print(\"\\\\n\\\\nğŸ“Š ã‚°ãƒ©ãƒ•è¡¨ç¤º:\")\n    notebook_monitor.plot_training_progress()\n    notebook_monitor.generate_final_report()\n\nprint(\"ğŸ“Š ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ æº–å‚™å®Œäº†\")\nprint(\"\\\\nä½¿ç”¨æ–¹æ³•:\")\nprint(\"1. demo_notebook_monitoring()  # ãƒ‡ãƒ¢è¡¨ç¤º\")\nprint(\"2. notebook_monitor.start_monitoring()  # å®Ÿéš›ã®ç›£è¦–é–‹å§‹\")\nprint(\"3. å­¦ç¿’ä¸­ã«è‡ªå‹•æ›´æ–°\")\nprint(\"4. notebook_monitor.generate_final_report()  # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ\")\nprint(\"=\" * 50)\nprint(\"ğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: ã‚»ãƒ«11ã‚’å®Ÿè¡Œã—ã¦å®Ÿè¡Œã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æº–å‚™\")\nprint(\"=\" * 50)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# ğŸ“š FAQãƒ»ãƒ˜ãƒ«ãƒ—ã‚»ã‚¯ã‚·ãƒ§ãƒ³\n\n## â“ ã‚ˆãã‚ã‚‹è³ªå•\n\n### ğŸš€ å®Ÿè¡Œé–¢é€£\n\n**Q: åˆå›å®Ÿè¡Œã§ä½•ã‚’ã™ã‚Œã°ã„ã„ã§ã™ã‹ï¼Ÿ**\nA: ä»¥ä¸‹ã‚’é †ç•ªã«å®Ÿè¡Œï¼š\n1. ã‚»ãƒ«1-8ã‚’ä¸Šã‹ã‚‰é †ç•ªã«å®Ÿè¡Œ\n2. `result = notebook_runner.complete_auto_training()` ã‚’å®Ÿè¡Œ\n\n**Q: ã©ã®è¨­å®šã‚’é¸ã¹ã°ã„ã„ã§ã™ã‹ï¼Ÿ**\nA: \n- åˆå¿ƒè€…ãƒ»æ¤œè¨¼ç”¨: `validation` (1K ã‚µãƒ³ãƒ—ãƒ«, ç´„5åˆ†)\n- ä¸­ç´šè€…ãƒ»ãƒãƒ©ãƒ³ã‚¹: `medium` (10K ã‚µãƒ³ãƒ—ãƒ«, ç´„30åˆ†)  \n- ä¸Šç´šè€…ãƒ»æœ¬æ ¼å­¦ç¿’: `production` (50K ã‚µãƒ³ãƒ—ãƒ«, ç´„2æ™‚é–“)\n\n**Q: å­¦ç¿’ãŒé€”ä¸­ã§æ­¢ã¾ã‚Šã¾ã—ãŸ**\nA: ä»¥ä¸‹ã‚’è©¦ã—ã¦ãã ã•ã„ï¼š\n```python\n# GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢\ntorch.cuda.empty_cache()\ngc.collect()\n\n# ã‚ˆã‚Šå°ã•ãªè¨­å®šã§å†å®Ÿè¡Œ\nresult = quick_training()\n```\n\n### ğŸ’¾ ãƒ¡ãƒ¢ãƒªé–¢é€£\n\n**Q: GPU ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¾ã™**\nA: ä»¥ä¸‹ã®é †ç•ªã§å¯¾å‡¦ï¼š\n1. `torch.cuda.empty_cache()` å®Ÿè¡Œ\n2. ã‚ˆã‚Šå°ã•ãªãƒãƒƒãƒã‚µã‚¤ã‚º: `config.batch_size = 1`\n3. validationè¨­å®šã‚’ä½¿ç”¨: `quick_training()`\n4. ã‚«ã‚¹ã‚¿ãƒ è¨­å®š: `custom_quick_training(dataset_size=500, batch_size=1)`\n\n**Q: ã©ã®ãã‚‰ã„ã®GPUãƒ¡ãƒ¢ãƒªãŒå¿…è¦ã§ã™ã‹ï¼Ÿ**\nA:\n- validation: 8GBä»¥ä¸Šæ¨å¥¨\n- medium: 12GBä»¥ä¸Šæ¨å¥¨  \n- production: 16GBä»¥ä¸Šæ¨å¥¨\n\n### ğŸ“Š çµæœé–¢é€£\n\n**Q: å­¦ç¿’çµæœã¯ã©ã“ã«ä¿å­˜ã•ã‚Œã¾ã™ã‹ï¼Ÿ**\nA: `./llada_sft_output/` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«è‡ªå‹•ä¿å­˜ã•ã‚Œã¾ã™ã€‚\n\n**Q: å­¦ç¿’ã®é€²æ—ã¯ã©ã“ã§ç¢ºèªã§ãã¾ã™ã‹ï¼Ÿ**\nA: ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…ã«è‡ªå‹•è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚è¿½åŠ ã§ `notebook_monitor.plot_training_progress()` ã§è©³ç´°ã‚°ãƒ©ãƒ•è¡¨ç¤ºå¯èƒ½ã€‚\n\n**Q: ç”Ÿæˆå“è³ªã‚’ç¢ºèªã—ãŸã„ã§ã™**\nA: å­¦ç¿’å®Œäº†å¾Œã«è‡ªå‹•è©•ä¾¡ãŒå®Ÿè¡Œã•ã‚Œã¾ã™ã€‚æ‰‹å‹•å®Ÿè¡Œã¯ï¼š\n```python\nevaluate_sft_model()\n```\n\n### ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n\n**Q: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“**\nA: è‡ªå‹•ã§ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã«åˆ‡ã‚Šæ›¿ã‚ã‚Šã¾ã™ã€‚å•é¡Œã‚ã‚Šã¾ã›ã‚“ã€‚\n\n**Q: å­¦ç¿’æå¤±ãŒä¸‹ãŒã‚Šã¾ã›ã‚“**\nA: ä»¥ä¸‹ã‚’ç¢ºèªï¼š\n1. å­¦ç¿’ç‡ãŒé©åˆ‡ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2e-5ï¼‰\n2. ãƒ‡ãƒ¼ã‚¿é‡ãŒååˆ†ã‹ï¼ˆæœ€ä½1000ã‚µãƒ³ãƒ—ãƒ«æ¨å¥¨ï¼‰\n3. ã‚¨ãƒãƒƒã‚¯æ•°ã‚’å¢—ã‚„ã™\n\n**Q: ç”ŸæˆçµæœãŒæœŸå¾…ã¨é•ã„ã¾ã™**\nA: SFTå¾Œã®è¿½åŠ èª¿æ•´æ–¹æ³•ï¼š\n1. ã‚ˆã‚Šå¤šãã®ã‚¨ãƒãƒƒã‚¯ã§å­¦ç¿’\n2. ç•°ãªã‚‹æ¸©åº¦è¨­å®šã§ç”Ÿæˆãƒ†ã‚¹ãƒˆ\n3. ã‚ˆã‚Šå¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å†å­¦ç¿’\n\n## ğŸ› ï¸ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºä¾‹\n\n### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´\n```python\n# ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´\nconfig.batch_size = 1  # ãƒ¡ãƒ¢ãƒªä¸è¶³æ™‚\n\n# å­¦ç¿’ç‡èª¿æ•´  \nconfig.learning_rate = 1e-5  # ã‚ˆã‚Šä¿å®ˆçš„\n\n# ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºèª¿æ•´\nconfig.dataset_size = 5000  # ä¸­é–“ã‚µã‚¤ã‚º\n```\n\n### è©³ç´°è¨­å®šä¾‹\n```python\n# é«˜é€Ÿãƒ†ã‚¹ãƒˆç”¨\nresult = custom_quick_training(\n    dataset_size=100, \n    batch_size=1, \n    epochs=1\n)\n\n# ãƒãƒ©ãƒ³ã‚¹å‹\nresult = custom_quick_training(\n    dataset_size=2000,\n    batch_size=2, \n    epochs=2\n)\n```\n\n## ğŸ†˜ ç·Šæ€¥æ™‚ã®å¯¾å‡¦æ³•\n\n### å®Œå…¨ãƒªã‚»ãƒƒãƒˆæ‰‹é †\n```python\n# 1. GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢\ntorch.cuda.empty_cache()\ngc.collect()\n\n# 2. ã‚«ãƒ¼ãƒãƒ«å†èµ·å‹•ï¼ˆRuntime > Restart Runtimeï¼‰\n\n# 3. ã‚»ãƒ«1ã‹ã‚‰å†å®Ÿè¡Œ\n\n# 4. æœ€å°è¨­å®šã§å®Ÿè¡Œ\nresult = custom_quick_training(dataset_size=100, batch_size=1, epochs=1)\n```\n\n### ã‚¨ãƒ©ãƒ¼å ±å‘Š\nã‚¨ãƒ©ãƒ¼ãŒè§£æ±ºã—ãªã„å ´åˆã¯ã€ä»¥ä¸‹ã®æƒ…å ±ã¨å…±ã«ã”å ±å‘Šãã ã•ã„ï¼š\n- å®Ÿè¡Œã—ãŸã‚³ãƒãƒ³ãƒ‰\n- ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å…¨æ–‡\n- GPUæƒ…å ±ï¼ˆ`nvidia-smi`ã®çµæœï¼‰\n- ä½¿ç”¨ã—ãŸè¨­å®šï¼ˆvalidation/medium/productionï¼‰\n\n---\n\n## ğŸ“ è¿½åŠ ã‚µãƒãƒ¼ãƒˆ\n\n**å®Ÿè¡ŒæˆåŠŸã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ:**\n- âœ… ã‚»ãƒ«1-8ã‚’é †ç•ªã«å®Ÿè¡Œå®Œäº†\n- âœ… ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãªã—\n- âœ… GPUæƒ…å ±è¡¨ç¤ºæ¸ˆã¿\n- âœ… `result = notebook_runner.complete_auto_training()` å®Ÿè¡Œ\n\n**å­¦ç¿’æˆåŠŸã®ç¢ºèª:**\n- âœ… ã€Œå­¦ç¿’å®Œäº†ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º\n- âœ… æœ€çµ‚æå¤±ãŒè¡¨ç¤ºã•ã‚Œã‚‹\n- âœ… ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n- âœ… è‡ªå‹•è©•ä¾¡çµæœè¡¨ç¤º\n\nã™ã¹ã¦ç¢ºèªã§ãã‚Œã°å­¦ç¿’æˆåŠŸã§ã™ï¼ğŸ‰",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sft_evaluation_and_generation"
   },
   "outputs": [],
   "source": "# ğŸ® å®Œå…¨è‡ªå‹•å®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…å®Œçµï¼‰\n\nclass NotebookSFTRunner:\n    \"\"\"ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…å®Œçµå‹ã®SFTå­¦ç¿’å®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ \"\"\"\n    \n    def __init__(self):\n        self.is_training = False\n        self.training_result = None\n        \n    def complete_auto_training(self, config_name=\"validation\", show_progress=True):\n        \"\"\"\n        å®Œå…¨è‡ªå‹•SFTå­¦ç¿’å®Ÿè¡Œï¼ˆãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…å®Œçµï¼‰\n        \n        Args:\n            config_name: ä½¿ç”¨ã™ã‚‹è¨­å®š (\"validation\", \"medium\", \"production\")\n            show_progress: é€²æ—è¡¨ç¤ºã‚’è¡Œã†ã‹ã©ã†ã‹\n        \"\"\"\n        \n        print(\"ğŸ¬ LLaDA SFTå­¦ç¿’ å®Œå…¨è‡ªå‹•å®Ÿè¡Œã‚’é–‹å§‹ã—ã¾ã™\")\n        print(\"=\"*60)\n        print(\"ğŸ”„ ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…å®Œçµã‚·ã‚¹ãƒ†ãƒ :\")\n        print(\"  âœ… å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ä¸è¦\")\n        print(\"  âœ… WandBä¸ä½¿ç”¨\")\n        print(\"  âœ… é€²æ—ã¯ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…è¡¨ç¤º\")\n        print(\"  âœ… çµæœã¯ã‚»ãƒ«å‡ºåŠ›ã«ä¿å­˜\")\n        print(\"=\"*60)\n        \n        # 1. ç’°å¢ƒãƒã‚§ãƒƒã‚¯\n        print(\"\\\\nğŸ” 1. ç’°å¢ƒãƒã‚§ãƒƒã‚¯...\")\n        gpu_available = torch.cuda.is_available()\n        if gpu_available:\n            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n            print(f\"  âœ… GPU: {torch.cuda.get_device_name(0)}\")\n            print(f\"  âœ… ãƒ¡ãƒ¢ãƒª: {gpu_memory:.1f} GB\")\n            \n            # è‡ªå‹•è¨­å®šé¸æŠ\n            if gpu_memory >= 15 and config_name == \"validation\":\n                recommended_config = \"medium\"\n                print(f\"  ğŸ’¡ ååˆ†ãªãƒ¡ãƒ¢ãƒªãŒã‚ã‚Šã¾ã™ã€‚{recommended_config}è¨­å®šã‚’æ¨å¥¨\")\n            else:\n                recommended_config = config_name\n        else:\n            recommended_config = \"validation\"\n            print(\"  âš ï¸  GPUæœªæ¤œå‡º - CPUå­¦ç¿’ï¼ˆvalidationè¨­å®šå›ºå®šï¼‰\")\n        \n        # 2. è¨­å®šé©ç”¨\n        print(f\"\\\\nâš™ï¸  2. è¨­å®šé©ç”¨: {recommended_config}\")\n        global config, CONFIG_NAME\n        CONFIG_NAME = recommended_config\n        config = select_config(recommended_config)\n        \n        # 3. ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–\n        print(\"\\\\nğŸ§¹ 3. ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–...\")\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            gc.collect()\n            print(f\"  âœ… GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢å®Œäº†\")\n        \n        # 4. é€²æ—ç›£è¦–é–‹å§‹\n        if show_progress:\n            print(\"\\\\nğŸ“Š 4. é€²æ—ç›£è¦–é–‹å§‹...\")\n            notebook_monitor.start_monitoring()\n        \n        # 5. å­¦ç¿’å®Ÿè¡Œ\n        print(\"\\\\nğŸš€ 5. å­¦ç¿’é–‹å§‹...\")\n        print(\"â° 3ç§’å¾Œã«é–‹å§‹...\")\n        time.sleep(3)\n        \n        try:\n            # å®Ÿéš›ã®å­¦ç¿’å®Ÿè¡Œ\n            training_result = self._execute_training(show_progress)\n            \n            # 6. çµæœè¡¨ç¤º\n            print(\"\\\\nğŸ‰ å­¦ç¿’å®Œäº†!\")\n            self._display_results(training_result, show_progress)\n            \n            # 7. è‡ªå‹•è©•ä¾¡\n            print(\"\\\\nğŸ” 7. è‡ªå‹•è©•ä¾¡å®Ÿè¡Œ...\")\n            self._run_auto_evaluation()\n            \n            # 8. æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ\n            if show_progress:\n                print(\"\\\\nğŸ“„ 8. æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ...\")\n                notebook_monitor.plot_training_progress()\n                notebook_monitor.generate_final_report()\n            \n            # æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n            print(\"\\\\n\" + \"ğŸŠ\" * 20)\n            print(\"ğŸ† LLaDA SFTå­¦ç¿’ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ!\")\n            print(\"ğŸŠ\" * 20)\n            \n            return training_result\n            \n        except Exception as e:\n            print(f\"\\\\nâŒ å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}\")\n            self._display_troubleshooting()\n            raise\n    \n    def _execute_training(self, show_progress=True):\n        \"\"\"å®Ÿéš›ã®å­¦ç¿’ã‚’å®Ÿè¡Œ\"\"\"\n        \n        # å­¦ç¿’å‰ãƒã‚§ãƒƒã‚¯\n        print(f\"ğŸ“‹ å­¦ç¿’è¨­å®šç¢ºèª:\")\n        print(f\"  - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(train_dataset)} ã‚µãƒ³ãƒ—ãƒ«\")\n        print(f\"  - ãƒãƒƒãƒã‚µã‚¤ã‚º: {config.batch_size}\")\n        print(f\"  - ã‚¨ãƒãƒƒã‚¯æ•°: {config.num_epochs}\")\n        print(f\"  - æ¨å®šæ™‚é–“: {self._estimate_time()} åˆ†\")\n        \n        # å­¦ç¿’å‰è©•ä¾¡\n        if val_dataset and len(val_dataset) > 0:\n            print(\"\\\\nğŸ“Š å­¦ç¿’å‰è©•ä¾¡...\")\n            try:\n                initial_eval = trainer.evaluate()\n                print(f\"  åˆæœŸæå¤±: {initial_eval['eval_loss']:.4f}\")\n            except Exception as e:\n                print(f\"  åˆæœŸè©•ä¾¡ã‚¹ã‚­ãƒƒãƒ—: {e}\")\n        \n        # ãƒ¡ã‚¤ãƒ³å­¦ç¿’\n        print(\"\\\\nğŸ¯ ãƒ¡ã‚¤ãƒ³å­¦ç¿’å®Ÿè¡Œä¸­...\")\n        start_time = time.time()\n        \n        # Trainerè‡ªä½“ã®å­¦ç¿’å®Ÿè¡Œ\n        training_result = trainer.train()\n        \n        training_duration = time.time() - start_time\n        print(f\"\\\\nâ±ï¸  å­¦ç¿’æ™‚é–“: {training_duration/60:.1f} åˆ†\")\n        \n        return training_result\n    \n    def _display_results(self, training_result, show_progress):\n        \"\"\"çµæœã‚’è¡¨ç¤º\"\"\"\n        print(\"\\\\nğŸ“Š å­¦ç¿’çµæœ:\")\n        print(f\"  âœ… æœ€çµ‚æå¤±: {training_result.training_loss:.4f}\")\n        print(f\"  âœ… å­¦ç¿’æ™‚é–“: {training_result.metrics['train_runtime']:.1f} ç§’\")\n        print(f\"  âœ… ã‚µãƒ³ãƒ—ãƒ«/ç§’: {training_result.metrics['train_samples_per_second']:.2f}\")\n        \n        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n        print(\"\\\\nğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­...\")\n        try:\n            trainer.save_model()\n            tokenizer.save_pretrained(config.output_dir)\n            print(f\"  âœ… ä¿å­˜å®Œäº†: {config.output_dir}\")\n        except Exception as e:\n            print(f\"  âš ï¸  ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}\")\n    \n    def _run_auto_evaluation(self):\n        \"\"\"è‡ªå‹•è©•ä¾¡ã‚’å®Ÿè¡Œ\"\"\"\n        model.eval()\n        \n        test_questions = [\n            \"æ—¥æœ¬ã®é¦–éƒ½ã¯ã©ã“ã§ã™ã‹ï¼Ÿ\",\n            \"æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ\",\n            \"Pythonã§ãƒªã‚¹ãƒˆã‚’é€†é †ã«ã™ã‚‹æ–¹æ³•ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚\"\n        ]\n        \n        print(f\"\\\\nğŸ§ª {len(test_questions)} å€‹ã®è³ªå•ã§è©•ä¾¡ä¸­...\")\n        \n        for i, question in enumerate(test_questions, 1):\n            print(f\"\\\\n--- è©•ä¾¡ {i} ---\")\n            print(f\"è³ªå•: {question}\")\n            \n            try:\n                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæº–å‚™\n                formatted_text, _ = processor.format_for_sft(question, \"\")\n                prompt_only = formatted_text.replace(processor.special_tokens['eos'], \"\")\n                \n                # ç”Ÿæˆå®Ÿè¡Œ\n                inputs = tokenizer(prompt_only, return_tensors=\"pt\", max_length=256, truncation=True).to(device)\n                \n                with torch.no_grad():\n                    outputs = model.generate(\n                        **inputs,\n                        max_new_tokens=80,\n                        do_sample=True,\n                        temperature=0.7,\n                        top_p=0.9,\n                        pad_token_id=tokenizer.pad_token_id\n                    )\n                    \n                    generated_text = tokenizer.decode(\n                        outputs[0][inputs.input_ids.shape[1]:], \n                        skip_special_tokens=True\n                    )\n                    \n                    print(f\"å›ç­”: {generated_text.strip()}\")\n                    \n            except Exception as e:\n                print(f\"è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}\")\n        \n        print(\"\\\\nâœ… è‡ªå‹•è©•ä¾¡å®Œäº†\")\n    \n    def _estimate_time(self):\n        \"\"\"å­¦ç¿’æ™‚é–“ã‚’æ¨å®š\"\"\"\n        base_time = 0.05  # ç§’/ã‚µãƒ³ãƒ—ãƒ«\n        total_samples = config.dataset_size * config.num_epochs\n        estimated_seconds = total_samples * base_time / config.batch_size\n        return max(1, int(estimated_seconds / 60))\n    \n    def _display_troubleshooting(self):\n        \"\"\"ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°è¡¨ç¤º\"\"\"\n        print(\"\\\\n\" + \"ğŸ”§\" * 20)\n        print(\"ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰\")\n        print(\"ğŸ”§\" * 20)\n        print(\"ğŸ’¡ è§£æ±ºç­–:\")\n        print(\"  1. ã‚ˆã‚Šå°ã•ãªè¨­å®šã‚’è©¦ã™: complete_auto_training('validation')\")\n        print(\"  2. GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢: torch.cuda.empty_cache()\")\n        print(\"  3. ãƒãƒƒãƒã‚µã‚¤ã‚ºå‰Šæ¸›: config.batch_size = 1\")\n        print(\"  4. ã‚»ãƒ«å†èµ·å‹•å¾Œã«å†å®Ÿè¡Œ\")\n        print(\"ğŸ”§\" * 20)\n\n# çµ±åˆå®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ \nnotebook_runner = NotebookSFTRunner()\n\n# ç°¡å˜å®Ÿè¡Œé–¢æ•°ç¾¤\ndef quick_training():\n    \"\"\"æœ€é€Ÿå®Ÿè¡Œï¼ˆvalidationè¨­å®šï¼‰\"\"\"\n    print(\"âš¡ ã‚¯ã‚¤ãƒƒã‚¯å­¦ç¿’é–‹å§‹...\")\n    return notebook_runner.complete_auto_training(\"validation\")\n\ndef medium_training():\n    \"\"\"ä¸­è¦æ¨¡å­¦ç¿’ï¼ˆmediumè¨­å®šï¼‰\"\"\"\n    print(\"ğŸ’ª ä¸­è¦æ¨¡å­¦ç¿’é–‹å§‹...\")\n    return notebook_runner.complete_auto_training(\"medium\")\n\ndef production_training():\n    \"\"\"æœ¬æ ¼å­¦ç¿’ï¼ˆproductionè¨­å®šï¼‰\"\"\"\n    print(\"ğŸ”¥ æœ¬æ ¼å­¦ç¿’é–‹å§‹...\")\n    return notebook_runner.complete_auto_training(\"production\")\n\ndef custom_quick_training(dataset_size=500, batch_size=1, epochs=1):\n    \"\"\"ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§ã®é«˜é€Ÿå­¦ç¿’\"\"\"\n    print(f\"ğŸ›ï¸ ã‚«ã‚¹ã‚¿ãƒ å­¦ç¿’: {dataset_size}ã‚µãƒ³ãƒ—ãƒ«, ãƒãƒƒãƒ{batch_size}, {epochs}ã‚¨ãƒãƒƒã‚¯\")\n    \n    # è¨­å®šã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º\n    global config\n    config.dataset_size = dataset_size\n    config.batch_size = batch_size\n    config.num_epochs = epochs\n    \n    return notebook_runner.complete_auto_training(\"validation\")\n\n# ãƒ¡ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹\nprint(\"ğŸ¯ ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…å®ŒçµSFTå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ æº–å‚™å®Œäº†\")\nprint(\"\\\\n\" + \"ğŸš€\" * 25)\nprint(\"å³åº§ã«å®Ÿè¡Œå¯èƒ½ãªã‚³ãƒãƒ³ãƒ‰:\")\nprint(\"ğŸš€\" * 25)\nprint()\nprint(\"ğŸ¥‡ å®Œå…¨è‡ªå‹•ï¼ˆåˆå›æ¨å¥¨ï¼‰:\")\nprint(\"   result = notebook_runner.complete_auto_training()\")\nprint()\nprint(\"âš¡ ã‚¯ã‚¤ãƒƒã‚¯å®Ÿè¡Œ:\")\nprint(\"   result = quick_training()\")\nprint()\nprint(\"ğŸ’ª ä¸­è¦æ¨¡å®Ÿè¡Œ:\")\nprint(\"   result = medium_training()\")\nprint()\nprint(\"ğŸ”¥ æœ¬æ ¼å®Ÿè¡Œ:\")\nprint(\"   result = production_training()\")\nprint()\nprint(\"ğŸ›ï¸ ã‚«ã‚¹ã‚¿ãƒ å®Ÿè¡Œ:\")\nprint(\"   result = custom_quick_training(dataset_size=1000, batch_size=2, epochs=1)\")\nprint()\nprint(\"ğŸš€\" * 25)\nprint(\"âœ¨ ç‰¹å¾´:\")\nprint(\"  âœ… ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…å®Œçµï¼ˆå¤–éƒ¨ãƒ„ãƒ¼ãƒ«ä¸è¦ï¼‰\")\nprint(\"  âœ… è‡ªå‹•ã‚¨ãƒ©ãƒ¼å›å¾©\")\nprint(\"  âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—è¡¨ç¤º\")\nprint(\"  âœ… è‡ªå‹•è©•ä¾¡ãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ\")\nprint(\"ğŸš€\" * 25)"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}