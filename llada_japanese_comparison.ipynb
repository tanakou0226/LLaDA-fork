{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaDA: æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ¯”è¼ƒå®Ÿé¨“ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€LLaDAï¼ˆLarge Language Diffusion with mAskingï¼‰ã‚’ä½¿ã£ã¦æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã®æ‹¡æ•£è¨€èªãƒ¢ãƒ‡ãƒ«ã®å‹•ä½œã‚’æ¯”è¼ƒåˆ†æã—ã¾ã™ã€‚\n",
    "\n",
    "## ğŸ“š ç›®æ¬¡\n",
    "1. [ç’°å¢ƒè¨­å®šã¨ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿](#1-ç’°å¢ƒè¨­å®šã¨ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿)\n",
    "2. [æ—¥æœ¬èªã§ã®åŸºæœ¬çš„ãªç”Ÿæˆå®Ÿé¨“](#2-æ—¥æœ¬èªã§ã®åŸºæœ¬çš„ãªç”Ÿæˆå®Ÿé¨“)\n",
    "3. [è‹±èªvsæ—¥æœ¬èªã®æ¯”è¼ƒå®Ÿé¨“](#3-è‹±èªvsæ—¥æœ¬èªã®æ¯”è¼ƒå®Ÿé¨“)\n",
    "4. [æ—¥æœ¬èªç‰¹æœ‰ã®ç¾è±¡ã®åˆ†æ](#4-æ—¥æœ¬èªç‰¹æœ‰ã®ç¾è±¡ã®åˆ†æ)\n",
    "5. [ç•°ãªã‚‹æ—¥æœ¬èªã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½æ¯”è¼ƒ](#5-ç•°ãªã‚‹æ—¥æœ¬èªã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½æ¯”è¼ƒ)\n",
    "6. [ã¾ã¨ã‚ã¨è€ƒå¯Ÿ](#6-ã¾ã¨ã‚ã¨è€ƒå¯Ÿ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒè¨­å®šã¨ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colabã§ã®å®Ÿè¡Œç”¨\n",
    "import sys\n",
    "\n",
    "# Google Colabã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Google Colabç’°å¢ƒã‚’æ¤œå‡ºã—ã¾ã—ãŸ\")\n",
    "    \n",
    "    # å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    !pip install transformers==4.49.0 accelerate==0.34.2 torch numpy matplotlib ipywidgets\n",
    "    \n",
    "    # GPUãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯\n",
    "    import torch\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å®Ÿè¡Œä¸­ã§ã™\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from typing import List, Tuple, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆColabç”¨ï¼‰\n",
    "if IN_COLAB:\n",
    "    !apt-get -qq install fonts-noto-cjk\n",
    "    import matplotlib.font_manager as fm\n",
    "    plt.rcParams['font.family'] = ['DejaVu Sans', 'Noto Sans CJK JP']\n",
    "\n",
    "print(\"ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLaDAãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿\n",
    "print(\"LLaDAãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...ï¼ˆæ•°åˆ†ã‹ã‹ã‚Šã¾ã™ï¼‰\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿\n",
    "model_name = 'GSAI-ML/LLaDA-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True, \n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device).eval()\n",
    "\n",
    "print(\"ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "print(f\"ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {sum(p.numel() for p in model.parameters()) / 1e9:.1f}B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é‡è¦ãªå®šæ•°ã¨ã‚³ã‚¢é–¢æ•°ã®å®šç¾©\n",
    "MASK_ID = 126336  # [MASK]ãƒˆãƒ¼ã‚¯ãƒ³ã®ID\n",
    "\n",
    "def add_gumbel_noise(logits, temperature):\n",
    "    \"\"\"\n",
    "    Gumbelãƒã‚¤ã‚ºã‚’è¿½åŠ ã—ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’è¡Œã†\n",
    "    \"\"\"\n",
    "    if temperature == 0:\n",
    "        return logits\n",
    "    \n",
    "    logits = logits.to(torch.float64)\n",
    "    noise = torch.rand_like(logits, dtype=torch.float64)\n",
    "    gumbel_noise = (-torch.log(noise)) ** temperature\n",
    "    return logits.exp() / gumbel_noise\n",
    "\n",
    "def get_num_transfer_tokens(mask_index, steps):\n",
    "    \"\"\"\n",
    "    å„ã‚¹ãƒ†ãƒƒãƒ—ã§ä½•å€‹ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒã‚¹ã‚¯è§£é™¤ã™ã‚‹ã‹ã‚’è¨ˆç®—\n",
    "    \"\"\"\n",
    "    mask_num = mask_index.sum(dim=1, keepdim=True)\n",
    "    base = mask_num // steps\n",
    "    remainder = mask_num % steps\n",
    "    \n",
    "    num_transfer_tokens = torch.zeros(\n",
    "        mask_num.size(0), steps, \n",
    "        device=mask_index.device, \n",
    "        dtype=torch.int64\n",
    "    ) + base\n",
    "    \n",
    "    for i in range(mask_num.size(0)):\n",
    "        num_transfer_tokens[i, :remainder[i]] += 1\n",
    "    \n",
    "    return num_transfer_tokens\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_llada(model, prompt, steps=32, gen_length=64, block_length=32, \n",
    "                   temperature=0.0, cfg_scale=0.0, remasking='low_confidence'):\n",
    "    \"\"\"\n",
    "    å®Œå…¨ãªLLaDAç”Ÿæˆé–¢æ•°\n",
    "    \"\"\"\n",
    "    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "    if isinstance(prompt, str):\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        formatted_prompt = tokenizer.apply_chat_template(\n",
    "            messages, add_generation_prompt=True, tokenize=False\n",
    "        )\n",
    "        input_ids = tokenizer(formatted_prompt)['input_ids']\n",
    "        input_ids = torch.tensor(input_ids).to(device).unsqueeze(0)\n",
    "    else:\n",
    "        input_ids = prompt\n",
    "    \n",
    "    prompt_length = input_ids.shape[1]\n",
    "    \n",
    "    # å¿œç­”éƒ¨åˆ†ã‚’[MASK]ã§åˆæœŸåŒ–\n",
    "    x = torch.full(\n",
    "        (1, prompt_length + gen_length), \n",
    "        MASK_ID, \n",
    "        dtype=torch.long\n",
    "    ).to(device)\n",
    "    x[:, :prompt_length] = input_ids.clone()\n",
    "    \n",
    "    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆCFGç”¨ï¼‰\n",
    "    prompt_index = (x != MASK_ID)\n",
    "    \n",
    "    # ãƒ–ãƒ­ãƒƒã‚¯å‡¦ç†ã®è¨­å®š\n",
    "    block_length = min(block_length, gen_length)\n",
    "    num_blocks = (gen_length + block_length - 1) // block_length\n",
    "    \n",
    "    if steps % num_blocks != 0:\n",
    "        adjusted_steps = ((steps + num_blocks - 1) // num_blocks) * num_blocks\n",
    "        print(f\"ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’ {steps} ã‹ã‚‰ {adjusted_steps} ã«èª¿æ•´ã—ã¾ã—ãŸ\")\n",
    "        steps = adjusted_steps\n",
    "    \n",
    "    steps_per_block = steps // num_blocks\n",
    "    generation_history = []\n",
    "    \n",
    "    print(f\"ç”Ÿæˆé–‹å§‹: {num_blocks}ãƒ–ãƒ­ãƒƒã‚¯ x {steps_per_block}ã‚¹ãƒ†ãƒƒãƒ— = è¨ˆ{steps}ã‚¹ãƒ†ãƒƒãƒ—\")\n",
    "    \n",
    "    # å„ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‡¦ç†\n",
    "    for block_idx in range(num_blocks):\n",
    "        block_start = prompt_length + block_idx * block_length\n",
    "        block_end = min(prompt_length + (block_idx + 1) * block_length, x.shape[1])\n",
    "        \n",
    "        print(f\"\\nãƒ–ãƒ­ãƒƒã‚¯ {block_idx + 1}/{num_blocks} (ä½ç½® {block_start}-{block_end-1}):\")\n",
    "        \n",
    "        block_mask_index = (x[:, block_start:block_end] == MASK_ID)\n",
    "        \n",
    "        if not block_mask_index.any():\n",
    "            print(f\"  ãƒ–ãƒ­ãƒƒã‚¯ {block_idx + 1}: ãƒã‚¹ã‚¯ãªã—ã€ã‚¹ã‚­ãƒƒãƒ—\")\n",
    "            continue\n",
    "            \n",
    "        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps_per_block)\n",
    "        \n",
    "        for step in range(steps_per_block):\n",
    "            mask_index = (x == MASK_ID)\n",
    "            \n",
    "            if not mask_index.any():\n",
    "                print(f\"  ã‚¹ãƒ†ãƒƒãƒ— {step + 1}: å…¨ãƒˆãƒ¼ã‚¯ãƒ³ç¢ºå®šæ¸ˆã¿\")\n",
    "                break\n",
    "            \n",
    "            # åˆ†é¡å™¨ãƒ•ãƒªãƒ¼ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ (CFG)\n",
    "            if cfg_scale > 0.0:\n",
    "                un_x = x.clone()\n",
    "                un_x[prompt_index] = MASK_ID\n",
    "                x_ = torch.cat([x, un_x], dim=0)\n",
    "                logits = model(x_).logits\n",
    "                logits, un_logits = torch.chunk(logits, 2, dim=0)\n",
    "                logits = un_logits + (cfg_scale + 1) * (logits - un_logits)\n",
    "            else:\n",
    "                logits = model(x).logits\n",
    "            \n",
    "            # Gumbelãƒã‚¤ã‚ºã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "            logits_with_noise = add_gumbel_noise(logits, temperature)\n",
    "            x0 = torch.argmax(logits_with_noise, dim=-1)\n",
    "            \n",
    "            # å†ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥ã«åŸºã¥ãä¿¡é ¼åº¦è¨ˆç®—\n",
    "            if remasking == 'low_confidence':\n",
    "                p = F.softmax(logits, dim=-1)\n",
    "                x0_p = torch.squeeze(\n",
    "                    torch.gather(p, dim=-1, index=torch.unsqueeze(x0, -1)), -1\n",
    "                )\n",
    "            elif remasking == 'random':\n",
    "                x0_p = torch.rand((x0.shape[0], x0.shape[1]), device=x0.device)\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Remasking strategy '{remasking}' not implemented\")\n",
    "            \n",
    "            x0_p[:, block_end:] = -float('inf')\n",
    "            \n",
    "            x0 = torch.where(mask_index, x0, x)\n",
    "            confidence = torch.where(mask_index, x0_p, -float('inf'))\n",
    "            \n",
    "            # é«˜ä¿¡é ¼åº¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸æŠã—ã¦ãƒã‚¹ã‚¯è§£é™¤\n",
    "            transfer_index = torch.zeros_like(x0, dtype=torch.bool, device=device)\n",
    "            for j in range(confidence.shape[0]):\n",
    "                num_tokens_to_transfer = min(\n",
    "                    num_transfer_tokens[j, step].item(),\n",
    "                    mask_index[j].sum().item()\n",
    "                )\n",
    "                if num_tokens_to_transfer > 0:\n",
    "                    _, select_index = torch.topk(confidence[j], k=num_tokens_to_transfer)\n",
    "                    transfer_index[j, select_index] = True\n",
    "            \n",
    "            x[transfer_index] = x0[transfer_index]\n",
    "            \n",
    "            # é€²æ—ã‚’è¨˜éŒ²\n",
    "            current_text = tokenizer.decode(\n",
    "                x[0, prompt_length:], skip_special_tokens=False\n",
    "            )\n",
    "            avg_conf = confidence[0, transfer_index[0]].mean().item() if transfer_index[0].any() else 0\n",
    "            \n",
    "            generation_history.append({\n",
    "                'block': block_idx,\n",
    "                'step': step,\n",
    "                'global_step': block_idx * steps_per_block + step,\n",
    "                'text': current_text,\n",
    "                'num_revealed': transfer_index[0].sum().item(),\n",
    "                'avg_confidence': avg_conf\n",
    "            })\n",
    "            \n",
    "            print(f\"  ã‚¹ãƒ†ãƒƒãƒ— {step+1:2d}: {transfer_index[0].sum().item():2d}å€‹ç¢ºå®š \"\n",
    "                  f\"(ä¿¡é ¼åº¦: {avg_conf:.3f})\")\n",
    "    \n",
    "    # æœ€çµ‚çµæœ\n",
    "    final_text = tokenizer.decode(\n",
    "        x[0, prompt_length:], skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return final_text, generation_history\n",
    "\n",
    "print(\"ã‚³ã‚¢é–¢æ•°ã®å®šç¾©ãŒå®Œäº†ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‹¡å¼µå¯è¦–åŒ–é–¢æ•°ã®å®šç¾©\n",
    "def detect_token_changes(prev_tokens, curr_tokens):\n",
    "    \"\"\"\n",
    "    å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã¨ç¾åœ¨ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã®ãƒˆãƒ¼ã‚¯ãƒ³å¤‰åŒ–ã‚’æ¤œå‡º\n",
    "    \"\"\"\n",
    "    changes = []\n",
    "    \n",
    "    if prev_tokens is None:\n",
    "        for i, token in enumerate(curr_tokens):\n",
    "            if token != '[MASK]':\n",
    "                changes.append((i, 'new', token))\n",
    "    else:\n",
    "        for i, (prev_token, curr_token) in enumerate(zip(prev_tokens, curr_tokens)):\n",
    "            if prev_token != curr_token:\n",
    "                if prev_token == '[MASK]' and curr_token != '[MASK]':\n",
    "                    changes.append((i, 'revealed', curr_token))\n",
    "                elif prev_token != '[MASK]' and curr_token == '[MASK]':\n",
    "                    changes.append((i, 'masked', prev_token))\n",
    "                elif prev_token != '[MASK]' and curr_token != '[MASK]':\n",
    "                    changes.append((i, 'changed', curr_token))\n",
    "    \n",
    "    return changes\n",
    "\n",
    "def get_confidence_color(confidence):\n",
    "    \"\"\"\n",
    "    ä¿¡é ¼åº¦ã«åŸºã¥ã„ã¦è½ã¡ç€ã„ãŸè‰²ã‚’æ±ºå®š\n",
    "    \"\"\"\n",
    "    if confidence >= 0.8:\n",
    "        return '\\033[38;5;28m'  # æ·±ã„ç·‘\n",
    "    elif confidence >= 0.6:\n",
    "        return '\\033[38;5;94m'  # ã‚ªãƒªãƒ¼ãƒ–\n",
    "    elif confidence >= 0.4:\n",
    "        return '\\033[38;5;130m'  # ãƒ–ãƒ©ã‚¦ãƒ³\n",
    "    else:\n",
    "        return '\\033[38;5;240m'  # ã‚°ãƒ¬ãƒ¼\n",
    "\n",
    "def visualize_generation_with_highlights(history, title=\"LLaDA Generation Process\", show_details=True):\n",
    "    \"\"\"\n",
    "    æ–°ã—ãç”Ÿæˆã•ã‚ŒãŸå˜èªã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤ºã™ã‚‹æ‹¡å¼µå¯è¦–åŒ–é–¢æ•°\n",
    "    \"\"\"\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if show_details:\n",
    "        print(\"ä¿¡é ¼åº¦è‰²åˆ†ã‘:\")\n",
    "        print(\"  \\033[38;5;28mâ– \\033[0m é«˜ä¿¡é ¼åº¦ (â‰¥0.8)  \\033[38;5;94mâ– \\033[0m ä¸­ä¿¡é ¼åº¦ (â‰¥0.6)  \\033[38;5;130mâ– \\033[0m ä½ä¿¡é ¼åº¦ (â‰¥0.4)  \\033[38;5;240mâ– \\033[0m æ¥µä½ (<0.4)\")\n",
    "        print(\"  å¼·èª¿: \\033[1må¤ªå­—\\033[0m = æ–°è¦ç”Ÿæˆ  \\033[2mè–„å­—\\033[0m = å†ãƒã‚¹ã‚¯  \\033[4mä¸‹ç·š\\033[0m = å¤‰æ›´\")\n",
    "        print()\n",
    "    \n",
    "    prev_tokens = None\n",
    "    \n",
    "    for i, entry in enumerate(history):\n",
    "        global_step = entry.get('global_step', entry['step'])\n",
    "        block = entry.get('block', 0)\n",
    "        step = entry['step']\n",
    "        text = entry['text']\n",
    "        num_revealed = entry['num_revealed']\n",
    "        avg_conf = entry['avg_confidence']\n",
    "        \n",
    "        clean_text = text.replace('<|reserved_special_token_250|>', '[MASK]')\n",
    "        curr_tokens = clean_text.split()\n",
    "        \n",
    "        changes = detect_token_changes(prev_tokens, curr_tokens)\n",
    "        \n",
    "        block_info = f\"[Block {block+1}]\" if 'block' in entry else \"\"\n",
    "        print(f\"Step {global_step+1:2d} {block_info} (+{num_revealed:2d} tokens, conf={avg_conf:.3f}):\")\n",
    "        \n",
    "        highlighted_text = []\n",
    "        change_map = {pos: (change_type, token) for pos, change_type, token in changes}\n",
    "        \n",
    "        for pos, token in enumerate(curr_tokens):\n",
    "            if pos in change_map:\n",
    "                change_type, _ = change_map[pos]\n",
    "                color = get_confidence_color(avg_conf)\n",
    "                \n",
    "                if change_type == 'revealed':\n",
    "                    highlighted_text.append(f\"{color}\\033[1m{token}\\033[0m\")\n",
    "                elif change_type == 'masked':\n",
    "                    highlighted_text.append(f\"\\033[2m[MASK]\\033[0m\")\n",
    "                elif change_type == 'changed':\n",
    "                    highlighted_text.append(f\"{color}\\033[4m{token}\\033[0m\")\n",
    "                else:\n",
    "                    highlighted_text.append(f\"{color}{token}\\033[0m\")\n",
    "            else:\n",
    "                if token == '[MASK]':\n",
    "                    highlighted_text.append(f\"\\033[38;5;245m{token}\\033[0m\")\n",
    "                else:\n",
    "                    highlighted_text.append(token)\n",
    "        \n",
    "        print(f\"  {' '.join(highlighted_text)}\")\n",
    "        \n",
    "        if show_details and changes:\n",
    "            change_summary = []\n",
    "            for pos, change_type, token in changes:\n",
    "                if change_type == 'revealed':\n",
    "                    change_summary.append(f\"pos{pos}:æ–°è¦({token})\")\n",
    "                elif change_type == 'masked':\n",
    "                    change_summary.append(f\"pos{pos}:ãƒã‚¹ã‚¯\")\n",
    "                elif change_type == 'changed':\n",
    "                    change_summary.append(f\"pos{pos}:å¤‰æ›´({token})\")\n",
    "            \n",
    "            if change_summary:\n",
    "                print(f\"    å¤‰åŒ–: {', '.join(change_summary)}\")\n",
    "        \n",
    "        print()\n",
    "        prev_tokens = curr_tokens\n",
    "\n",
    "def compare_generation_results(results, titles, detailed_analysis=True):\n",
    "    \"\"\"\n",
    "    è¤‡æ•°ã®ç”Ÿæˆçµæœã‚’è©³ç´°ã«æ¯”è¼ƒ\n",
    "    \"\"\"\n",
    "    print(\"ç”Ÿæˆçµæœæ¯”è¼ƒåˆ†æ\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # åŸºæœ¬çµ±è¨ˆã®æ¯”è¼ƒ\n",
    "    print(\"\\nåŸºæœ¬çµ±è¨ˆ:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, (result, title) in enumerate(zip(results, titles)):\n",
    "        text, history = result\n",
    "        avg_conf = np.mean([h['avg_confidence'] for h in history if h['avg_confidence'] > 0])\n",
    "        total_steps = len(history)\n",
    "        final_length = len(text.split())\n",
    "        \n",
    "        print(f\"{title}:\")\n",
    "        print(f\"  æœ€çµ‚ãƒ†ã‚­ã‚¹ãƒˆé•·: {final_length}èª\")\n",
    "        print(f\"  ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°: {total_steps}\")\n",
    "        print(f\"  å¹³å‡ä¿¡é ¼åº¦: {avg_conf:.3f}\")\n",
    "        print(f\"  æœ€çµ‚çµæœ: {text}\")\n",
    "        print()\n",
    "    \n",
    "    if detailed_analysis:\n",
    "        print(\"\\nè©³ç´°åˆ†æ:\")\n",
    "        print(\"-\" * 40)\n",
    "        for i, (result, title) in enumerate(zip(results, titles)):\n",
    "            text, history = result\n",
    "            print(f\"\\n{title}ã®ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ï¼ˆæœ€å¾Œã®5ã‚¹ãƒ†ãƒƒãƒ—ï¼‰:\")\n",
    "            visualize_generation_with_highlights(history[-5:], f\"{title}ã®è©³ç´°\", show_details=False)\n",
    "\n",
    "print(\"å¯è¦–åŒ–é–¢æ•°ã®å®šç¾©ãŒå®Œäº†ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æ—¥æœ¬èªã§ã®åŸºæœ¬çš„ãªç”Ÿæˆå®Ÿé¨“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ—¥æœ¬èªã§ã®åŸºæœ¬çš„ãªç”Ÿæˆå®Ÿé¨“\n",
    "japanese_prompts = [\n",
    "    \"æ—¥æœ¬ã®é¦–éƒ½ã¯ã©ã“ã§ã™ã‹ï¼Ÿ\",\n",
    "    \"æ¡œã®å­£ç¯€ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚\",\n",
    "    \"ãŠã„ã—ã„ãƒ©ãƒ¼ãƒ¡ãƒ³ã®ä½œã‚Šæ–¹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\"\n",
    "]\n",
    "\n",
    "print(\"=== æ—¥æœ¬èªã§ã®åŸºæœ¬çš„ãªç”Ÿæˆå®Ÿé¨“ ===\")\n",
    "japanese_results = []\n",
    "\n",
    "for i, prompt in enumerate(japanese_prompts):\n",
    "    print(f\"\\n{i+1}. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    result, history = generate_llada(\n",
    "        model, prompt,\n",
    "        steps=16, gen_length=64, block_length=32,\n",
    "        temperature=0.0, cfg_scale=0.0, remasking='low_confidence'\n",
    "    )\n",
    "    \n",
    "    japanese_results.append((result, history))\n",
    "    print(f\"\\nç”Ÿæˆçµæœ: {result}\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ—¥æœ¬èªç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã®è©³ç´°å¯è¦–åŒ–\n",
    "print(\"=== æ—¥æœ¬èªç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã®è©³ç´°åˆ†æ ===\")\n",
    "\n",
    "for i, (prompt, (result, history)) in enumerate(zip(japanese_prompts, japanese_results)):\n",
    "    print(f\"\\n{i+1}. {prompt}\")\n",
    "    print(\"ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ï¼ˆæœ€å¾Œã®6ã‚¹ãƒ†ãƒƒãƒ—ï¼‰:\")\n",
    "    visualize_generation_with_highlights(\n",
    "        history[-6:], \n",
    "        f\"æ—¥æœ¬èªç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ {i+1}\",\n",
    "        show_details=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. è‹±èªvsæ—¥æœ¬èªã®æ¯”è¼ƒå®Ÿé¨“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¾å¿œã™ã‚‹è‹±èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§åŒã˜å®Ÿé¨“\n",
    "english_prompts = [\n",
    "    \"What is the capital of Japan?\",\n",
    "    \"Please tell me about cherry blossom season.\",\n",
    "    \"Please explain how to make delicious ramen.\"\n",
    "]\n",
    "\n",
    "print(\"=== è‹±èªã§ã®å¯¾å¿œå®Ÿé¨“ ===\")\n",
    "english_results = []\n",
    "\n",
    "for i, prompt in enumerate(english_prompts):\n",
    "    print(f\"\\n{i+1}. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    result, history = generate_llada(\n",
    "        model, prompt,\n",
    "        steps=16, gen_length=64, block_length=32,\n",
    "        temperature=0.0, cfg_scale=0.0, remasking='low_confidence'\n",
    "    )\n",
    "    \n",
    "    english_results.append((result, history))\n",
    "    print(f\"\\nç”Ÿæˆçµæœ: {result}\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ—¥æœ¬èªvsè‹±èªã®è©³ç´°æ¯”è¼ƒ\n",
    "print(\"=== æ—¥æœ¬èª vs è‹±èª è©³ç´°æ¯”è¼ƒ ===\")\n",
    "\n",
    "comparison_topics = [\n",
    "    \"æ—¥æœ¬ã®é¦–éƒ½ã«ã¤ã„ã¦\",\n",
    "    \"æ¡œã®å­£ç¯€ã«ã¤ã„ã¦\", \n",
    "    \"ãƒ©ãƒ¼ãƒ¡ãƒ³ã®ä½œã‚Šæ–¹ã«ã¤ã„ã¦\"\n",
    "]\n",
    "\n",
    "for i in range(len(japanese_prompts)):\n",
    "    print(f\"\\n=== {comparison_topics[i]} ===\")\n",
    "    \n",
    "    # åŸºæœ¬çµ±è¨ˆæ¯”è¼ƒ\n",
    "    jp_text, jp_history = japanese_results[i]\n",
    "    en_text, en_history = english_results[i]\n",
    "    \n",
    "    jp_avg_conf = np.mean([h['avg_confidence'] for h in jp_history if h['avg_confidence'] > 0])\n",
    "    en_avg_conf = np.mean([h['avg_confidence'] for h in en_history if h['avg_confidence'] > 0])\n",
    "    \n",
    "    jp_tokens = len(jp_text.split())\n",
    "    en_tokens = len(en_text.split())\n",
    "    \n",
    "    print(f\"\\nçµ±è¨ˆæ¯”è¼ƒ:\")\n",
    "    print(f\"  æ—¥æœ¬èª: {jp_tokens}èª, å¹³å‡ä¿¡é ¼åº¦ {jp_avg_conf:.3f}\")\n",
    "    print(f\"  è‹±èª:   {en_tokens}èª, å¹³å‡ä¿¡é ¼åº¦ {en_avg_conf:.3f}\")\n",
    "    \n",
    "    print(f\"\\næœ€çµ‚çµæœæ¯”è¼ƒ:\")\n",
    "    print(f\"  æ—¥æœ¬èª: {jp_text}\")\n",
    "    print(f\"  è‹±èª:   {en_text}\")\n",
    "    \n",
    "    # ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹æ¯”è¼ƒï¼ˆæœ€å¾Œã®4ã‚¹ãƒ†ãƒƒãƒ—ï¼‰\n",
    "    print(f\"\\nç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹æ¯”è¼ƒï¼ˆæœ€å¾Œã®4ã‚¹ãƒ†ãƒƒãƒ—ï¼‰:\")\n",
    "    print(\"\\næ—¥æœ¬èª:\")\n",
    "    visualize_generation_with_highlights(\n",
    "        jp_history[-4:], \"æ—¥æœ¬èªç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹\", show_details=False\n",
    "    )\n",
    "    \n",
    "    print(\"\\nè‹±èª:\")\n",
    "    visualize_generation_with_highlights(\n",
    "        en_history[-4:], \"è‹±èªç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹\", show_details=False\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ—¥æœ¬èªç‰¹æœ‰ã®ç¾è±¡ã®åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ—¥æœ¬èªç‰¹æœ‰ã®è¤‡é›‘ãªè¡¨ç¾ã§ã®å®Ÿé¨“\n",
    "complex_japanese_prompts = [\n",
    "    \"ã€å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ã€ã®é­…åŠ›ã«ã¤ã„ã¦è«–ã˜ã¦ãã ã•ã„ã€‚\",\n",
    "    \"æ—¥æœ¬ã®å››å­£ã®ç¾ã—ã•ã‚’ä¿³å¥ã§è¡¨ç¾ã—ã¦ãã ã•ã„ã€‚\", \n",
    "    \"é–¢è¥¿å¼ã§å¤§é˜ªã®é­…åŠ›ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\",\n",
    "    \"æ•¬èªã‚’ä½¿ã£ã¦ä¼šç¤¾ã§ã®æŒ¨æ‹¶ã®ä»•æ–¹ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚\"\n",
    "]\n",
    "\n",
    "print(\"=== æ—¥æœ¬èªç‰¹æœ‰è¡¨ç¾ã§ã®ç”Ÿæˆå®Ÿé¨“ ===\")\n",
    "complex_results = []\n",
    "\n",
    "for i, prompt in enumerate(complex_japanese_prompts):\n",
    "    print(f\"\\n{i+1}. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    result, history = generate_llada(\n",
    "        model, prompt,\n",
    "        steps=20, gen_length=80, block_length=40,\n",
    "        temperature=0.2, cfg_scale=0.0, remasking='low_confidence'\n",
    "    )\n",
    "    \n",
    "    complex_results.append((result, history))\n",
    "    print(f\"\\nç”Ÿæˆçµæœ: {result}\")\n",
    "    \n",
    "    # ä¿¡é ¼åº¦åˆ†æ\n",
    "    avg_conf = np.mean([h['avg_confidence'] for h in history if h['avg_confidence'] > 0])\n",
    "    print(f\"å¹³å‡ä¿¡é ¼åº¦: {avg_conf:.3f}\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ—¥æœ¬èªè¡¨ç¾ã®è¤‡é›‘ã•ã¨ç”Ÿæˆå“è³ªã®é–¢ä¿‚åˆ†æ\n",
    "print(\"=== æ—¥æœ¬èªè¡¨ç¾ã®è¤‡é›‘ã•åˆ†æ ===\")\n",
    "\n",
    "# å„çµæœã®è©³ç´°åˆ†æ\n",
    "analysis_labels = [\n",
    "    \"æ–‡å­¦çš„è¡¨ç¾\",\n",
    "    \"éŸ»å¾‹çš„è¡¨ç¾ï¼ˆä¿³å¥ï¼‰\",\n",
    "    \"æ–¹è¨€è¡¨ç¾ï¼ˆé–¢è¥¿å¼ï¼‰\", \n",
    "    \"æ•¬èªè¡¨ç¾\"\n",
    "]\n",
    "\n",
    "for i, ((result, history), label) in enumerate(zip(complex_results, analysis_labels)):\n",
    "    print(f\"\\n=== {label} ===\")\n",
    "    \n",
    "    # çµ±è¨ˆæƒ…å ±\n",
    "    avg_conf = np.mean([h['avg_confidence'] for h in history if h['avg_confidence'] > 0])\n",
    "    conf_std = np.std([h['avg_confidence'] for h in history if h['avg_confidence'] > 0])\n",
    "    total_steps = len(history)\n",
    "    final_length = len(result.split())\n",
    "    \n",
    "    print(f\"çµ±è¨ˆ:\")\n",
    "    print(f\"  æœ€çµ‚é•·: {final_length}èª\")\n",
    "    print(f\"  ã‚¹ãƒ†ãƒƒãƒ—æ•°: {total_steps}\")\n",
    "    print(f\"  å¹³å‡ä¿¡é ¼åº¦: {avg_conf:.3f} (Â±{conf_std:.3f})\")\n",
    "    print(f\"  çµæœ: {result}\")\n",
    "    \n",
    "    print(f\"\\nç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ï¼ˆæœ€å¾Œã®5ã‚¹ãƒ†ãƒƒãƒ—ï¼‰:\")\n",
    "    visualize_generation_with_highlights(\n",
    "        history[-5:], f\"{label}ã®ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹\", show_details=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ç•°ãªã‚‹æ—¥æœ¬èªã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½æ¯”è¼ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç•°ãªã‚‹ã‚¿ã‚¤ãƒ—ã®æ—¥æœ¬èªã‚¿ã‚¹ã‚¯ã§ã®æ¯”è¼ƒå®Ÿé¨“\n",
    "task_experiments = {\n",
    "    \"æ•°å­¦å•é¡Œ\": \"12 + 15 = ? ã“ã®è¨ˆç®—ã‚’æ®µéšçš„ã«è§£ã„ã¦ãã ã•ã„ã€‚\",\n",
    "    \"æ–™ç†ãƒ¬ã‚·ãƒ”\": \"åµç„¼ãã®ä½œã‚Šæ–¹ã‚’ç°¡æ½”ã«æ•™ãˆã¦ãã ã•ã„ã€‚\",\n",
    "    \"ç§‘å­¦èª¬æ˜\": \"å…‰åˆæˆã®ã—ãã¿ã‚’åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚\",\n",
    "    \"æ–‡åŒ–ç´¹ä»‹\": \"æ—¥æœ¬ã®èŒ¶é“ã®ç²¾ç¥ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\",\n",
    "    \"å®Ÿç”¨æƒ…å ±\": \"æ±äº¬é§…ã‹ã‚‰ç¾½ç”°ç©ºæ¸¯ã¸ã®è¡Œãæ–¹ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚\"\n",
    "}\n",
    "\n",
    "print(\"=== ç•°ãªã‚‹æ—¥æœ¬èªã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½æ¯”è¼ƒ ===\")\n",
    "task_results = {}\n",
    "\n",
    "for task_name, prompt in task_experiments.items():\n",
    "    print(f\"\\nã€{task_name}ã€‘\")\n",
    "    print(f\"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    result, history = generate_llada(\n",
    "        model, prompt,\n",
    "        steps=18, gen_length=72, block_length=36,\n",
    "        temperature=0.1, cfg_scale=0.0, remasking='low_confidence'\n",
    "    )\n",
    "    \n",
    "    task_results[task_name] = (result, history)\n",
    "    \n",
    "    # å³åº§ã«çµæœè¡¨ç¤º\n",
    "    avg_conf = np.mean([h['avg_confidence'] for h in history if h['avg_confidence'] > 0])\n",
    "    print(f\"\\nçµæœ: {result}\")\n",
    "    print(f\"å¹³å‡ä¿¡é ¼åº¦: {avg_conf:.3f}\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚¿ã‚¹ã‚¯åˆ¥æ€§èƒ½åˆ†æã¨ã‚°ãƒ©ãƒ•åŒ–\n",
    "print(\"=== ã‚¿ã‚¹ã‚¯åˆ¥æ€§èƒ½åˆ†æ ===\")\n",
    "\n",
    "# çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã®åé›†\n",
    "task_stats = {}\n",
    "for task_name, (result, history) in task_results.items():\n",
    "    avg_conf = np.mean([h['avg_confidence'] for h in history if h['avg_confidence'] > 0])\n",
    "    conf_std = np.std([h['avg_confidence'] for h in history if h['avg_confidence'] > 0])\n",
    "    total_steps = len(history)\n",
    "    final_length = len(result.split())\n",
    "    \n",
    "    task_stats[task_name] = {\n",
    "        'avg_confidence': avg_conf,\n",
    "        'conf_std': conf_std,\n",
    "        'total_steps': total_steps,\n",
    "        'final_length': final_length,\n",
    "        'result': result\n",
    "    }\n",
    "\n",
    "# çµ±è¨ˆè¡¨ã®è¡¨ç¤º\n",
    "print(\"\\nã‚¿ã‚¹ã‚¯åˆ¥çµ±è¨ˆ:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'ã‚¿ã‚¹ã‚¯':<12} {'èªæ•°':<6} {'ã‚¹ãƒ†ãƒƒãƒ—':<8} {'å¹³å‡ä¿¡é ¼åº¦':<12} {'ä¿¡é ¼åº¦SD':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for task_name, stats in task_stats.items():\n",
    "    print(f\"{task_name:<12} {stats['final_length']:<6} {stats['total_steps']:<8} \"\n",
    "          f\"{stats['avg_confidence']:<12.3f} {stats['conf_std']:<10.3f}\")\n",
    "\n",
    "# ã‚°ãƒ©ãƒ•åŒ–\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# å¹³å‡ä¿¡é ¼åº¦ã®æ¯”è¼ƒ\n",
    "task_names = list(task_stats.keys())\n",
    "confidences = [task_stats[name]['avg_confidence'] for name in task_names]\n",
    "conf_stds = [task_stats[name]['conf_std'] for name in task_names]\n",
    "\n",
    "bars1 = ax1.bar(range(len(task_names)), confidences, \n",
    "                yerr=conf_stds, capsize=5, \n",
    "                color=['#2E8B57', '#8FBC8F', '#CD853F', '#DEB887', '#708090'])\n",
    "ax1.set_xlabel('ã‚¿ã‚¹ã‚¯ç¨®åˆ¥')\n",
    "ax1.set_ylabel('å¹³å‡ä¿¡é ¼åº¦')\n",
    "ax1.set_title('ã‚¿ã‚¹ã‚¯åˆ¥å¹³å‡ä¿¡é ¼åº¦')\n",
    "ax1.set_xticks(range(len(task_names)))\n",
    "ax1.set_xticklabels(task_names, rotation=45, ha='right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# æœ€çµ‚èªæ•°ã®æ¯”è¼ƒ\n",
    "lengths = [task_stats[name]['final_length'] for name in task_names]\n",
    "bars2 = ax2.bar(range(len(task_names)), lengths,\n",
    "                color=['#2E8B57', '#8FBC8F', '#CD853F', '#DEB887', '#708090'])\n",
    "ax2.set_xlabel('ã‚¿ã‚¹ã‚¯ç¨®åˆ¥')\n",
    "ax2.set_ylabel('ç”Ÿæˆèªæ•°')\n",
    "ax2.set_title('ã‚¿ã‚¹ã‚¯åˆ¥ç”Ÿæˆèªæ•°')\n",
    "ax2.set_xticks(range(len(task_names)))\n",
    "ax2.set_xticklabels(task_names, rotation=45, ha='right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å„ã‚¿ã‚¹ã‚¯ã®è©³ç´°ãªç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹åˆ†æ\n",
    "print(\"=== å„ã‚¿ã‚¹ã‚¯ã®ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹è©³ç´°åˆ†æ ===\")\n",
    "\n",
    "for task_name, (result, history) in task_results.items():\n",
    "    print(f\"\\nã€{task_name}ã€‘ã®ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹åˆ†æ\")\n",
    "    print(f\"æœ€çµ‚çµæœ: {result}\")\n",
    "    print(\"\\nç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ï¼ˆæœ€å¾Œã®6ã‚¹ãƒ†ãƒƒãƒ—ï¼‰:\")\n",
    "    \n",
    "    visualize_generation_with_highlights(\n",
    "        history[-6:], \n",
    "        f\"{task_name}ã®è©³ç´°ãƒ—ãƒ­ã‚»ã‚¹\",\n",
    "        show_details=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ã¾ã¨ã‚ã¨è€ƒå¯Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å…¨ä½“çš„ãªåˆ†æã¨ã¾ã¨ã‚\n",
    "print(\"=== æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®Ÿé¨“ã®ç·åˆåˆ†æ ===\")\n",
    "\n",
    "# 1. è¨€èªåˆ¥æ¯”è¼ƒã®è¦ç´„\n",
    "print(\"\\n1. æ—¥æœ¬èª vs è‹±èªã®æ¯”è¼ƒçµæœ:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "jp_confidences = []\n",
    "en_confidences = []\n",
    "jp_lengths = []\n",
    "en_lengths = []\n",
    "\n",
    "for (jp_result, jp_hist), (en_result, en_hist) in zip(japanese_results, english_results):\n",
    "    jp_conf = np.mean([h['avg_confidence'] for h in jp_hist if h['avg_confidence'] > 0])\n",
    "    en_conf = np.mean([h['avg_confidence'] for h in en_hist if h['avg_confidence'] > 0])\n",
    "    \n",
    "    jp_confidences.append(jp_conf)\n",
    "    en_confidences.append(en_conf)\n",
    "    jp_lengths.append(len(jp_result.split()))\n",
    "    en_lengths.append(len(en_result.split()))\n",
    "\n",
    "print(f\"å¹³å‡ä¿¡é ¼åº¦ - æ—¥æœ¬èª: {np.mean(jp_confidences):.3f}, è‹±èª: {np.mean(en_confidences):.3f}\")\n",
    "print(f\"å¹³å‡èªæ•°   - æ—¥æœ¬èª: {np.mean(jp_lengths):.1f}èª, è‹±èª: {np.mean(en_lengths):.1f}èª\")\n",
    "\n",
    "# 2. æ—¥æœ¬èªã‚¿ã‚¹ã‚¯åˆ¥åˆ†æ\n",
    "print(\"\\n2. æ—¥æœ¬èªã‚¿ã‚¹ã‚¯åˆ¥æ€§èƒ½ãƒ©ãƒ³ã‚­ãƒ³ã‚°:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# ä¿¡é ¼åº¦ã§ã‚½ãƒ¼ãƒˆ\n",
    "sorted_tasks = sorted(task_stats.items(), key=lambda x: x[1]['avg_confidence'], reverse=True)\n",
    "\n",
    "print(\"ä¿¡é ¼åº¦é †ãƒ©ãƒ³ã‚­ãƒ³ã‚°:\")\n",
    "for i, (task_name, stats) in enumerate(sorted_tasks, 1):\n",
    "    print(f\"  {i}. {task_name}: {stats['avg_confidence']:.3f}\")\n",
    "\n",
    "# 3. ç‰¹å¾´çš„ãªè¦³å¯Ÿçµæœ\n",
    "print(\"\\n3. è¦³å¯Ÿã•ã‚ŒãŸç‰¹å¾´:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "highest_conf_task = sorted_tasks[0][0]\n",
    "lowest_conf_task = sorted_tasks[-1][0]\n",
    "\n",
    "print(f\"â€¢ æœ€ã‚‚å®‰å®šã—ãŸç”Ÿæˆ: {highest_conf_task}ï¼ˆä¿¡é ¼åº¦: {sorted_tasks[0][1]['avg_confidence']:.3f}ï¼‰\")\n",
    "print(f\"â€¢ æœ€ã‚‚æŒ‘æˆ¦çš„ãªã‚¿ã‚¹ã‚¯: {lowest_conf_task}ï¼ˆä¿¡é ¼åº¦: {sorted_tasks[-1][1]['avg_confidence']:.3f}ï¼‰\")\n",
    "\n",
    "# 4. è¨€èªç‰¹æ€§ã®åˆ†æ\n",
    "print(\"\\n4. æ—¥æœ¬èªç‰¹æœ‰ã®ç”Ÿæˆç‰¹æ€§:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if np.mean(jp_confidences) > np.mean(en_confidences):\n",
    "    print(\"â€¢ æ—¥æœ¬èªã§ã®ç”Ÿæˆã®æ–¹ãŒé«˜ã„ä¿¡é ¼åº¦ã‚’ç¤ºã—ã¦ã„ã‚‹\")\n",
    "else:\n",
    "    print(\"â€¢ è‹±èªã§ã®ç”Ÿæˆã®æ–¹ãŒé«˜ã„ä¿¡é ¼åº¦ã‚’ç¤ºã—ã¦ã„ã‚‹\")\n",
    "\n",
    "if np.mean(jp_lengths) > np.mean(en_lengths):\n",
    "    print(\"â€¢ æ—¥æœ¬èªã®æ–¹ãŒã‚ˆã‚Šè©³ç´°ãªå¿œç­”ã‚’ç”Ÿæˆã™ã‚‹å‚¾å‘\")\n",
    "else:\n",
    "    print(\"â€¢ è‹±èªã®æ–¹ãŒã‚ˆã‚Šè©³ç´°ãªå¿œç­”ã‚’ç”Ÿæˆã™ã‚‹å‚¾å‘\")\n",
    "\n",
    "print(\"\\n5. æ¨å¥¨ã•ã‚Œã‚‹ä½¿ç”¨å ´é¢:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"â€¢ é«˜ç²¾åº¦ãŒå¿…è¦: {highest_conf_task}ãªã©ã®æ§‹é€ åŒ–ã‚¿ã‚¹ã‚¯\")\n",
    "print(f\"â€¢ å‰µé€ æ€§ãŒå¿…è¦: {lowest_conf_task}ãªã©ã®è¡¨ç¾çš„ã‚¿ã‚¹ã‚¯\")\n",
    "print(\"â€¢ å®Ÿç”¨çš„å¿œç”¨: æ–™ç†ãƒ¬ã‚·ãƒ”ã‚„å®Ÿç”¨æƒ…å ±ãªã©ã®å…·ä½“çš„æŒ‡ç¤º\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"å®Ÿé¨“å®Œäº†: æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã®LLaDAæ€§èƒ½åˆ†æ\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å®Ÿé¨“çµæœã‚µãƒãƒªãƒ¼\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€LLaDAã®æ—¥æœ¬èªå‡¦ç†èƒ½åŠ›ã‚’å¤šè§’çš„ã«åˆ†æã—ã¾ã—ãŸï¼š\n",
    "\n",
    "### ä¸»è¦ãªç™ºè¦‹\n",
    "\n",
    "1. **è¨€èªé–“æ¯”è¼ƒ**: æ—¥æœ¬èªã¨è‹±èªã§ã®ç”Ÿæˆå“è³ªã¨ç‰¹æ€§ã®é•ã„\n",
    "2. **ã‚¿ã‚¹ã‚¯åˆ¥æ€§èƒ½**: ç•°ãªã‚‹ç¨®é¡ã®æ—¥æœ¬èªã‚¿ã‚¹ã‚¯ã§ã®ç›¸å¯¾çš„ãªæ€§èƒ½\n",
    "3. **è¤‡é›‘è¡¨ç¾**: æ•¬èªã€æ–¹è¨€ã€éŸ»å¾‹ãªã©æ—¥æœ¬èªç‰¹æœ‰ã®è¡¨ç¾ã¸ã®å¯¾å¿œ\n",
    "4. **ä¿¡é ¼åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³**: ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã«ã‚ˆã‚‹ä¿¡é ¼åº¦ã®å¤‰å‹•\n",
    "\n",
    "### æ´»ç”¨ææ¡ˆ\n",
    "\n",
    "- **æ§‹é€ åŒ–ã‚¿ã‚¹ã‚¯**: æ•°å­¦ã€æ–™ç†ãƒ¬ã‚·ãƒ”ãªã©æ˜ç¢ºãªæ‰‹é †ãŒã‚ã‚‹ã‚¿ã‚¹ã‚¯\n",
    "- **æ–‡åŒ–çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„**: æ—¥æœ¬ç‰¹æœ‰ã®æ–‡åŒ–ã‚„æ…£ç¿’ã«é–¢ã™ã‚‹èª¬æ˜\n",
    "- **å®Ÿç”¨çš„æƒ…å ±**: äº¤é€šæ¡ˆå†…ã‚„å®Ÿç”¨çš„ãªæŒ‡ç¤º\n",
    "\n",
    "### ä»Šå¾Œã®ç ”ç©¶æ–¹å‘\n",
    "\n",
    "- ã‚ˆã‚Šé•·ã„æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã§ã®æ€§èƒ½è©•ä¾¡\n",
    "- å°‚é–€åˆ†é‡ï¼ˆæ³•å¾‹ã€åŒ»ç™‚ãªã©ï¼‰ã§ã®æ—¥æœ¬èªå‡¦ç†\n",
    "- å¤šæ§˜ãªæ—¥æœ¬èªæ–¹è¨€ã§ã®æ¯”è¼ƒå®Ÿé¨“"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}