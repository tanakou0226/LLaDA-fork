{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaDA: Êó•Êú¨Ë™û„Éó„É≠„É≥„Éó„ÉàÊØîËºÉÂÆüÈ®ì„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ\n",
    "\n",
    "„Åì„ÅÆ„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ„Åß„ÅØ„ÄÅLLaDAÔºàLarge Language Diffusion with mAskingÔºâ„Çí‰Ωø„Å£„Å¶Êó•Êú¨Ë™û„Éó„É≠„É≥„Éó„Éà„Åß„ÅÆÊã°Êï£Ë®ÄË™û„É¢„Éá„É´„ÅÆÂãï‰Ωú„ÇíÊØîËºÉÂàÜÊûê„Åó„Åæ„Åô„ÄÇ\n",
    "\n",
    "## üìö ÁõÆÊ¨°\n",
    "1. [Áí∞Â¢ÉË®≠ÂÆö„Å®„É¢„Éá„É´Ë™≠„ÅøËæº„Åø](#1-Áí∞Â¢ÉË®≠ÂÆö„Å®„É¢„Éá„É´Ë™≠„ÅøËæº„Åø)\n",
    "2. [Êó•Êú¨Ë™û„Åß„ÅÆÂü∫Êú¨ÁöÑ„Å™ÁîüÊàêÂÆüÈ®ì](#2-Êó•Êú¨Ë™û„Åß„ÅÆÂü∫Êú¨ÁöÑ„Å™ÁîüÊàêÂÆüÈ®ì)\n",
    "3. [Ëã±Ë™ûvsÊó•Êú¨Ë™û„ÅÆÊØîËºÉÂÆüÈ®ì](#3-Ëã±Ë™ûvsÊó•Êú¨Ë™û„ÅÆÊØîËºÉÂÆüÈ®ì)\n",
    "4. [Êó•Êú¨Ë™ûÁâπÊúâ„ÅÆÁèæË±°„ÅÆÂàÜÊûê](#4-Êó•Êú¨Ë™ûÁâπÊúâ„ÅÆÁèæË±°„ÅÆÂàÜÊûê)\n",
    "5. [Áï∞„Å™„ÇãÊó•Êú¨Ë™û„Çø„Çπ„ÇØ„Åß„ÅÆÊÄßËÉΩÊØîËºÉ](#5-Áï∞„Å™„ÇãÊó•Êú¨Ë™û„Çø„Çπ„ÇØ„Åß„ÅÆÊÄßËÉΩÊØîËºÉ)\n",
    "6. [„Åæ„Å®„ÇÅ„Å®ËÄÉÂØü](#6-„Åæ„Å®„ÇÅ„Å®ËÄÉÂØü)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Áí∞Â¢ÉË®≠ÂÆö„Å®„É¢„Éá„É´Ë™≠„ÅøËæº„Åø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab„Åß„ÅÆÂÆüË°åÁî®\n",
    "import sys\n",
    "\n",
    "# Google Colab„Åã„Å©„ÅÜ„Åã„Çí„ÉÅ„Çß„ÉÉ„ÇØ\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Google ColabÁí∞Â¢É„ÇíÊ§úÂá∫„Åó„Åæ„Åó„Åü\")\n",
    "    \n",
    "    # ÂøÖË¶Å„Å™„É©„Ç§„Éñ„É©„É™„Çí„Ç§„É≥„Çπ„Éà„Éº„É´\n",
    "    !pip install transformers==4.49.0 accelerate==0.34.2 torch numpy matplotlib ipywidgets\n",
    "    \n",
    "    # GPU„ÅåÂà©Áî®ÂèØËÉΩ„Åã„ÉÅ„Çß„ÉÉ„ÇØ\n",
    "    import torch\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"„É≠„Éº„Ç´„É´Áí∞Â¢É„ÅßÂÆüË°å‰∏≠„Åß„Åô\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂøÖË¶Å„Å™„É©„Ç§„Éñ„É©„É™„Çí„Ç§„É≥„Éù„Éº„Éà\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from typing import List, Tuple, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Êó•Êú¨Ë™û„Éï„Ç©„É≥„ÉàË®≠ÂÆöÔºàColabÁî®Ôºâ\n",
    "if IN_COLAB:\n",
    "    !apt-get -qq install fonts-noto-cjk\n",
    "    import matplotlib.font_manager as fm\n",
    "    plt.rcParams['font.family'] = ['DejaVu Sans', 'Noto Sans CJK JP']\n",
    "\n",
    "print(\"„É©„Ç§„Éñ„É©„É™„ÅÆ„Ç§„É≥„Éù„Éº„Éà„ÅåÂÆå‰∫Ü„Åó„Åæ„Åó„Åü\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLaDA„É¢„Éá„É´„Å®„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„ÅÆË™≠„ÅøËæº„Åø\n",
    "print(\"LLaDA„É¢„Éá„É´„ÇíË™≠„ÅøËæº„Åø‰∏≠...ÔºàÊï∞ÂàÜ„Åã„Åã„Çä„Åæ„ÅôÔºâ\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"‰ΩøÁî®„Éá„Éê„Ç§„Çπ: {device}\")\n",
    "\n",
    "# „É¢„Éá„É´„Å®„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„ÅÆË™≠„ÅøËæº„Åø\n",
    "model_name = 'GSAI-ML/LLaDA-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True, \n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device).eval()\n",
    "\n",
    "print(\"„É¢„Éá„É´„ÅÆË™≠„ÅøËæº„Åø„ÅåÂÆå‰∫Ü„Åó„Åæ„Åó„ÅüÔºÅ\")\n",
    "print(f\"„É¢„Éá„É´„Çµ„Ç§„Ç∫: {sum(p.numel() for p in model.parameters()) / 1e9:.1f}B „Éë„É©„É°„Éº„Çø\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÈáçË¶Å„Å™ÂÆöÊï∞„Å®„Ç≥„Ç¢Èñ¢Êï∞„ÅÆÂÆöÁæ©\n",
    "MASK_ID = 126336  # [MASK]„Éà„Éº„ÇØ„É≥„ÅÆID\n",
    "\n",
    "def add_gumbel_noise(logits, temperature):\n",
    "    \"\"\"\n",
    "    Gumbel„Éé„Ç§„Ç∫„ÇíËøΩÂä†„Åó„Å¶„Çµ„É≥„Éó„É™„É≥„Ç∞„ÇíË°å„ÅÜ\n",
    "    \"\"\"\n",
    "    if temperature == 0:\n",
    "        return logits\n",
    "    \n",
    "    logits = logits.to(torch.float64)\n",
    "    noise = torch.rand_like(logits, dtype=torch.float64)\n",
    "    gumbel_noise = (-torch.log(noise)) ** temperature\n",
    "    return logits.exp() / gumbel_noise\n",
    "\n",
    "def get_num_transfer_tokens(mask_index, steps):\n",
    "    \"\"\"\n",
    "    ÂêÑ„Çπ„ÉÜ„ÉÉ„Éó„Åß‰ΩïÂÄã„ÅÆ„Éà„Éº„ÇØ„É≥„Çí„Éû„Çπ„ÇØËß£Èô§„Åô„Çã„Åã„ÇíË®àÁÆó\n",
    "    \"\"\"\n",
    "    mask_num = mask_index.sum(dim=1, keepdim=True)\n",
    "    base = mask_num // steps\n",
    "    remainder = mask_num % steps\n",
    "    \n",
    "    num_transfer_tokens = torch.zeros(\n",
    "        mask_num.size(0), steps, \n",
    "        device=mask_index.device, \n",
    "        dtype=torch.int64\n",
    "    ) + base\n",
    "    \n",
    "    for i in range(mask_num.size(0)):\n",
    "        num_transfer_tokens[i, :remainder[i]] += 1\n",
    "    \n",
    "    return num_transfer_tokens\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_llada(model, prompt, steps=32, gen_length=64, block_length=32, \n",
    "                   temperature=0.0, cfg_scale=0.0, remasking='low_confidence'):\n",
    "    \"\"\"\n",
    "    ÂÆåÂÖ®„Å™LLaDAÁîüÊàêÈñ¢Êï∞\n",
    "    \"\"\"\n",
    "    # „Éó„É≠„É≥„Éó„Éà„Çí„Éà„Éº„ÇØ„É≥Âåñ\n",
    "    if isinstance(prompt, str):\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        formatted_prompt = tokenizer.apply_chat_template(\n",
    "            messages, add_generation_prompt=True, tokenize=False\n",
    "        )\n",
    "        input_ids = tokenizer(formatted_prompt)['input_ids']\n",
    "        input_ids = torch.tensor(input_ids).to(device).unsqueeze(0)\n",
    "    else:\n",
    "        input_ids = prompt\n",
    "    \n",
    "    prompt_length = input_ids.shape[1]\n",
    "    \n",
    "    # ÂøúÁ≠îÈÉ®ÂàÜ„Çí[MASK]„ÅßÂàùÊúüÂåñ\n",
    "    x = torch.full(\n",
    "        (1, prompt_length + gen_length), \n",
    "        MASK_ID, \n",
    "        dtype=torch.long\n",
    "    ).to(device)\n",
    "    x[:, :prompt_length] = input_ids.clone()\n",
    "    \n",
    "    # „Éó„É≠„É≥„Éó„ÉàÈÉ®ÂàÜ„ÅÆ„Ç§„É≥„Éá„ÉÉ„ÇØ„ÇπÔºàCFGÁî®Ôºâ\n",
    "    prompt_index = (x != MASK_ID)\n",
    "    \n",
    "    # „Éñ„É≠„ÉÉ„ÇØÂá¶ÁêÜ„ÅÆË®≠ÂÆö\n",
    "    block_length = min(block_length, gen_length)\n",
    "    num_blocks = (gen_length + block_length - 1) // block_length\n",
    "    \n",
    "    if steps % num_blocks != 0:\n",
    "        adjusted_steps = ((steps + num_blocks - 1) // num_blocks) * num_blocks\n",
    "        print(f\"„Çπ„ÉÜ„ÉÉ„ÉóÊï∞„Çí {steps} „Åã„Çâ {adjusted_steps} „Å´Ë™øÊï¥„Åó„Åæ„Åó„Åü\")\n",
    "        steps = adjusted_steps\n",
    "    \n",
    "    steps_per_block = steps // num_blocks\n",
    "    generation_history = []\n",
    "    \n",
    "    print(f\"ÁîüÊàêÈñãÂßã: {num_blocks}„Éñ„É≠„ÉÉ„ÇØ x {steps_per_block}„Çπ„ÉÜ„ÉÉ„Éó = Ë®à{steps}„Çπ„ÉÜ„ÉÉ„Éó\")\n",
    "    \n",
    "    # ÂêÑ„Éñ„É≠„ÉÉ„ÇØ„ÇíÂá¶ÁêÜ\n",
    "    for block_idx in range(num_blocks):\n",
    "        block_start = prompt_length + block_idx * block_length\n",
    "        block_end = min(prompt_length + (block_idx + 1) * block_length, x.shape[1])\n",
    "        \n",
    "        print(f\"\\n„Éñ„É≠„ÉÉ„ÇØ {block_idx + 1}/{num_blocks} (‰ΩçÁΩÆ {block_start}-{block_end-1}):\")\n",
    "        \n",
    "        block_mask_index = (x[:, block_start:block_end] == MASK_ID)\n",
    "        \n",
    "        if not block_mask_index.any():\n",
    "            print(f\"  „Éñ„É≠„ÉÉ„ÇØ {block_idx + 1}: „Éû„Çπ„ÇØ„Å™„Åó„ÄÅ„Çπ„Ç≠„ÉÉ„Éó\")\n",
    "            continue\n",
    "            \n",
    "        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps_per_block)\n",
    "        \n",
    "        for step in range(steps_per_block):\n",
    "            mask_index = (x == MASK_ID)\n",
    "            \n",
    "            if not mask_index.any():\n",
    "                print(f\"  „Çπ„ÉÜ„ÉÉ„Éó {step + 1}: ÂÖ®„Éà„Éº„ÇØ„É≥Á¢∫ÂÆöÊ∏à„Åø\")\n",
    "                break\n",
    "            \n",
    "            # ÂàÜÈ°ûÂô®„Éï„É™„Éº„Ç¨„Ç§„ÉÄ„É≥„Çπ (CFG)\n",
    "            if cfg_scale > 0.0:\n",
    "                un_x = x.clone()\n",
    "                un_x[prompt_index] = MASK_ID\n",
    "                x_ = torch.cat([x, un_x], dim=0)\n",
    "                logits = model(x_).logits\n",
    "                logits, un_logits = torch.chunk(logits, 2, dim=0)\n",
    "                logits = un_logits + (cfg_scale + 1) * (logits - un_logits)\n",
    "            else:\n",
    "                logits = model(x).logits\n",
    "            \n",
    "            # Gumbel„Éé„Ç§„Ç∫„Åß„Çµ„É≥„Éó„É™„É≥„Ç∞\n",
    "            logits_with_noise = add_gumbel_noise(logits, temperature)\n",
    "            x0 = torch.argmax(logits_with_noise, dim=-1)\n",
    "            \n",
    "            # ÂÜç„Éû„Çπ„Ç≠„É≥„Ç∞Êà¶Áï•„Å´Âü∫„Å•„Åè‰ø°È†ºÂ∫¶Ë®àÁÆó\n",
    "            if remasking == 'low_confidence':\n",
    "                p = F.softmax(logits, dim=-1)\n",
    "                x0_p = torch.squeeze(\n",
    "                    torch.gather(p, dim=-1, index=torch.unsqueeze(x0, -1)), -1\n",
    "                )\n",
    "            elif remasking == 'random':\n",
    "                x0_p = torch.rand((x0.shape[0], x0.shape[1]), device=x0.device)\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Remasking strategy '{remasking}' not implemented\")\n",
    "            \n",
    "            x0_p[:, block_end:] = -float('inf')\n",
    "            \n",
    "            x0 = torch.where(mask_index, x0, x)\n",
    "            confidence = torch.where(mask_index, x0_p, -float('inf'))\n",
    "            \n",
    "            # È´ò‰ø°È†ºÂ∫¶„ÅÆ„Éà„Éº„ÇØ„É≥„ÇíÈÅ∏Êäû„Åó„Å¶„Éû„Çπ„ÇØËß£Èô§\n",
    "            transfer_index = torch.zeros_like(x0, dtype=torch.bool, device=device)\n",
    "            for j in range(confidence.shape[0]):\n",
    "                num_tokens_to_transfer = min(\n",
    "                    num_transfer_tokens[j, step].item(),\n",
    "                    mask_index[j].sum().item()\n",
    "                )\n",
    "                if num_tokens_to_transfer > 0:\n",
    "                    _, select_index = torch.topk(confidence[j], k=num_tokens_to_transfer)\n",
    "                    transfer_index[j, select_index] = True\n",
    "            \n",
    "            x[transfer_index] = x0[transfer_index]\n",
    "            \n",
    "            # ÈÄ≤Êçó„ÇíË®òÈå≤\n",
    "            current_text = tokenizer.decode(\n",
    "                x[0, prompt_length:], skip_special_tokens=False\n",
    "            )\n",
    "            avg_conf = confidence[0, transfer_index[0]].mean().item() if transfer_index[0].any() else 0\n",
    "            \n",
    "            generation_history.append({\n",
    "                'block': block_idx,\n",
    "                'step': step,\n",
    "                'global_step': block_idx * steps_per_block + step,\n",
    "                'text': current_text,\n",
    "                'num_revealed': transfer_index[0].sum().item(),\n",
    "                'avg_confidence': avg_conf\n",
    "            })\n",
    "            \n",
    "            print(f\"  „Çπ„ÉÜ„ÉÉ„Éó {step+1:2d}: {transfer_index[0].sum().item():2d}ÂÄãÁ¢∫ÂÆö \"\n",
    "                  f\"(‰ø°È†ºÂ∫¶: {avg_conf:.3f})\")\n",
    "    \n",
    "    # ÊúÄÁµÇÁµêÊûú\n",
    "    final_text = tokenizer.decode(\n",
    "        x[0, prompt_length:], skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return final_text, generation_history\n",
    "\n",
    "print(\"„Ç≥„Ç¢Èñ¢Êï∞„ÅÆÂÆöÁæ©„ÅåÂÆå‰∫Ü„Åó„Åæ„Åó„Åü\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Êã°ÂºµÂèØË¶ñÂåñÈñ¢Êï∞„ÅÆÂÆöÁæ©\n",
    "def detect_token_changes(prev_tokens, curr_tokens):\n",
    "    \"\"\"\n",
    "    Ââç„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó„Å®ÁèæÂú®„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó„Åß„ÅÆ„Éà„Éº„ÇØ„É≥Â§âÂåñ„ÇíÊ§úÂá∫\n",
    "    \"\"\"\n",
    "    changes = []\n",
    "    \n",
    "    if prev_tokens is None:\n",
    "        for i, token in enumerate(curr_tokens):\n",
    "            if token != '[MASK]':\n",
    "                changes.append((i, 'new', token))\n",
    "    else:\n",
    "        for i, (prev_token, curr_token) in enumerate(zip(prev_tokens, curr_tokens)):\n",
    "            if prev_token != curr_token:\n",
    "                if prev_token == '[MASK]' and curr_token != '[MASK]':\n",
    "                    changes.append((i, 'revealed', curr_token))\n",
    "                elif prev_token != '[MASK]' and curr_token == '[MASK]':\n",
    "                    changes.append((i, 'masked', prev_token))\n",
    "                elif prev_token != '[MASK]' and curr_token != '[MASK]':\n",
    "                    changes.append((i, 'changed', curr_token))\n",
    "    \n",
    "    return changes\n",
    "\n",
    "def get_confidence_color(confidence):\n",
    "    \"\"\"\n",
    "    ‰ø°È†ºÂ∫¶„Å´Âü∫„Å•„ÅÑ„Å¶ËêΩ„Å°ÁùÄ„ÅÑ„ÅüËâ≤„ÇíÊ±∫ÂÆö\n",
    "    \"\"\"\n",
    "    if confidence >= 0.8:\n",
    "        return '\\033[38;5;28m'  # Ê∑±„ÅÑÁ∑ë\n",
    "    elif confidence >= 0.6:\n",
    "        return '\\033[38;5;94m'  # „Ç™„É™„Éº„Éñ\n",
    "    elif confidence >= 0.4:\n",
    "        return '\\033[38;5;130m'  # „Éñ„É©„Ç¶„É≥\n",
    "    else:\n",
    "        return '\\033[38;5;240m'  # „Ç∞„É¨„Éº\n",
    "\n",
    "def visualize_generation_with_highlights(history, title=\"LLaDA Generation Process\", show_details=True):\n",
    "    \"\"\"\n",
    "    Êñ∞„Åó„ÅèÁîüÊàê„Åï„Çå„ÅüÂçòË™û„Çí„Éè„Ç§„É©„Ç§„ÉàË°®Á§∫„Åô„ÇãÊã°ÂºµÂèØË¶ñÂåñÈñ¢Êï∞\n",
    "    \"\"\"\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if show_details:\n",
    "        print(\"‰ø°È†ºÂ∫¶Ëâ≤ÂàÜ„Åë:\")\n",
    "        print(\"  \\033[38;5;28m‚ñ†\\033[0m È´ò‰ø°È†ºÂ∫¶ (‚â•0.8)  \\033[38;5;94m‚ñ†\\033[0m ‰∏≠‰ø°È†ºÂ∫¶ (‚â•0.6)  \\033[38;5;130m‚ñ†\\033[0m ‰Ωé‰ø°È†ºÂ∫¶ (‚â•0.4)  \\033[38;5;240m‚ñ†\\033[0m Ê•µ‰Ωé (<0.4)\")\n",
    "        print(\"  Âº∑Ë™ø: \\033[1mÂ§™Â≠ó\\033[0m = Êñ∞Ë¶èÁîüÊàê  \\033[2mËñÑÂ≠ó\\033[0m = ÂÜç„Éû„Çπ„ÇØ  \\033[4m‰∏ãÁ∑ö\\033[0m = Â§âÊõ¥\")\n",
    "        print()\n",
    "    \n",
    "    prev_tokens = None\n",
    "    \n",
    "    for i, entry in enumerate(history):\n",
    "        global_step = entry.get('global_step', entry['step'])\n",
    "        block = entry.get('block', 0)\n",
    "        step = entry['step']\n",
    "        text = entry['text']\n",
    "        num_revealed = entry['num_revealed']\n",
    "        avg_conf = entry['avg_confidence']\n",
    "        \n",
    "        clean_text = text.replace('<|reserved_special_token_250|>', '[MASK]')\n",
    "        curr_tokens = clean_text.split()\n",
    "        \n",
    "        changes = detect_token_changes(prev_tokens, curr_tokens)\n",
    "        \n",
    "        block_info = f\"[Block {block+1}]\" if 'block' in entry else \"\"\n",
    "        print(f\"Step {global_step+1:2d} {block_info} (+{num_revealed:2d} tokens, conf={avg_conf:.3f}):\")\n",
    "        \n",
    "        highlighted_text = []\n",
    "        change_map = {pos: (change_type, token) for pos, change_type, token in changes}\n",
    "        \n",
    "        for pos, token in enumerate(curr_tokens):\n",
    "            if pos in change_map:\n",
    "                change_type, _ = change_map[pos]\n",
    "                color = get_confidence_color(avg_conf)\n",
    "                \n",
    "                if change_type == 'revealed':\n",
    "                    highlighted_text.append(f\"{color}\\033[1m{token}\\033[0m\")\n",
    "                elif change_type == 'masked':\n",
    "                    highlighted_text.append(f\"\\033[2m[MASK]\\033[0m\")\n",
    "                elif change_type == 'changed':\n",
    "                    highlighted_text.append(f\"{color}\\033[4m{token}\\033[0m\")\n",
    "                else:\n",
    "                    highlighted_text.append(f\"{color}{token}\\033[0m\")\n",
    "            else:\n",
    "                if token == '[MASK]':\n",
    "                    highlighted_text.append(f\"\\033[38;5;245m{token}\\033[0m\")\n",
    "                else:\n",
    "                    highlighted_text.append(token)\n",
    "        \n",
    "        print(f\"  {' '.join(highlighted_text)}\")\n",
    "        \n",
    "        if show_details and changes:\n",
    "            change_summary = []\n",
    "            for pos, change_type, token in changes:\n",
    "                if change_type == 'revealed':\n",
    "                    change_summary.append(f\"pos{pos}:Êñ∞Ë¶è({token})\")\n",
    "                elif change_type == 'masked':\n",
    "                    change_summary.append(f\"pos{pos}:„Éû„Çπ„ÇØ\")\n",
    "                elif change_type == 'changed':\n",
    "                    change_summary.append(f\"pos{pos}:Â§âÊõ¥({token})\")\n",
    "            \n",
    "            if change_summary:\n",
    "                print(f\"    Â§âÂåñ: {', '.join(change_summary)}\")\n",
    "        \n",
    "        print()\n",
    "        prev_tokens = curr_tokens\n",
    "\n",
    "def compare_generation_results(results, titles, detailed_analysis=True):\n",
    "    \"\"\"\n",
    "    Ë§áÊï∞„ÅÆÁîüÊàêÁµêÊûú„ÇíË©≥Á¥∞„Å´ÊØîËºÉ\n",
    "    \"\"\"\n",
    "    print(\"ÁîüÊàêÁµêÊûúÊØîËºÉÂàÜÊûê\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Âü∫Êú¨Áµ±Ë®à„ÅÆÊØîËºÉ\n",
    "    print(\"\\nÂü∫Êú¨Áµ±Ë®à:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, (result, title) in enumerate(zip(results, titles)):\n",
    "        text, history = result\n",
    "        avg_conf = np.mean([h['avg_confidence'] for h in history if h['avg_confidence'] > 0])\n",
    "        total_steps = len(history)\n",
    "        final_length = len(text.split())\n",
    "        \n",
    "        print(f\"{title}:\")\n",
    "        print(f\"  ÊúÄÁµÇ„ÉÜ„Ç≠„Çπ„ÉàÈï∑: {final_length}Ë™û\")\n",
    "        print(f\"  Á∑è„Çπ„ÉÜ„ÉÉ„ÉóÊï∞: {total_steps}\")\n",
    "        print(f\"  Âπ≥Âùá‰ø°È†ºÂ∫¶: {avg_conf:.3f}\")\n",
    "        print(f\"  ÊúÄÁµÇÁµêÊûú: {text}\")\n",
    "        print()\n",
    "    \n",
    "    if detailed_analysis:\n",
    "        print(\"\\nË©≥Á¥∞ÂàÜÊûê:\")\n",
    "        print(\"-\" * 40)\n",
    "        for i, (result, title) in enumerate(zip(results, titles)):\n",
    "            text, history = result\n",
    "            print(f\"\\n{title}„ÅÆÁîüÊàê„Éó„É≠„Çª„ÇπÔºàÊúÄÂæå„ÅÆ5„Çπ„ÉÜ„ÉÉ„ÉóÔºâ:\")\n",
    "            visualize_generation_with_highlights(history[-5:], f\"{title}„ÅÆË©≥Á¥∞\", show_details=False)\n",
    "\n",
    "print(\"ÂèØË¶ñÂåñÈñ¢Êï∞„ÅÆÂÆöÁæ©„ÅåÂÆå‰∫Ü„Åó„Åæ„Åó„Åü\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Êó•Êú¨Ë™û„Åß„ÅÆÂü∫Êú¨ÁöÑ„Å™ÁîüÊàêÂÆüÈ®ì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Êó•Êú¨Ë™û„Åß„ÅÆÂü∫Êú¨ÁöÑ„Å™ÁîüÊàêÂÆüÈ®ì\n",
    "japanese_prompts = [\n",
    "    \"Êó•Êú¨„ÅÆÈ¶ñÈÉΩ„ÅØ„Å©„Åì„Åß„Åô„ÅãÔºü\",\n",
    "    \"Ê°ú„ÅÆÂ≠£ÁØÄ„Å´„Å§„ÅÑ„Å¶Êïô„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\",\n",
    "    \"„Åä„ÅÑ„Åó„ÅÑ„É©„Éº„É°„É≥„ÅÆ‰Ωú„ÇäÊñπ„ÇíË™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\"\n",
    "]\n",
    "\n",
    "print(\"=== Êó•Êú¨Ë™û„Åß„ÅÆÂü∫Êú¨ÁöÑ„Å™ÁîüÊàêÂÆüÈ®ì ===\")\n",
    "japanese_results = []\n",
    "\n",
    "for i, prompt in enumerate(japanese_prompts):\n",
    "    print(f\"\\n{i+1}. „Éó„É≠„É≥„Éó„Éà: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    result, history = generate_llada(\n",
    "        model, prompt,\n",
    "        steps=16, gen_length=64, block_length=32,\n",
    "        temperature=0.0, cfg_scale=0.0, remasking='low_confidence'\n",
    "    )\n",
    "    \n",
    "    japanese_results.append((result, history))\n",
    "    print(f\"\\nÁîüÊàêÁµêÊûú: {result}\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Êó•Êú¨Ë™ûÁîüÊàê„Éó„É≠„Çª„Çπ„ÅÆË©≥Á¥∞ÂèØË¶ñÂåñ\n",
    "print(\"=== Êó•Êú¨Ë™ûÁîüÊàê„Éó„É≠„Çª„Çπ„ÅÆË©≥Á¥∞ÂàÜÊûê ===\")\n",
    "\n",
    "for i, (prompt, (result, history)) in enumerate(zip(japanese_prompts, japanese_results)):\n",
    "    print(f\"\\n{i+1}. {prompt}\")\n",
    "    print(\"ÁîüÊàê„Éó„É≠„Çª„ÇπÔºàÊúÄÂæå„ÅÆ6„Çπ„ÉÜ„ÉÉ„ÉóÔºâ:\")\n",
    "    visualize_generation_with_highlights(\n",
    "        history[-6:], \n",
    "        f\"Êó•Êú¨Ë™ûÁîüÊàê„Éó„É≠„Çª„Çπ {i+1}\",\n",
    "        show_details=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ëã±Ë™ûvsÊó•Êú¨Ë™û„ÅÆÊØîËºÉÂÆüÈ®ì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂØæÂøú„Åô„ÇãËã±Ë™û„Éó„É≠„É≥„Éó„Éà„ÅßÂêå„ÅòÂÆüÈ®ì\n",
    "english_prompts = [\n",
    "    \"What is the capital of Japan?\",\n",
    "    \"Please tell me about cherry blossom season.\",\n",
    "    \"Please explain how to make delicious ramen.\"\n",
    "]\n",
    "\n",
    "print(\"=== Ëã±Ë™û„Åß„ÅÆÂØæÂøúÂÆüÈ®ì ===\")\n",
    "english_results = []\n",
    "\n",
    "for i, prompt in enumerate(english_prompts):\n",
    "    print(f\"\\n{i+1}. „Éó„É≠„É≥„Éó„Éà: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    result, history = generate_llada(\n",
    "        model, prompt,\n",
    "        steps=16, gen_length=64, block_length=32,\n",
    "        temperature=0.0, cfg_scale=0.0, remasking='low_confidence'\n",
    "    )\n",
    "    \n",
    "    english_results.append((result, history))\n",
    "    print(f\"\\nÁîüÊàêÁµêÊûú: {result}\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Êó•Êú¨Ë™ûvsËã±Ë™û„ÅÆË©≥Á¥∞ÊØîËºÉ\n",
    "print(\"=== Êó•Êú¨Ë™û vs Ëã±Ë™û Ë©≥Á¥∞ÊØîËºÉ ===\")\n",
    "\n",
    "comparison_topics = [\n",
    "    \"Êó•Êú¨„ÅÆÈ¶ñÈÉΩ„Å´„Å§„ÅÑ„Å¶\",\n",
    "    \"Ê°ú„ÅÆÂ≠£ÁØÄ„Å´„Å§„ÅÑ„Å¶\", \n",
    "    \"„É©„Éº„É°„É≥„ÅÆ‰Ωú„ÇäÊñπ„Å´„Å§„ÅÑ„Å¶\"\n",
    "]\n",
    "\n",
    "for i in range(len(japanese_prompts)):\n",
    "    print(f\"\\n=== {comparison_topics[i]} ===\")\n",
    "    \n",
    "    # Âü∫Êú¨Áµ±Ë®àÊØîËºÉ\n",
    "    jp_text, jp_history = japanese_results[i]\n",
    "    en_text, en_history = english_results[i]\n",
    "    \n",
    "    jp_avg_conf = np.mean([h['avg_confidence'] for h in jp_history if h['avg_confidence'] > 0])\n",
    "    en_avg_conf = np.mean([h['avg_confidence'] for h in en_history if h['avg_confidence'] > 0])\n",
    "    \n",
    "    jp_tokens = len(jp_text.split())\n",
    "    en_tokens = len(en_text.split())\n",
    "    \n",
    "    print(f\"\\nÁµ±Ë®àÊØîËºÉ:\")\n",
    "    print(f\"  Êó•Êú¨Ë™û: {jp_tokens}Ë™û, Âπ≥Âùá‰ø°È†ºÂ∫¶ {jp_avg_conf:.3f}\")\n",
    "    print(f\"  Ëã±Ë™û:   {en_tokens}Ë™û, Âπ≥Âùá‰ø°È†ºÂ∫¶ {en_avg_conf:.3f}\")\n",
    "    \n",
    "    print(f\"\\nÊúÄÁµÇÁµêÊûúÊØîËºÉ:\")\n",
    "    print(f\"  Êó•Êú¨Ë™û: {jp_text}\")\n",
    "    print(f\"  Ëã±Ë™û:   {en_text}\")\n",
    "    \n",
    "    # ÁîüÊàê„Éó„É≠„Çª„ÇπÊØîËºÉÔºàÊúÄÂæå„ÅÆ4„Çπ„ÉÜ„ÉÉ„ÉóÔºâ\n",
    "    print(f\"\\nÁîüÊàê„Éó„É≠„Çª„ÇπÊØîËºÉÔºàÊúÄÂæå„ÅÆ4„Çπ„ÉÜ„ÉÉ„ÉóÔºâ:\")\n",
    "    print(\"\\nÊó•Êú¨Ë™û:\")\n",
    "    visualize_generation_with_highlights(\n",
    "        jp_history[-4:], \"Êó•Êú¨Ë™ûÁîüÊàê„Éó„É≠„Çª„Çπ\", show_details=False\n",
    "    )\n",
    "    \n",
    "    print(\"\\nËã±Ë™û:\")\n",
    "    visualize_generation_with_highlights(\n",
    "        en_history[-4:], \"Ëã±Ë™ûÁîüÊàê„Éó„É≠„Çª„Çπ\", show_details=False\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Êó•Êú¨Ë™ûÁâπÊúâ„ÅÆÁèæË±°„ÅÆÂàÜÊûê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Êó•Êú¨Ë™ûÁâπÊúâ„ÅÆË§áÈõë„Å™Ë°®Áèæ„Åß„ÅÆÂÆüÈ®ì\n",
    "complex_japanese_prompts = [\n",
    "    \"„ÄéÂêæËº©„ÅØÁå´„Åß„ÅÇ„Çã„Äè„ÅÆÈ≠ÖÂäõ„Å´„Å§„ÅÑ„Å¶Ë´ñ„Åò„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\",\n",
    "    \"Êó•Êú¨„ÅÆÂõõÂ≠£„ÅÆÁæé„Åó„Åï„Çí‰ø≥Âè•„ÅßË°®Áèæ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\", \n",
    "    \"Èñ¢Ë•øÂºÅ„ÅßÂ§ßÈò™„ÅÆÈ≠ÖÂäõ„ÇíË™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\",\n",
    "    \"Êï¨Ë™û„Çí‰Ωø„Å£„Å¶‰ºöÁ§æ„Åß„ÅÆÊå®Êã∂„ÅÆ‰ªïÊñπ„ÇíÊïô„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\"\n",
    "]\n",
    "\n",
    "print(\"=== Êó•Êú¨Ë™ûÁâπÊúâË°®Áèæ„Åß„ÅÆÁîüÊàêÂÆüÈ®ì ===\")\n",
    "complex_results = []\n",
    "\n",
    "for i, prompt in enumerate(complex_japanese_prompts):\n",
    "    print(f\"\\n{i+1}. „Éó„É≠„É≥„Éó„Éà: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    result, history = generate_llada(\n",
    "        model, prompt,\n",
    "        steps=20, gen_length=80, block_length=40,\n",
    "        temperature=0.2, cfg_scale=0.0, remasking='low_confidence'\n",
    "    )\n",
    "    \n",
    "    complex_results.append((result, history))\n",
    "    print(f\"\\nÁîüÊàêÁµêÊûú: {result}\")\n",
    "    \n",
    "    # ‰ø°È†ºÂ∫¶ÂàÜÊûê\n",
    "    avg_conf = np.mean([h['avg_confidence'] for h in history if h['avg_confidence'] > 0])\n",
    "    print(f\"Âπ≥Âùá‰ø°È†ºÂ∫¶: {avg_conf:.3f}\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Êó•Êú¨Ë™ûË°®Áèæ„ÅÆË§áÈõë„Åï„Å®ÁîüÊàêÂìÅË≥™„ÅÆÈñ¢‰øÇÂàÜÊûê\n",
    "print(\"=== Êó•Êú¨Ë™ûË°®Áèæ„ÅÆË§áÈõë„ÅïÂàÜÊûê ===\")\n",
    "\n",
    "# ÂêÑÁµêÊûú„ÅÆË©≥Á¥∞ÂàÜÊûê\n",
    "analysis_labels = [\n",
    "    \"ÊñáÂ≠¶ÁöÑË°®Áèæ\",\n",
    "    \"ÈüªÂæãÁöÑË°®ÁèæÔºà‰ø≥Âè•Ôºâ\",\n",
    "    \"ÊñπË®ÄË°®ÁèæÔºàÈñ¢Ë•øÂºÅÔºâ\", \n",
    "    \"Êï¨Ë™ûË°®Áèæ\"\n",
    "]\n",
    "\n",
    "for i, ((result, history), label) in enumerate(zip(complex_results, analysis_labels)):\n",
    "    print(f\"\\n=== {label} ===\")\n",
    "    \n",
    "    # Áµ±Ë®àÊÉÖÂ†±\n",
    "    avg_conf = np.mean([h['avg_confidence'] for h in history if h['avg_confidence'] > 0])\n",
    "    conf_std = np.std([h['avg_confidence'] for h in history if h['avg_confidence'] > 0])\n",
    "    total_steps = len(history)\n",
    "    final_length = len(result.split())\n",
    "    \n",
    "    print(f\"Áµ±Ë®à:\")\n",
    "    print(f\"  ÊúÄÁµÇÈï∑: {final_length}Ë™û\")\n",
    "    print(f\"  „Çπ„ÉÜ„ÉÉ„ÉóÊï∞: {total_steps}\")\n",
    "    print(f\"  Âπ≥Âùá‰ø°È†ºÂ∫¶: {avg_conf:.3f} (¬±{conf_std:.3f})\")\n",
    "    print(f\"  ÁµêÊûú: {result}\")\n",
    "    \n",
    "    print(f\"\\nÁîüÊàê„Éó„É≠„Çª„ÇπÔºàÊúÄÂæå„ÅÆ5„Çπ„ÉÜ„ÉÉ„ÉóÔºâ:\")\n",
    "    visualize_generation_with_highlights(\n",
    "        history[-5:], f\"{label}„ÅÆÁîüÊàê„Éó„É≠„Çª„Çπ\", show_details=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Áï∞„Å™„ÇãÊó•Êú¨Ë™û„Çø„Çπ„ÇØ„Åß„ÅÆÊÄßËÉΩÊØîËºÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Áï∞„Å™„Çã„Çø„Ç§„Éó„ÅÆÊó•Êú¨Ë™û„Çø„Çπ„ÇØ„Åß„ÅÆÊØîËºÉÂÆüÈ®ì\n",
    "task_experiments = {\n",
    "    \"Êï∞Â≠¶ÂïèÈ°å\": \"12 + 15 = ? „Åì„ÅÆË®àÁÆó„ÇíÊÆµÈöéÁöÑ„Å´Ëß£„ÅÑ„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\",\n",
    "    \"ÊñôÁêÜ„É¨„Ç∑„Éî\": \"ÂçµÁÑº„Åç„ÅÆ‰Ωú„ÇäÊñπ„ÇíÁ∞°ÊΩî„Å´Êïô„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\",\n",
    "    \"ÁßëÂ≠¶Ë™¨Êòé\": \"ÂÖâÂêàÊàê„ÅÆ„Åó„Åè„Åø„ÇíÂàÜ„Åã„Çä„ÇÑ„Åô„ÅèË™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\",\n",
    "    \"ÊñáÂåñÁ¥π‰ªã\": \"Êó•Êú¨„ÅÆËå∂ÈÅì„ÅÆÁ≤æÁ•û„Å´„Å§„ÅÑ„Å¶Ë™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\",\n",
    "    \"ÂÆüÁî®ÊÉÖÂ†±\": \"Êù±‰∫¨ÈßÖ„Åã„ÇâÁæΩÁî∞Á©∫Ê∏Ø„Å∏„ÅÆË°å„ÅçÊñπ„ÇíÊïô„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\"\n",
    "}\n",
    "\n",
    "print(\"=== Áï∞„Å™„ÇãÊó•Êú¨Ë™û„Çø„Çπ„ÇØ„Åß„ÅÆÊÄßËÉΩÊØîËºÉ ===\")\n",
    "task_results = {}\n",
    "\n",
    "for task_name, prompt in task_experiments.items():\n",
    "    print(f\"\\n„Äê{task_name}„Äë\")\n",
    "    print(f\"„Éó„É≠„É≥„Éó„Éà: {prompt}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    result, history = generate_llada(\n",
    "        model, prompt,\n",
    "        steps=18, gen_length=72, block_length=36,\n",
    "        temperature=0.1, cfg_scale=0.0, remasking='low_confidence'\n",
    "    )\n",
    "    \n",
    "    task_results[task_name] = (result, history)\n",
    "    \n",
    "    # Âç≥Â∫ß„Å´ÁµêÊûúË°®Á§∫\n",
    "    avg_conf = np.mean([h['avg_confidence'] for h in history if h['avg_confidence'] > 0])\n",
    "    print(f\"\\nÁµêÊûú: {result}\")\n",
    "    print(f\"Âπ≥Âùá‰ø°È†ºÂ∫¶: {avg_conf:.3f}\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# „Çø„Çπ„ÇØÂà•ÊÄßËÉΩÂàÜÊûê„Å®„Ç∞„É©„ÉïÂåñ\n",
    "print(\"=== „Çø„Çπ„ÇØÂà•ÊÄßËÉΩÂàÜÊûê ===\")\n",
    "\n",
    "# Áµ±Ë®à„Éá„Éº„Çø„ÅÆÂèéÈõÜ\n",
    "task_stats = {}\n",
    "for task_name, (result, history) in task_results.items():\n",
    "    avg_conf = np.mean([h['avg_confidence'] for h in history if h['avg_confidence'] > 0])\n",
    "    conf_std = np.std([h['avg_confidence'] for h in history if h['avg_confidence'] > 0])\n",
    "    total_steps = len(history)\n",
    "    final_length = len(result.split())\n",
    "    \n",
    "    task_stats[task_name] = {\n",
    "        'avg_confidence': avg_conf,\n",
    "        'conf_std': conf_std,\n",
    "        'total_steps': total_steps,\n",
    "        'final_length': final_length,\n",
    "        'result': result\n",
    "    }\n",
    "\n",
    "# Áµ±Ë®àË°®„ÅÆË°®Á§∫\n",
    "print(\"\\n„Çø„Çπ„ÇØÂà•Áµ±Ë®à:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'„Çø„Çπ„ÇØ':<12} {'Ë™ûÊï∞':<6} {'„Çπ„ÉÜ„ÉÉ„Éó':<8} {'Âπ≥Âùá‰ø°È†ºÂ∫¶':<12} {'‰ø°È†ºÂ∫¶SD':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for task_name, stats in task_stats.items():\n",
    "    print(f\"{task_name:<12} {stats['final_length']:<6} {stats['total_steps']:<8} \"\n",
    "          f\"{stats['avg_confidence']:<12.3f} {stats['conf_std']:<10.3f}\")\n",
    "\n",
    "# „Ç∞„É©„ÉïÂåñ\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Âπ≥Âùá‰ø°È†ºÂ∫¶„ÅÆÊØîËºÉ\n",
    "task_names = list(task_stats.keys())\n",
    "confidences = [task_stats[name]['avg_confidence'] for name in task_names]\n",
    "conf_stds = [task_stats[name]['conf_std'] for name in task_names]\n",
    "\n",
    "bars1 = ax1.bar(range(len(task_names)), confidences, \n",
    "                yerr=conf_stds, capsize=5, \n",
    "                color=['#2E8B57', '#8FBC8F', '#CD853F', '#DEB887', '#708090'])\n",
    "ax1.set_xlabel('„Çø„Çπ„ÇØÁ®ÆÂà•')\n",
    "ax1.set_ylabel('Âπ≥Âùá‰ø°È†ºÂ∫¶')\n",
    "ax1.set_title('„Çø„Çπ„ÇØÂà•Âπ≥Âùá‰ø°È†ºÂ∫¶')\n",
    "ax1.set_xticks(range(len(task_names)))\n",
    "ax1.set_xticklabels(task_names, rotation=45, ha='right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ÊúÄÁµÇË™ûÊï∞„ÅÆÊØîËºÉ\n",
    "lengths = [task_stats[name]['final_length'] for name in task_names]\n",
    "bars2 = ax2.bar(range(len(task_names)), lengths,\n",
    "                color=['#2E8B57', '#8FBC8F', '#CD853F', '#DEB887', '#708090'])\n",
    "ax2.set_xlabel('„Çø„Çπ„ÇØÁ®ÆÂà•')\n",
    "ax2.set_ylabel('ÁîüÊàêË™ûÊï∞')\n",
    "ax2.set_title('„Çø„Çπ„ÇØÂà•ÁîüÊàêË™ûÊï∞')\n",
    "ax2.set_xticks(range(len(task_names)))\n",
    "ax2.set_xticklabels(task_names, rotation=45, ha='right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂêÑ„Çø„Çπ„ÇØ„ÅÆË©≥Á¥∞„Å™ÁîüÊàê„Éó„É≠„Çª„ÇπÂàÜÊûê\n",
    "print(\"=== ÂêÑ„Çø„Çπ„ÇØ„ÅÆÁîüÊàê„Éó„É≠„Çª„ÇπË©≥Á¥∞ÂàÜÊûê ===\")\n",
    "\n",
    "for task_name, (result, history) in task_results.items():\n",
    "    print(f\"\\n„Äê{task_name}„Äë„ÅÆÁîüÊàê„Éó„É≠„Çª„ÇπÂàÜÊûê\")\n",
    "    print(f\"ÊúÄÁµÇÁµêÊûú: {result}\")\n",
    "    print(\"\\nÁîüÊàê„Éó„É≠„Çª„ÇπÔºàÊúÄÂæå„ÅÆ6„Çπ„ÉÜ„ÉÉ„ÉóÔºâ:\")\n",
    "    \n",
    "    visualize_generation_with_highlights(\n",
    "        history[-6:], \n",
    "        f\"{task_name}„ÅÆË©≥Á¥∞„Éó„É≠„Çª„Çπ\",\n",
    "        show_details=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. „Åæ„Å®„ÇÅ„Å®ËÄÉÂØü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂÖ®‰ΩìÁöÑ„Å™ÂàÜÊûê„Å®„Åæ„Å®„ÇÅ\n",
    "print(\"=== Êó•Êú¨Ë™û„Éó„É≠„É≥„Éó„ÉàÂÆüÈ®ì„ÅÆÁ∑èÂêàÂàÜÊûê ===\")\n",
    "\n",
    "# 1. Ë®ÄË™ûÂà•ÊØîËºÉ„ÅÆË¶ÅÁ¥Ñ\n",
    "print(\"\\n1. Êó•Êú¨Ë™û vs Ëã±Ë™û„ÅÆÊØîËºÉÁµêÊûú:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "jp_confidences = []\n",
    "en_confidences = []\n",
    "jp_lengths = []\n",
    "en_lengths = []\n",
    "\n",
    "for (jp_result, jp_hist), (en_result, en_hist) in zip(japanese_results, english_results):\n",
    "    jp_conf = np.mean([h['avg_confidence'] for h in jp_hist if h['avg_confidence'] > 0])\n",
    "    en_conf = np.mean([h['avg_confidence'] for h in en_hist if h['avg_confidence'] > 0])\n",
    "    \n",
    "    jp_confidences.append(jp_conf)\n",
    "    en_confidences.append(en_conf)\n",
    "    jp_lengths.append(len(jp_result.split()))\n",
    "    en_lengths.append(len(en_result.split()))\n",
    "\n",
    "print(f\"Âπ≥Âùá‰ø°È†ºÂ∫¶ - Êó•Êú¨Ë™û: {np.mean(jp_confidences):.3f}, Ëã±Ë™û: {np.mean(en_confidences):.3f}\")\n",
    "print(f\"Âπ≥ÂùáË™ûÊï∞   - Êó•Êú¨Ë™û: {np.mean(jp_lengths):.1f}Ë™û, Ëã±Ë™û: {np.mean(en_lengths):.1f}Ë™û\")\n",
    "\n",
    "# 2. Êó•Êú¨Ë™û„Çø„Çπ„ÇØÂà•ÂàÜÊûê\n",
    "print(\"\\n2. Êó•Êú¨Ë™û„Çø„Çπ„ÇØÂà•ÊÄßËÉΩ„É©„É≥„Ç≠„É≥„Ç∞:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# ‰ø°È†ºÂ∫¶„Åß„ÇΩ„Éº„Éà\n",
    "sorted_tasks = sorted(task_stats.items(), key=lambda x: x[1]['avg_confidence'], reverse=True)\n",
    "\n",
    "print(\"‰ø°È†ºÂ∫¶È†Ü„É©„É≥„Ç≠„É≥„Ç∞:\")\n",
    "for i, (task_name, stats) in enumerate(sorted_tasks, 1):\n",
    "    print(f\"  {i}. {task_name}: {stats['avg_confidence']:.3f}\")\n",
    "\n",
    "# 3. ÁâπÂæ¥ÁöÑ„Å™Ë¶≥ÂØüÁµêÊûú\n",
    "print(\"\\n3. Ë¶≥ÂØü„Åï„Çå„ÅüÁâπÂæ¥:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "highest_conf_task = sorted_tasks[0][0]\n",
    "lowest_conf_task = sorted_tasks[-1][0]\n",
    "\n",
    "print(f\"‚Ä¢ ÊúÄ„ÇÇÂÆâÂÆö„Åó„ÅüÁîüÊàê: {highest_conf_task}Ôºà‰ø°È†ºÂ∫¶: {sorted_tasks[0][1]['avg_confidence']:.3f}Ôºâ\")\n",
    "print(f\"‚Ä¢ ÊúÄ„ÇÇÊåëÊà¶ÁöÑ„Å™„Çø„Çπ„ÇØ: {lowest_conf_task}Ôºà‰ø°È†ºÂ∫¶: {sorted_tasks[-1][1]['avg_confidence']:.3f}Ôºâ\")\n",
    "\n",
    "# 4. Ë®ÄË™ûÁâπÊÄß„ÅÆÂàÜÊûê\n",
    "print(\"\\n4. Êó•Êú¨Ë™ûÁâπÊúâ„ÅÆÁîüÊàêÁâπÊÄß:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if np.mean(jp_confidences) > np.mean(en_confidences):\n",
    "    print(\"‚Ä¢ Êó•Êú¨Ë™û„Åß„ÅÆÁîüÊàê„ÅÆÊñπ„ÅåÈ´ò„ÅÑ‰ø°È†ºÂ∫¶„ÇíÁ§∫„Åó„Å¶„ÅÑ„Çã\")\n",
    "else:\n",
    "    print(\"‚Ä¢ Ëã±Ë™û„Åß„ÅÆÁîüÊàê„ÅÆÊñπ„ÅåÈ´ò„ÅÑ‰ø°È†ºÂ∫¶„ÇíÁ§∫„Åó„Å¶„ÅÑ„Çã\")\n",
    "\n",
    "if np.mean(jp_lengths) > np.mean(en_lengths):\n",
    "    print(\"‚Ä¢ Êó•Êú¨Ë™û„ÅÆÊñπ„Åå„Çà„ÇäË©≥Á¥∞„Å™ÂøúÁ≠î„ÇíÁîüÊàê„Åô„ÇãÂÇæÂêë\")\n",
    "else:\n",
    "    print(\"‚Ä¢ Ëã±Ë™û„ÅÆÊñπ„Åå„Çà„ÇäË©≥Á¥∞„Å™ÂøúÁ≠î„ÇíÁîüÊàê„Åô„ÇãÂÇæÂêë\")\n",
    "\n",
    "print(\"\\n5. Êé®Â•®„Åï„Çå„Çã‰ΩøÁî®Â†¥Èù¢:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"‚Ä¢ È´òÁ≤æÂ∫¶„ÅåÂøÖË¶Å: {highest_conf_task}„Å™„Å©„ÅÆÊßãÈÄ†Âåñ„Çø„Çπ„ÇØ\")\n",
    "print(f\"‚Ä¢ ÂâµÈÄ†ÊÄß„ÅåÂøÖË¶Å: {lowest_conf_task}„Å™„Å©„ÅÆË°®ÁèæÁöÑ„Çø„Çπ„ÇØ\")\n",
    "print(\"‚Ä¢ ÂÆüÁî®ÁöÑÂøúÁî®: ÊñôÁêÜ„É¨„Ç∑„Éî„ÇÑÂÆüÁî®ÊÉÖÂ†±„Å™„Å©„ÅÆÂÖ∑‰ΩìÁöÑÊåáÁ§∫\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ÂÆüÈ®ìÂÆå‰∫Ü: Êó•Êú¨Ë™û„Éó„É≠„É≥„Éó„Éà„Åß„ÅÆLLaDAÊÄßËÉΩÂàÜÊûê\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÂÆüÈ®ìÁµêÊûú„Çµ„Éû„É™„Éº\n",
    "\n",
    "„Åì„ÅÆ„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ„Åß„ÅØ„ÄÅLLaDA„ÅÆÊó•Êú¨Ë™ûÂá¶ÁêÜËÉΩÂäõ„ÇíÂ§öËßíÁöÑ„Å´ÂàÜÊûê„Åó„Åæ„Åó„ÅüÔºö\n",
    "\n",
    "### ‰∏ªË¶Å„Å™Áô∫Ë¶ã\n",
    "\n",
    "1. **Ë®ÄË™ûÈñìÊØîËºÉ**: Êó•Êú¨Ë™û„Å®Ëã±Ë™û„Åß„ÅÆÁîüÊàêÂìÅË≥™„Å®ÁâπÊÄß„ÅÆÈÅï„ÅÑ\n",
    "2. **„Çø„Çπ„ÇØÂà•ÊÄßËÉΩ**: Áï∞„Å™„ÇãÁ®ÆÈ°û„ÅÆÊó•Êú¨Ë™û„Çø„Çπ„ÇØ„Åß„ÅÆÁõ∏ÂØæÁöÑ„Å™ÊÄßËÉΩ\n",
    "3. **Ë§áÈõëË°®Áèæ**: Êï¨Ë™û„ÄÅÊñπË®Ä„ÄÅÈüªÂæã„Å™„Å©Êó•Êú¨Ë™ûÁâπÊúâ„ÅÆË°®Áèæ„Å∏„ÅÆÂØæÂøú\n",
    "4. **‰ø°È†ºÂ∫¶„Éë„Çø„Éº„É≥**: „Çø„Çπ„ÇØ„Çø„Ç§„Éó„Å´„Çà„Çã‰ø°È†ºÂ∫¶„ÅÆÂ§âÂãï\n",
    "\n",
    "### Ê¥ªÁî®ÊèêÊ°à\n",
    "\n",
    "- **ÊßãÈÄ†Âåñ„Çø„Çπ„ÇØ**: Êï∞Â≠¶„ÄÅÊñôÁêÜ„É¨„Ç∑„Éî„Å™„Å©ÊòéÁ¢∫„Å™ÊâãÈ†Ü„Åå„ÅÇ„Çã„Çø„Çπ„ÇØ\n",
    "- **ÊñáÂåñÁöÑ„Ç≥„É≥„ÉÜ„É≥„ÉÑ**: Êó•Êú¨ÁâπÊúâ„ÅÆÊñáÂåñ„ÇÑÊÖ£Áøí„Å´Èñ¢„Åô„ÇãË™¨Êòé\n",
    "- **ÂÆüÁî®ÁöÑÊÉÖÂ†±**: ‰∫§ÈÄöÊ°àÂÜÖ„ÇÑÂÆüÁî®ÁöÑ„Å™ÊåáÁ§∫\n",
    "\n",
    "### ‰ªäÂæå„ÅÆÁ†îÁ©∂ÊñπÂêë\n",
    "\n",
    "- „Çà„ÇäÈï∑„ÅÑÊó•Êú¨Ë™û„ÉÜ„Ç≠„Çπ„Éà„Åß„ÅÆÊÄßËÉΩË©ï‰æ°\n",
    "- Â∞ÇÈñÄÂàÜÈáéÔºàÊ≥ïÂæã„ÄÅÂåªÁôÇ„Å™„Å©Ôºâ„Åß„ÅÆÊó•Êú¨Ë™ûÂá¶ÁêÜ\n",
    "- Â§öÊßò„Å™Êó•Êú¨Ë™ûÊñπË®Ä„Åß„ÅÆÊØîËºÉÂÆüÈ®ì"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}