{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaDA: æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ˆã‚‹è¿½åŠ å­¦ç¿’ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€LLaDAï¼ˆLarge Language Diffusion with mAskingï¼‰ã‚’æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è¿½åŠ å­¦ç¿’ï¼ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰ã™ã‚‹æ–¹æ³•ã‚’å®Ÿè£…ã—ã¾ã™ã€‚\n",
    "\n",
    "## ğŸ“š ç›®æ¬¡\n",
    "1. [ç’°å¢ƒè¨­å®šã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª](#1-ç’°å¢ƒè¨­å®šã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª)\n",
    "2. [æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™](#2-æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™)\n",
    "3. [LLaDAãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã¨è¨­å®š](#3-lladaãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã¨è¨­å®š)\n",
    "4. [æ‹¡æ•£å­¦ç¿’ã®å®Ÿè£…](#4-æ‹¡æ•£å­¦ç¿’ã®å®Ÿè£…)\n",
    "5. [å­¦ç¿’ã®å®Ÿè¡Œ](#5-å­¦ç¿’ã®å®Ÿè¡Œ)\n",
    "6. [å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡](#6-å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡)\n",
    "7. [ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã¨æ´»ç”¨](#7-ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã¨æ´»ç”¨)\n",
    "\n",
    "## âš ï¸ æ³¨æ„äº‹é …\n",
    "- ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯æ•™è‚²ç›®çš„ã§ã€LLaDAã®å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç†è§£ã™ã‚‹ãŸã‚ã®ã‚‚ã®ã§ã™\n",
    "- å®Ÿéš›ã®å­¦ç¿’ã«ã¯å¤§é‡ã®GPUãƒ¡ãƒ¢ãƒªã¨æ™‚é–“ãŒå¿…è¦ã§ã™\n",
    "- Google Colab Proã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ã®é«˜æ€§èƒ½GPUã‚’æ¨å¥¨ã—ã¾ã™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Google Colabã§ã®å®Ÿè¡Œç”¨\nimport sys\n\n# Google Colabã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    print(\"Google Colabç’°å¢ƒã‚’æ¤œå‡ºã—ã¾ã—ãŸ\")\n    \n    # å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n    !pip install transformers==4.49.0 accelerate==0.34.2 torch numpy matplotlib\n    !pip install datasets==2.18.0 wandb tqdm scikit-learn\n    # LoRAç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’è¿½åŠ \n    !pip install peft==0.13.2 bitsandbytes==0.44.1\n    \n    # GPUãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯\n    import torch\n    print(f\"CUDA available: {torch.cuda.is_available()}\")\n    if torch.cuda.is_available():\n        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n        \n        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯\n        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n        if gpu_memory < 12:\n            print(\"âš ï¸ è­¦å‘Š: GPUãƒ¡ãƒ¢ãƒªãŒ12GBæœªæº€ã§ã™ã€‚å­¦ç¿’æ™‚ã«ãƒ¡ãƒ¢ãƒªä¸è¶³ãŒç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\")\n            print(\"   æ¨å¥¨: Google Colab Pro ã¾ãŸã¯ A100 GPU\")\n        else:\n            print(\"âœ… ååˆ†ãªGPUãƒ¡ãƒ¢ãƒªãŒåˆ©ç”¨å¯èƒ½ã§ã™\")\n            \n        # LoRAä½¿ç”¨ã§ã®ãƒ¡ãƒ¢ãƒªæ¨å®š\n        print(f\"\\n=== LoRAä½¿ç”¨æ™‚ã®ãƒ¡ãƒ¢ãƒªæœ€é©åŒ– ===\")\n        print(\"âœ… ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡çš„å­¦ç¿’ã«ã‚ˆã‚Šå¤§å¹…ãªãƒ¡ãƒ¢ãƒªå‰Šæ¸›ãŒæœŸå¾…ã•ã‚Œã¾ã™\")\n        print(f\"   æ¨å®šãƒ¡ãƒ¢ãƒªå‰Šæ¸›: 90%ä»¥ä¸Š\")\n        print(f\"   å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: å…¨ä½“ã®1%æœªæº€\")\nelse:\n    print(\"ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å®Ÿè¡Œä¸­ã§ã™\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom transformers import (\n    AutoTokenizer, AutoModel, AutoConfig,\n    get_linear_schedule_with_warmup\n)\nfrom datasets import load_dataset, Dataset as HFDataset\nimport json\nimport random\nfrom tqdm import tqdm\nimport time\nimport os\nfrom typing import List, Dict, Tuple, Optional\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# LoRAç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\ntry:\n    from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n    PEFT_AVAILABLE = True\n    print(\"âœ… PEFT (LoRA) ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨å¯èƒ½ã§ã™\")\nexcept ImportError:\n    PEFT_AVAILABLE = False\n    print(\"âš ï¸ PEFT ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n    print(\"   ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n\n# æ··åˆç²¾åº¦å­¦ç¿’ç”¨\ntry:\n    from torch.cuda.amp import autocast, GradScaler\n    AMP_AVAILABLE = torch.cuda.is_available()\n    print(\"âœ… æ··åˆç²¾åº¦å­¦ç¿’ãŒåˆ©ç”¨å¯èƒ½ã§ã™\" if AMP_AVAILABLE else \"âŒ CUDAä¸ä½¿ç”¨ã®ãŸã‚æ··åˆç²¾åº¦ã¯ç„¡åŠ¹\")\nexcept ImportError:\n    AMP_AVAILABLE = False\n    print(\"âŒ æ··åˆç²¾åº¦å­¦ç¿’ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“\")\n\n# å†ç¾æ€§ã®ãŸã‚ã®ã‚·ãƒ¼ãƒ‰è¨­å®š\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\nprint(\"ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸ\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoConfig,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# å†ç¾æ€§ã®ãŸã‚ã®ã‚·ãƒ¼ãƒ‰è¨­å®š\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "print(\"ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™\n",
    "\n",
    "æ—¥æœ¬èªã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã—ã¾ã™ã€‚è¤‡æ•°ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰é«˜å“è³ªãªæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’åé›†ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# æ—¥æœ¬èªAlpacaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¨­å®š\nclass JapaneseDatasetConfig:\n    \"\"\"æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¨­å®šã‚¯ãƒ©ã‚¹\"\"\"\n    \n    # åˆ©ç”¨å¯èƒ½ãªæ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n    DATASETS = {\n        'alpaca_ja_fujiki': {\n            'name': 'fujiki/japanese_alpaca_data',\n            'description': 'æ—¥æœ¬èªAlpacaæŒ‡ç¤ºå¿œç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (52K samples)',\n            'format': 'alpaca'\n        },\n        'wikipedia_ja': {\n            'name': 'wikipedia',\n            'config': '20231101.ja',\n            'text_column': 'text',\n            'description': 'æ—¥æœ¬èªWikipedia',\n            'format': 'text'\n        },\n        'cc100_ja': {\n            'name': 'cc100',\n            'config': 'ja', \n            'text_column': 'text',\n            'description': 'Common Crawlæ—¥æœ¬èª',\n            'format': 'text'\n        },\n        'oscar_ja': {\n            'name': 'oscar',\n            'config': 'unshuffled_deduplicated_ja',\n            'text_column': 'text', \n            'description': 'OSCARæ—¥æœ¬èªã‚³ãƒ¼ãƒ‘ã‚¹',\n            'format': 'text'\n        }\n    }\n    \n    # ã‚µãƒ³ãƒ—ãƒ«æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰\n    SAMPLE_ALPACA_DATA = [\n        {\n            \"instruction\": \"æ—¥æœ¬ã®é¦–éƒ½ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\",\n            \"input\": \"\",\n            \"output\": \"æ—¥æœ¬ã®é¦–éƒ½ã¯æ±äº¬ã§ã™ã€‚æ±äº¬ã¯é–¢æ±åœ°æ–¹ã«ä½ç½®ã—ã€æ—¥æœ¬ã®æ”¿æ²»ã€çµŒæ¸ˆã€æ–‡åŒ–ã®ä¸­å¿ƒåœ°ã¨ã—ã¦æ©Ÿèƒ½ã—ã¦ã„ã¾ã™ã€‚äººå£ã¯ç´„1400ä¸‡äººã§ã€ä¸–ç•Œæœ€å¤§ç´šã®éƒ½å¸‚åœã‚’å½¢æˆã—ã¦ã„ã¾ã™ã€‚\"\n        },\n        {\n            \"instruction\": \"ä»¥ä¸‹ã®æ–‡ç« ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚\",\n            \"input\": \"æ¡œã¯æ—¥æœ¬ã®è±¡å¾´çš„ãªèŠ±ã¨ã—ã¦çŸ¥ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚æ˜¥ã«ãªã‚‹ã¨å…¨å›½å„åœ°ã§æ¡œãŒå’²ãã€å¤šãã®äººã€…ãŒãŠèŠ±è¦‹ã‚’æ¥½ã—ã¿ã¾ã™ã€‚æ¡œã®é–‹èŠ±ã¯å—ã‹ã‚‰åŒ—ã¸ã¨é€²ã¿ã€æ¡œå‰ç·šã¨å‘¼ã°ã‚Œã¾ã™ã€‚\",\n            \"output\": \"æ¡œã¯æ—¥æœ¬ã®è±¡å¾´çš„ãªèŠ±ã§ã€æ˜¥ã®å…¨å›½çš„ãªé–‹èŠ±ï¼ˆæ¡œå‰ç·šï¼‰ã«ã‚ˆã‚Šå¤šãã®äººã€…ãŒãŠèŠ±è¦‹ã‚’æ¥½ã—ã‚“ã§ã„ã¾ã™ã€‚\"\n        },\n        {\n            \"instruction\": \"å¥åº·çš„ãªé£Ÿäº‹ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ã—ã¦ãã ã•ã„ã€‚\",\n            \"input\": \"é‹å‹•ä¸è¶³ã§ä½“é‡ãŒå¢—åŠ ã—ã¦ã„ã‚‹\",\n            \"output\": \"ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸé£Ÿäº‹ã‚’å¿ƒãŒã‘ã€é‡èœã‚’å¤šãæ‘‚å–ã—ã€é©åº¦ãªé‹å‹•ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚ã¾ãŸã€è¦å‰‡æ­£ã—ã„é£Ÿäº‹æ™‚é–“ã‚’å®ˆã‚Šã€é–“é£Ÿã‚’æ§ãˆã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\"\n        },\n        {\n            \"instruction\": \"æ¬¡ã®è‹±èªã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚\",\n            \"input\": \"Good morning, how are you today?\",\n            \"output\": \"ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ã€ä»Šæ—¥ã¯ã„ã‹ãŒãŠéã”ã—ã§ã™ã‹ï¼Ÿ\"\n        },\n        {\n            \"instruction\": \"ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°åˆå¿ƒè€…ã«ãŠã™ã™ã‚ã®è¨€èªã‚’æ•™ãˆã¦ãã ã•ã„ã€‚\",\n            \"input\": \"\",\n            \"output\": \"ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°åˆå¿ƒè€…ã«ã¯ã€Pythonã€JavaScriptã€JavaãŒãŠã™ã™ã‚ã§ã™ã€‚Pythonã¯æ–‡æ³•ãŒåˆ†ã‹ã‚Šã‚„ã™ãã€JavaScriptã¯Webã‚µã‚¤ãƒˆåˆ¶ä½œã«æ´»ç”¨ã§ãã€Javaã¯ä¼æ¥­ã‚·ã‚¹ãƒ†ãƒ ã§åºƒãä½¿ã‚ã‚Œã¦ã„ã¾ã™ã€‚\"\n        }\n    ]\n\ndef format_alpaca_instruction(instruction, input_text=\"\", output=\"\"):\n    \"\"\"\n    Alpacaå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ£ãƒƒãƒˆå½¢å¼ã«å¤‰æ›\n    \"\"\"\n    if input_text.strip():\n        prompt = f\"{instruction}\\n\\nå…¥åŠ›: {input_text}\"\n    else:\n        prompt = instruction\n    \n    return prompt, output\n\ndef load_japanese_alpaca_dataset(dataset_name='sample', max_samples=1000, min_length=20):\n    \"\"\"\n    æ—¥æœ¬èªAlpacaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€\n    \n    Args:\n        dataset_name: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå\n        max_samples: æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°\n        min_length: æœ€å°ãƒ†ã‚­ã‚¹ãƒˆé•·\n    \"\"\"\n    print(f\"æ—¥æœ¬èªAlpacaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ '{dataset_name}' ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n    \n    if dataset_name == 'sample':\n        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨\n        texts = []\n        for item in JapaneseDatasetConfig.SAMPLE_ALPACA_DATA:\n            prompt, response = format_alpaca_instruction(\n                item['instruction'], \n                item['input'], \n                item['output']\n            )\n            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨å¿œç­”ã‚’çµåˆ\n            full_text = f\"{prompt}\\n\\n{response}\"\n            texts.append(full_text)\n        \n        # ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’å¢—ã‚„ã™ãŸã‚ã«ç¹°ã‚Šè¿”ã—\n        texts = texts * (max_samples // len(texts) + 1)\n        texts = texts[:max_samples]\n        print(f\"ã‚µãƒ³ãƒ—ãƒ«Alpacaãƒ‡ãƒ¼ã‚¿ {len(texts)} ä»¶ã‚’æº–å‚™ã—ã¾ã—ãŸ\")\n        return texts\n    \n    elif dataset_name == 'alpaca_ja_fujiki':\n        # Hugging Face ã®æ—¥æœ¬èªAlpacaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨\n        try:\n            from datasets import load_dataset\n            dataset = load_dataset(\n                'fujiki/japanese_alpaca_data',\n                split=f'train[:{max_samples}]'\n            )\n            \n            texts = []\n            for item in tqdm(dataset, desc=\"Alpacaãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­\"):\n                instruction = item.get('instruction', '').strip()\n                input_text = item.get('input', '').strip()\n                output = item.get('output', '').strip()\n                \n                if len(instruction) < 5 or len(output) < min_length:\n                    continue\n                \n                prompt, response = format_alpaca_instruction(instruction, input_text, output)\n                full_text = f\"{prompt}\\n\\n{response}\"\n                \n                if len(full_text) >= min_length:\n                    texts.append(full_text)\n                \n                if len(texts) >= max_samples:\n                    break\n            \n            print(f\"æ—¥æœ¬èªAlpacaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã‹ã‚‰ {len(texts)} ä»¶ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ\")\n            return texts\n            \n        except Exception as e:\n            print(f\"Alpacaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}\")\n            print(\"ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™\")\n            return load_japanese_alpaca_dataset('sample', max_samples, min_length)\n    \n    elif dataset_name in JapaneseDatasetConfig.DATASETS:\n        # å¾“æ¥ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n        config = JapaneseDatasetConfig.DATASETS[dataset_name]\n        if config['format'] == 'text':\n            try:\n                dataset = load_dataset(\n                    config['name'], \n                    config.get('config'),\n                    split=f'train[:{max_samples}]',\n                    streaming=True\n                )\n                \n                texts = []\n                for item in tqdm(dataset, desc=\"ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­\", total=max_samples):\n                    text = item[config['text_column']].strip()\n                    if len(text) >= min_length:\n                        texts.append(text)\n                    if len(texts) >= max_samples:\n                        break\n                \n                print(f\"{config['description']} ã‹ã‚‰ {len(texts)} ä»¶ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ\")\n                return texts\n                \n            except Exception as e:\n                print(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}\")\n                print(\"ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™\")\n                return load_japanese_alpaca_dataset('sample', max_samples, min_length)\n    \n    else:\n        raise ValueError(f\"æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_name}\")\n\n# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ã®ãƒ†ã‚¹ãƒˆ\nprint(\"åˆ©ç”¨å¯èƒ½ãªæ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:\")\nfor name, config in JapaneseDatasetConfig.DATASETS.items():\n    print(f\"  - {name}: {config['description']}\")\n\n# ã‚µãƒ³ãƒ—ãƒ«Alpacaãƒ‡ãƒ¼ã‚¿ã§å‹•ä½œç¢ºèª\nsample_texts = load_japanese_alpaca_dataset('sample', max_samples=10)\nprint(f\"\\nã‚µãƒ³ãƒ—ãƒ«Alpacaãƒ†ã‚­ã‚¹ãƒˆä¾‹:\")\nprint(f\"{sample_texts[0][:200]}...\")\n\n# Alpacaå½¢å¼ã®ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º\nsample_item = JapaneseDatasetConfig.SAMPLE_ALPACA_DATA[0]\nprint(f\"\\nAlpacaå½¢å¼ã®ä¾‹:\")\nprint(f\"æŒ‡ç¤º: {sample_item['instruction']}\")\nprint(f\"å…¥åŠ›: {sample_item['input']}\")\nprint(f\"å‡ºåŠ›: {sample_item['output']}\")\n\nprompt, response = format_alpaca_instruction(\n    sample_item['instruction'], \n    sample_item['input'], \n    sample_item['output']\n)\nprint(f\"\\nå¤‰æ›å¾Œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}\")\nprint(f\"å¿œç­”: {response}\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š\nclass TrainingConfig:\n    \"\"\"å­¦ç¿’è¨­å®šã‚¯ãƒ©ã‚¹\"\"\"\n    \n    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š - æ—¥æœ¬èªAlpacaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨\n    DATASET_NAME = 'sample'  # ãƒ¡ãƒ¢ãƒªä¸è¶³å¯¾ç­–ã§ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é–‹å§‹\n    MAX_SAMPLES = 50  # ã•ã‚‰ã«å‰Šæ¸›\n    MIN_TEXT_LENGTH = 50  # Alpacaå½¢å¼ã§ã¯æŒ‡ç¤ºå¿œç­”ã‚»ãƒƒãƒˆãªã®ã§ã‚„ã‚„é•·ã‚ã«\n    \n    # ãƒ¢ãƒ‡ãƒ«è¨­å®š\n    MODEL_NAME = 'GSAI-ML/LLaDA-8B-Instruct'\n    MAX_LENGTH = 128  # ã•ã‚‰ã«çŸ­ç¸®\n    \n    # å­¦ç¿’è¨­å®š - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦–\n    BATCH_SIZE = 1  # æœ€å°ãƒãƒƒãƒã‚µã‚¤ã‚º\n    GRADIENT_ACCUMULATION_STEPS = 2  # å‰Šæ¸›\n    LEARNING_RATE = 1e-5  # å°ã•ã‚ã®å­¦ç¿’ç‡\n    NUM_EPOCHS = 1  # çŸ­ç¸®\n    WARMUP_RATIO = 0.1\n    WEIGHT_DECAY = 0.01\n    \n    # æ‹¡æ•£è¨­å®š\n    MASK_RATIO_RANGE = (0.2, 0.7)  # ãƒã‚¹ã‚¯æ¯”ç‡ã‚’æ§ãˆã‚ã«\n    MASK_ID = 126336  # [MASK]ãƒˆãƒ¼ã‚¯ãƒ³ID\n    \n    # ä¿å­˜è¨­å®š\n    SAVE_STEPS = 25\n    EVAL_STEPS = 10\n    OUTPUT_DIR = './llada_japanese_alpaca_model'\n    \n    # Alpacaç‰¹æœ‰ã®è¨­å®š\n    USE_ALPACA_FORMAT = True\n    INSTRUCTION_RESPONSE_SEPARATOR = \"\\n\\n\"\n    \n    # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®š\n    USE_GRADIENT_CHECKPOINTING = True\n    USE_MIXED_PRECISION = True\n\n# è¨­å®šç¢ºèª\nprint(\"=== æ—¥æœ¬èªAlpacaå­¦ç¿’è¨­å®šï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ç‰ˆï¼‰ ===\")\nprint(f\"  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {TrainingConfig.DATASET_NAME}\")\nprint(f\"  æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°: {TrainingConfig.MAX_SAMPLES}\")\nprint(f\"  ãƒãƒƒãƒã‚µã‚¤ã‚º: {TrainingConfig.BATCH_SIZE}\")\nprint(f\"  å‹¾é…è“„ç©ã‚¹ãƒ†ãƒƒãƒ—: {TrainingConfig.GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"  å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º: {TrainingConfig.BATCH_SIZE * TrainingConfig.GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"  å­¦ç¿’ç‡: {TrainingConfig.LEARNING_RATE}\")\nprint(f\"  ã‚¨ãƒãƒƒã‚¯æ•°: {TrainingConfig.NUM_EPOCHS}\")\nprint(f\"  æœ€å¤§é•·: {TrainingConfig.MAX_LENGTH}\")\nprint(f\"  å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ: {TrainingConfig.USE_GRADIENT_CHECKPOINTING}\")\nprint(f\"  æ··åˆç²¾åº¦: {TrainingConfig.USE_MIXED_PRECISION}\")\n\n# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®¹é‡ã®æ¨å®š\ndataset_info = JapaneseDatasetConfig.DATASETS.get(TrainingConfig.DATASET_NAME, {})\nprint(f\"\\n=== ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ± ===\")\nprint(f\"  åå‰: {'ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿' if TrainingConfig.DATASET_NAME == 'sample' else dataset_info.get('description', 'ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿')}\")\nprint(f\"  äºˆæƒ³ã‚µãƒ³ãƒ—ãƒ«æ•°: {TrainingConfig.MAX_SAMPLES:,}\")\nprint(f\"  å½¢å¼: {'Instruction-Response pairs' if TrainingConfig.USE_ALPACA_FORMAT else 'Raw text'}\")\n\n# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¨å®š\nif torch.cuda.is_available():\n    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n    # å¤§å¹…ã«å‰Šæ¸›ã•ã‚ŒãŸãƒ¡ãƒ¢ãƒªæ¨å®š\n    estimated_memory = TrainingConfig.BATCH_SIZE * TrainingConfig.MAX_LENGTH * 8 * 2 / 1e9  # å¤§å¹…å‰Šæ¸›\n    print(f\"\\n=== ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¨å®š ===\")\n    print(f\"  åˆ©ç”¨å¯èƒ½GPUãƒ¡ãƒ¢ãƒª: {gpu_memory_gb:.1f}GB\")\n    print(f\"  æ¨å®šä½¿ç”¨ãƒ¡ãƒ¢ãƒª: {estimated_memory:.1f}GB\")\n    print(f\"  ç¾åœ¨ã®GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.memory_allocated() / 1e9:.1f}GB\")\n    \n    # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹\n    print(f\"\\n=== ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–æˆ¦ç•¥ ===\")\n    print(\"âœ“ å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½¿ç”¨\")\n    print(\"âœ“ æ··åˆç²¾åº¦å­¦ç¿’\")\n    print(\"âœ“ æœ€å°ãƒãƒƒãƒã‚µã‚¤ã‚º\")\n    print(\"âœ“ çŸ­ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·\")\n    print(\"âœ“ å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\")\n\n# å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚ºã®èª¬æ˜\nprint(f\"\\n=== å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚ºï¼ˆãƒ¡ãƒ¢ãƒªåˆ¶ç´„ç‰ˆï¼‰ ===\")\nprint(\"Phase 1: æ¥µå°ã‚µãƒ³ãƒ—ãƒ«ã§ã®å‹•ä½œç¢ºèª â† ç¾åœ¨\")\nprint(\"Phase 2: LoRAå®Ÿè£…ã§ã®åŠ¹ç‡çš„å­¦ç¿’\")\nprint(\"Phase 3: ã‚ˆã‚Šå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã®æœ¬æ ¼å­¦ç¿’\")\nprint(f\"ç¾åœ¨: Phase 1 ({TrainingConfig.MAX_SAMPLES} ã‚µãƒ³ãƒ—ãƒ«)\")\n\n# è­¦å‘Šã¨æ¨å¥¨äº‹é …\nprint(f\"\\n=== é‡è¦ãªæ³¨æ„ ===\")\nprint(\"âš ï¸ ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ã«ã‚ˆã‚Šã€å­¦ç¿’ã¯æ¦‚å¿µå®Ÿè¨¼ãƒ¬ãƒ™ãƒ«ã§ã™\")\nprint(\"ğŸ’¡ æœ¬æ ¼çš„ãªå­¦ç¿’ã«ã¯ä»¥ä¸‹ãŒæ¨å¥¨ã•ã‚Œã¾ã™:\")\nprint(\"   - LoRA (Low-Rank Adaptation) ã®å®Ÿè£…\")\nprint(\"   - ã‚ˆã‚Šé«˜æ€§èƒ½ãªGPU (80GB A100ç­‰)\")\nprint(\"   - åˆ†æ•£å­¦ç¿’ç’°å¢ƒ\")\nprint(\"   - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡çš„å­¦ç¿’æ‰‹æ³•\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LLaDAãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã¨è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# LoRAå¯¾å¿œLLaDAãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿\nprint(\"=== LoRAå¯¾å¿œLLaDAãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­ ===\")\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n\n# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿\ntokenizer = AutoTokenizer.from_pretrained(\n    TrainingConfig.MODEL_NAME, \n    trust_remote_code=True\n)\n\n# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\nprint(\"ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...\")\nmodel = AutoModel.from_pretrained(\n    TrainingConfig.MODEL_NAME,\n    trust_remote_code=True,\n    torch_dtype=torch.bfloat16,\n    device_map='auto'\n)\n\nprint(f\"ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {sum(p.numel() for p in model.parameters()) / 1e9:.1f}B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\")\n\n# LoRAè¨­å®šã®é©ç”¨\nif PEFT_AVAILABLE:\n    print(\"\\n=== LoRAè¨­å®šã‚’é©ç”¨ä¸­ ===\")\n    \n    # LoRAè¨­å®š\n    lora_config = LoraConfig(\n        task_type=TaskType.CAUSAL_LM,\n        r=16,  # LoRAã®ãƒ©ãƒ³ã‚¯\n        lora_alpha=32,  # LoRAã‚¢ãƒ«ãƒ•ã‚¡\n        lora_dropout=0.1,  # LoRAãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ\n        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # å¯¾è±¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n        bias=\"none\",\n        inference_mode=False,\n    )\n    \n    # LoRAãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ\n    model = get_peft_model(model, lora_config)\n    \n    # å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç¢ºèª\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    all_params = sum(p.numel() for p in model.parameters())\n    \n    print(f\"âœ… LoRAé©ç”¨å®Œäº†!\")\n    print(f\"  å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {all_params / 1e9:.1f}B\")\n    print(f\"  å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {trainable_params / 1e6:.1f}M\")\n    print(f\"  å­¦ç¿’å¯èƒ½ç‡: {trainable_params / all_params * 100:.2f}%\")\n    \n    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç¢ºèª\n    if torch.cuda.is_available():\n        current_memory = torch.cuda.memory_allocated() / 1e9\n        print(f\"  ç¾åœ¨ã®GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {current_memory:.1f}GB\")\n    \nelse:\n    print(\"âš ï¸ PEFTåˆ©ç”¨ä¸å¯ã®ãŸã‚ã€é€šå¸¸ã®å­¦ç¿’ã‚’è¡Œã„ã¾ã™\")\n\n# å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®å®‰å…¨ãªæœ‰åŠ¹åŒ–\nprint(f\"\\n=== ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®š ===\")\nif TrainingConfig.USE_GRADIENT_CHECKPOINTING:\n    # ã‚ˆã‚Šå®‰å…¨ãªãƒã‚§ãƒƒã‚¯æ–¹æ³•\n    supports_gc = False\n    if hasattr(model, 'supports_gradient_checkpointing'):\n        supports_gc = model.supports_gradient_checkpointing\n    elif hasattr(model, 'gradient_checkpointing_enable'):\n        # é–¢æ•°ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n        try:\n            # ãƒ†ã‚¹ãƒˆå‘¼ã³å‡ºã—ï¼ˆå®Ÿéš›ã«ã¯å®Ÿè¡Œã—ãªã„ï¼‰\n            supports_gc = callable(getattr(model, 'gradient_checkpointing_enable', None))\n        except:\n            supports_gc = False\n    \n    if supports_gc:\n        try:\n            model.gradient_checkpointing_enable()\n            print(\"âœ… å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæœ‰åŠ¹åŒ–\")\n        except Exception as e:\n            print(f\"âš ï¸ å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæœ‰åŠ¹åŒ–ã«å¤±æ•—: {e}\")\n            print(\"   LoRAã¨æ··åˆç²¾åº¦ã«ã‚ˆã‚Šååˆ†ãªãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãŒæœŸå¾…ã•ã‚Œã¾ã™\")\n    else:\n        print(\"âš ï¸ LLaDAãƒ¢ãƒ‡ãƒ«ã¯å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“\")\n        print(\"   LoRAã«ã‚ˆã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸›ã§ååˆ†ãªãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãŒå®Ÿç¾ã•ã‚Œã¾ã™\")\n        print(f\"   å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {trainable_params / 1e6:.1f}M ã®ã¿\")\nelse:\n    print(\"å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ: ç„¡åŠ¹\")\n\n# ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã®ç¢ºèª\nprint(f\"\\n=== ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚µãƒãƒªãƒ¼ ===\")\noptimizations = []\nif PEFT_AVAILABLE:\n    optimizations.append(\"âœ… LoRA (ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡åŒ–)\")\nif TrainingConfig.USE_MIXED_PRECISION:\n    optimizations.append(\"âœ… æ··åˆç²¾åº¦å­¦ç¿’ (FP16)\")\nif supports_gc:\n    optimizations.append(\"âœ… å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ\")\nelse:\n    optimizations.append(\"âš ï¸ å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæœªå¯¾å¿œ\")\n\nfor opt in optimizations:\n    print(f\"  {opt}\")\n\nif PEFT_AVAILABLE:\n    print(f\"\\nğŸ’¡ LoRAã®åŠ¹æœ:\")\n    print(f\"  - å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: 99%ä»¥ä¸Šå‰Šæ¸›\")\n    print(f\"  - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: å¤§å¹…å‰Šæ¸›\")\n    print(f\"  - å­¦ç¿’é€Ÿåº¦: å‘ä¸Š\")\n\n# å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š\nmodel.train()\n\nprint(f\"\\nãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æƒ…å ±:\")\nprint(f\"  èªå½™ã‚µã‚¤ã‚º: {len(tokenizer)}\")\nprint(f\"  æœ€å¤§é•·: {tokenizer.model_max_length}\")\nprint(f\"  ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ID: {TrainingConfig.MASK_ID}\")\n\n# æ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®ãƒ†ã‚¹ãƒˆ\ntest_japanese = \"ã“ã‚“ã«ã¡ã¯ã€ä¸–ç•Œï¼ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã­ã€‚\"\ntokens = tokenizer.encode(test_japanese)\ndecoded = tokenizer.decode(tokens)\nprint(f\"\\næ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãƒ†ã‚¹ãƒˆ:\")\nprint(f\"  å…ƒãƒ†ã‚­ã‚¹ãƒˆ: {test_japanese}\")\nprint(f\"  ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(tokens)}\")\nprint(f\"  å¾©å…ƒãƒ†ã‚­ã‚¹ãƒˆ: {decoded}\")\n\nprint(\"\\n=== ãƒ¢ãƒ‡ãƒ«æº–å‚™å®Œäº† ===\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ‹¡æ•£å­¦ç¿’ã®å®Ÿè£…\n",
    "\n",
    "LLaDAã®æ‹¡æ•£ãƒ—ãƒ­ã‚»ã‚¹ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹ã¨å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè£…ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "class LLaDAAlpacaDiffusionDataset(Dataset):\n    \"\"\"\n    LLaDAæ‹¡æ•£å­¦ç¿’ç”¨Alpacaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹\n    æŒ‡ç¤ºå¿œç­”ãƒšã‚¢ã‚’åŠ¹æœçš„ã«å­¦ç¿’ã™ã‚‹ãŸã‚ã®æ”¹è‰¯ç‰ˆ\n    \"\"\"\n    \n    def __init__(self, texts, tokenizer, max_length=256, mask_ratio_range=(0.15, 0.85), alpaca_format=True):\n        self.texts = texts\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.mask_ratio_range = mask_ratio_range\n        self.mask_id = TrainingConfig.MASK_ID\n        self.alpaca_format = alpaca_format\n        self.instruction_separator = TrainingConfig.INSTRUCTION_RESPONSE_SEPARATOR\n        \n        print(f\"Alpacaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆæœŸåŒ–å®Œäº†: {len(texts)} ã‚µãƒ³ãƒ—ãƒ«\")\n        if alpaca_format:\n            print(\"æŒ‡ç¤ºå¿œç­”å½¢å¼ã§ã®æ‹¡æ•£å­¦ç¿’ã‚’ä½¿ç”¨\")\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def parse_alpaca_text(self, text):\n        \"\"\"\n        Alpacaå½¢å¼ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ‡ç¤ºéƒ¨åˆ†ã¨å¿œç­”éƒ¨åˆ†ã«åˆ†å‰²\n        \"\"\"\n        if self.instruction_separator in text:\n            parts = text.split(self.instruction_separator, 1)\n            if len(parts) == 2:\n                instruction_part = parts[0].strip()\n                response_part = parts[1].strip()\n                return instruction_part, response_part\n        \n        # åˆ†å‰²ã§ããªã„å ´åˆã¯å…¨ä½“ã‚’ä¸€ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦æ‰±ã†\n        return \"\", text.strip()\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        \n        if self.alpaca_format:\n            instruction_text, response_text = self.parse_alpaca_text(text)\n            \n            # æŒ‡ç¤ºéƒ¨åˆ†ã¨å¿œç­”éƒ¨åˆ†ã‚’çµåˆã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n            if instruction_text:\n                full_text = f\"{instruction_text}{self.instruction_separator}{response_text}\"\n            else:\n                full_text = response_text\n        else:\n            full_text = text\n        \n        # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n        encoding = self.tokenizer(\n            full_text,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        input_ids = encoding['input_ids'].squeeze(0)\n        attention_mask = encoding['attention_mask'].squeeze(0)\n        \n        # æœ‰åŠ¹ãªãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ã‚’ç‰¹å®š\n        valid_positions = (attention_mask == 1).nonzero(as_tuple=True)[0]\n        \n        if len(valid_positions) == 0:\n            # æœ‰åŠ¹ãªãƒˆãƒ¼ã‚¯ãƒ³ãŒãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\n            return {\n                'input_ids': input_ids,\n                'attention_mask': attention_mask,\n                'labels': input_ids.clone(),\n                'mask_positions': torch.zeros_like(input_ids, dtype=torch.bool)\n            }\n        \n        # Alpacaå½¢å¼ã®å ´åˆã€å¿œç­”éƒ¨åˆ†ã«ã‚ˆã‚Šå¤šããƒã‚¹ã‚¯ã‚’é©ç”¨\n        if self.alpaca_format and instruction_text:\n            # æŒ‡ç¤ºéƒ¨åˆ†ã®çµ‚äº†ä½ç½®ã‚’ç‰¹å®š\n            instruction_encoding = self.tokenizer(\n                instruction_text,\n                add_special_tokens=False,\n                return_tensors='pt'\n            )\n            instruction_length = instruction_encoding['input_ids'].shape[1]\n            \n            # å¿œç­”éƒ¨åˆ†ã®ä½ç½®ã‚’ç‰¹å®šï¼ˆæŒ‡ç¤ºéƒ¨åˆ† + ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ã®å¾Œï¼‰\n            separator_encoding = self.tokenizer(\n                self.instruction_separator,\n                add_special_tokens=False,\n                return_tensors='pt'\n            )\n            separator_length = separator_encoding['input_ids'].shape[1]\n            \n            response_start = min(instruction_length + separator_length, len(valid_positions) - 1)\n            \n            # å¿œç­”éƒ¨åˆ†ã«ã‚ˆã‚Šé‡ç‚¹çš„ã«ãƒã‚¹ã‚­ãƒ³ã‚°\n            response_positions = valid_positions[response_start:]\n            instruction_positions = valid_positions[:response_start]\n            \n            # å¿œç­”éƒ¨åˆ†ã‹ã‚‰å¤šãã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒã‚¹ã‚¯\n            response_mask_ratio = random.uniform(0.5, 0.8)  # å¿œç­”éƒ¨åˆ†ã¯å¤šã‚ã«ãƒã‚¹ã‚¯\n            instruction_mask_ratio = random.uniform(0.1, 0.3)  # æŒ‡ç¤ºéƒ¨åˆ†ã¯å°‘ãªã‚ã«ãƒã‚¹ã‚¯\n            \n            num_response_mask = max(1, int(len(response_positions) * response_mask_ratio))\n            num_instruction_mask = max(0, int(len(instruction_positions) * instruction_mask_ratio))\n            \n            # ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ\n            response_mask_indices = torch.randperm(len(response_positions))[:num_response_mask]\n            instruction_mask_indices = torch.randperm(len(instruction_positions))[:num_instruction_mask]\n            \n            # ãƒã‚¹ã‚¯ä½ç½®ã‚’çµåˆ\n            mask_positions = torch.cat([\n                instruction_positions[instruction_mask_indices] if len(instruction_mask_indices) > 0 else torch.tensor([], dtype=torch.long),\n                response_positions[response_mask_indices]\n            ])\n        else:\n            # é€šå¸¸ã®å‡ç­‰ãƒã‚¹ã‚­ãƒ³ã‚°\n            mask_ratio = random.uniform(*self.mask_ratio_range)\n            num_mask = max(1, int(len(valid_positions) * mask_ratio))\n            mask_indices = torch.randperm(len(valid_positions))[:num_mask]\n            mask_positions = valid_positions[mask_indices]\n        \n        # ãƒã‚¹ã‚¯ã•ã‚ŒãŸå…¥åŠ›ã‚’ä½œæˆ\n        masked_input_ids = input_ids.clone()\n        masked_input_ids[mask_positions] = self.mask_id\n        \n        # ãƒã‚¹ã‚¯ä½ç½®ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n        mask_bool = torch.zeros_like(input_ids, dtype=torch.bool)\n        mask_bool[mask_positions] = True\n        \n        return {\n            'input_ids': masked_input_ids,\n            'attention_mask': attention_mask,\n            'labels': input_ids,  # å…ƒã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒãƒ©ãƒ™ãƒ«\n            'mask_positions': mask_bool\n        }\n\ndef compute_alpaca_diffusion_loss(model, batch, device):\n    \"\"\"\n    Alpacaå½¢å¼ç”¨ã®æ‹¡æ•£æå¤±ã‚’è¨ˆç®—\n    \"\"\"\n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n    labels = batch['labels'].to(device)\n    mask_positions = batch['mask_positions'].to(device)\n    \n    # ãƒ¢ãƒ‡ãƒ«ã®å‰å‘ãä¼æ’­\n    outputs = model(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n    )\n    \n    logits = outputs.logits\n    \n    # ãƒã‚¹ã‚¯ä½ç½®ã®ã¿ã§æå¤±ã‚’è¨ˆç®—\n    masked_logits = logits[mask_positions]  # [num_masked_tokens, vocab_size]\n    masked_labels = labels[mask_positions]  # [num_masked_tokens]\n    \n    if len(masked_labels) == 0:\n        # ãƒã‚¹ã‚¯ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ãŒãªã„å ´åˆ\n        return torch.tensor(0.0, device=device, requires_grad=True)\n    \n    # ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±\n    loss = F.cross_entropy(masked_logits, masked_labels)\n    \n    return loss\n\n# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ\nprint(\"=== æ—¥æœ¬èªAlpacaå­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ ===\")\n\n# æ—¥æœ¬èªAlpacaãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\ntrain_texts = load_japanese_alpaca_dataset(\n    TrainingConfig.DATASET_NAME,\n    max_samples=TrainingConfig.MAX_SAMPLES,\n    min_length=TrainingConfig.MIN_TEXT_LENGTH\n)\n\nprint(f\"\\nèª­ã¿è¾¼ã¿å®Œäº†: {len(train_texts)} ã‚µãƒ³ãƒ—ãƒ«\")\n\n# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¨“ç·´/æ¤œè¨¼ã«åˆ†å‰²\nsplit_idx = int(len(train_texts) * 0.9)\ntrain_dataset = LLaDAAlpacaDiffusionDataset(\n    train_texts[:split_idx],\n    tokenizer,\n    max_length=TrainingConfig.MAX_LENGTH,\n    mask_ratio_range=TrainingConfig.MASK_RATIO_RANGE,\n    alpaca_format=TrainingConfig.USE_ALPACA_FORMAT\n)\n\nval_dataset = LLaDAAlpacaDiffusionDataset(\n    train_texts[split_idx:],\n    tokenizer,\n    max_length=TrainingConfig.MAX_LENGTH,\n    mask_ratio_range=TrainingConfig.MASK_RATIO_RANGE,\n    alpaca_format=TrainingConfig.USE_ALPACA_FORMAT\n)\n\nprint(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_dataset)} ã‚µãƒ³ãƒ—ãƒ«\")\nprint(f\"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(val_dataset)} ã‚µãƒ³ãƒ—ãƒ«\")\n\n# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆ\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=TrainingConfig.BATCH_SIZE,\n    shuffle=True,\n    num_workers=0\n)\n\nval_dataloader = DataLoader(\n    val_dataset,\n    batch_size=TrainingConfig.BATCH_SIZE,\n    shuffle=False,\n    num_workers=0\n)\n\nprint(f\"ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆå®Œäº†\")\nprint(f\"è¨“ç·´ãƒãƒƒãƒæ•°: {len(train_dataloader)}\")\nprint(f\"æ¤œè¨¼ãƒãƒƒãƒæ•°: {len(val_dataloader)}\")\n\n# ã‚µãƒ³ãƒ—ãƒ«ãƒãƒƒãƒã®ç¢ºèª\nsample_batch = next(iter(train_dataloader))\nprint(f\"\\n=== ã‚µãƒ³ãƒ—ãƒ«ãƒãƒƒãƒåˆ†æ ===\")\nfor key, value in sample_batch.items():\n    print(f\"  {key}: {value.shape}\")\n\n# ãƒã‚¹ã‚¯ç‡ã®ç¢ºèª\nmask_ratio = sample_batch['mask_positions'].float().mean().item()\nprint(f\"\\nã‚µãƒ³ãƒ—ãƒ«ãƒãƒƒãƒã®ãƒã‚¹ã‚¯ç‡: {mask_ratio:.3f}\")\n\n# ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã®è¡¨ç¤º\nsample_text = train_texts[0]\nprint(f\"\\n=== ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆä¾‹ ===\")\nprint(f\"{sample_text[:300]}{'...' if len(sample_text) > 300 else ''}\")\n\n# Alpacaå½¢å¼ã®åˆ†æ\nif TrainingConfig.USE_ALPACA_FORMAT:\n    dataset_instance = train_dataset\n    instruction, response = dataset_instance.parse_alpaca_text(sample_text)\n    print(f\"\\n=== Alpacaå½¢å¼åˆ†æ ===\")\n    print(f\"æŒ‡ç¤ºéƒ¨åˆ†: {instruction[:100]}{'...' if len(instruction) > 100 else ''}\")\n    print(f\"å¿œç­”éƒ¨åˆ†: {response[:100]}{'...' if len(response) > 100 else ''}\")\n\n# æå¤±è¨ˆç®—é–¢æ•°ã‚’æ›´æ–°\ncompute_diffusion_loss = compute_alpaca_diffusion_loss"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å­¦ç¿’ã®å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# LoRAå¯¾å¿œã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®è¨­å®š\nprint(\"=== LoRAå¯¾å¿œã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’è¨­å®šä¸­ ===\")\n\n# å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ã‚’å–å¾—\ntrainable_params = [p for p in model.parameters() if p.requires_grad]\ntrainable_param_count = sum(p.numel() for p in trainable_params)\nprint(f\"å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {trainable_param_count / 1e6:.1f}M\")\n\n# 8bit AdamWã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡å‘ä¸Šï¼‰\nif PEFT_AVAILABLE:\n    try:\n        import bitsandbytes as bnb\n        optimizer = bnb.optim.AdamW8bit(\n            trainable_params,\n            lr=TrainingConfig.LEARNING_RATE,\n            weight_decay=TrainingConfig.WEIGHT_DECAY,\n            betas=(0.9, 0.999),\n            eps=1e-8\n        )\n        print(\"âœ… 8bit AdamWã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡å‘ä¸Šï¼‰\")\n    except ImportError:\n        optimizer = AdamW(\n            trainable_params,\n            lr=TrainingConfig.LEARNING_RATE,\n            weight_decay=TrainingConfig.WEIGHT_DECAY,\n            betas=(0.9, 0.999),\n            eps=1e-8\n        )\n        print(\"âš ï¸ æ¨™æº–AdamWã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨\")\nelse:\n    optimizer = AdamW(\n        trainable_params,\n        lr=TrainingConfig.LEARNING_RATE,\n        weight_decay=TrainingConfig.WEIGHT_DECAY,\n        betas=(0.9, 0.999),\n        eps=1e-8\n    )\n    print(\"æ¨™æº–AdamWã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨\")\n\n# å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•°ã®è¨ˆç®—\ntotal_steps = len(train_dataloader) * TrainingConfig.NUM_EPOCHS\nwarmup_steps = int(total_steps * TrainingConfig.WARMUP_RATIO)\n\n# ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=warmup_steps,\n    num_training_steps=total_steps\n)\n\n# æ··åˆç²¾åº¦å­¦ç¿’ç”¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼\nscaler = None\nif AMP_AVAILABLE and TrainingConfig.USE_MIXED_PRECISION:\n    scaler = GradScaler()\n    print(\"âœ… æ··åˆç²¾åº¦å­¦ç¿’ç”¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼æº–å‚™å®Œäº†\")\n\nprint(f\"\\nå­¦ç¿’è¨­å®š:\")\nprint(f\"  ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°: {total_steps}\")\nprint(f\"  ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—: {warmup_steps}\")\nprint(f\"  åˆæœŸå­¦ç¿’ç‡: {TrainingConfig.LEARNING_RATE}\")\nprint(f\"  æ··åˆç²¾åº¦å­¦ç¿’: {'æœ‰åŠ¹' if scaler else 'ç„¡åŠ¹'}\")\n\n# å­¦ç¿’å±¥æ­´è¨˜éŒ²ç”¨\ntraining_history = {\n    'train_losses': [],\n    'val_losses': [],\n    'learning_rates': [],\n    'steps': []\n}\n\ndef evaluate_model_lora(model, dataloader, device, use_amp=False):\n    \"\"\"\n    LoRAå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡\n    \"\"\"\n    model.eval()\n    total_loss = 0\n    num_batches = 0\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            if use_amp and scaler:\n                with autocast():\n                    loss = compute_diffusion_loss(model, batch, device)\n            else:\n                loss = compute_diffusion_loss(model, batch, device)\n            \n            total_loss += loss.item()\n            num_batches += 1\n    \n    model.train()\n    return total_loss / max(num_batches, 1)\n\nprint(\"\\n=== LoRAå­¦ç¿’æº–å‚™å®Œäº† ===\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# LoRA + æ··åˆç²¾åº¦å¯¾å¿œå­¦ç¿’ãƒ«ãƒ¼ãƒ—ã®å®Ÿè¡Œ\nprint(\"=== LoRAå¯¾å¿œ LLaDAæ—¥æœ¬èªAlpacaå­¦ç¿’é–‹å§‹ ===\")\nprint(f\"ã‚¨ãƒãƒƒã‚¯æ•°: {TrainingConfig.NUM_EPOCHS}\")\nprint(f\"ãƒãƒƒãƒã‚µã‚¤ã‚º: {TrainingConfig.BATCH_SIZE}\")\nprint(f\"å‹¾é…è“„ç©ã‚¹ãƒ†ãƒƒãƒ—: {TrainingConfig.GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º: {TrainingConfig.BATCH_SIZE * TrainingConfig.GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"å­¦ç¿’ç‡: {TrainingConfig.LEARNING_RATE}\")\nprint(f\"LoRAä½¿ç”¨: {'ã¯ã„' if PEFT_AVAILABLE else 'ã„ã„ãˆ'}\")\nprint(f\"æ··åˆç²¾åº¦: {'ã¯ã„' if scaler else 'ã„ã„ãˆ'}\")\nprint(\"=\" * 60)\n\n# ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    print(f\"ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢å¾Œã®GPUãƒ¡ãƒ¢ãƒª: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")\n\n# å­¦ç¿’é–‹å§‹æ™‚åˆ»\nstart_time = time.time()\nglobal_step = 0\nbest_val_loss = float('inf')\n\n# åˆæœŸè©•ä¾¡\nprint(\"åˆæœŸè©•ä¾¡ä¸­...\")\nuse_amp = scaler is not None\ninitial_val_loss = evaluate_model_lora(model, val_dataloader, device, use_amp)\nprint(f\"åˆæœŸæ¤œè¨¼æå¤±: {initial_val_loss:.4f}\")\n\ntry:\n    for epoch in range(TrainingConfig.NUM_EPOCHS):\n        print(f\"\\nã‚¨ãƒãƒƒã‚¯ {epoch + 1}/{TrainingConfig.NUM_EPOCHS}\")\n        print(\"-\" * 40)\n        \n        epoch_start_time = time.time()\n        epoch_losses = []\n        \n        # è¨“ç·´ãƒ«ãƒ¼ãƒ—\n        model.train()\n        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n        \n        optimizer.zero_grad()  # åˆæœŸåŒ–\n        \n        for batch_idx, batch in enumerate(progress_bar):\n            \n            # æ··åˆç²¾åº¦å­¦ç¿’ã§ã®æå¤±è¨ˆç®—\n            if use_amp:\n                with autocast():\n                    loss = compute_diffusion_loss(model, batch, device)\n                    loss = loss / TrainingConfig.GRADIENT_ACCUMULATION_STEPS\n                \n                # ã‚¹ã‚±ãƒ¼ãƒ«ã•ã‚ŒãŸé€†ä¼æ’­\n                scaler.scale(loss).backward()\n            else:\n                # é€šå¸¸ã®æå¤±è¨ˆç®—\n                loss = compute_diffusion_loss(model, batch, device)\n                loss = loss / TrainingConfig.GRADIENT_ACCUMULATION_STEPS\n                loss.backward()\n            \n            # è¨˜éŒ²\n            current_loss = loss.item() * TrainingConfig.GRADIENT_ACCUMULATION_STEPS\n            epoch_losses.append(current_loss)\n            \n            # å‹¾é…è“„ç©ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°\n            if (batch_idx + 1) % TrainingConfig.GRADIENT_ACCUMULATION_STEPS == 0:\n                if use_amp:\n                    # æ··åˆç²¾åº¦ã§ã®æ›´æ–°\n                    scaler.unscale_(optimizer)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                    scaler.step(optimizer)\n                    scaler.update()\n                else:\n                    # é€šå¸¸ã®æ›´æ–°\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                    optimizer.step()\n                \n                scheduler.step()\n                optimizer.zero_grad()\n                global_step += 1\n            \n            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼æ›´æ–°\n            progress_bar.set_postfix({\n                'loss': f'{current_loss:.4f}',\n                'lr': f'{scheduler.get_last_lr()[0]:.2e}',\n                'gpu_mem': f'{torch.cuda.memory_allocated() / 1e9:.1f}GB' if torch.cuda.is_available() else 'N/A',\n                'step': global_step\n            })\n            \n            # å®šæœŸçš„ãªè©•ä¾¡ã¨ä¿å­˜\n            if global_step % TrainingConfig.EVAL_STEPS == 0 and global_step > 0:\n                # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n                \n                val_loss = evaluate_model_lora(model, val_dataloader, device, use_amp)\n                \n                # å±¥æ­´ã«è¨˜éŒ²\n                recent_losses = epoch_losses[-TrainingConfig.EVAL_STEPS:] if len(epoch_losses) >= TrainingConfig.EVAL_STEPS else epoch_losses\n                training_history['train_losses'].append(np.mean(recent_losses))\n                training_history['val_losses'].append(val_loss)\n                training_history['learning_rates'].append(scheduler.get_last_lr()[0])\n                training_history['steps'].append(global_step)\n                \n                print(f\"\\nStep {global_step}: Train Loss = {current_loss:.4f}, Val Loss = {val_loss:.4f}\")\n                \n                # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n                if val_loss < best_val_loss:\n                    best_val_loss = val_loss\n                    print(f\"ğŸ¯ æ–°ã—ã„ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ï¼æ¤œè¨¼æå¤±: {val_loss:.4f}\")\n                    \n                    # LoRAãƒ¢ãƒ‡ãƒ«ä¿å­˜\n                    if not os.path.exists(TrainingConfig.OUTPUT_DIR):\n                        os.makedirs(TrainingConfig.OUTPUT_DIR)\n                    \n                    if PEFT_AVAILABLE:\n                        # LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ã¿ä¿å­˜\n                        model.save_pretrained(TrainingConfig.OUTPUT_DIR)\n                        print(\"âœ… LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ä¿å­˜å®Œäº†\")\n                    \n                    # å­¦ç¿’æƒ…å ±ã®ä¿å­˜\n                    config_dict = {\n                        'DATASET_NAME': TrainingConfig.DATASET_NAME,\n                        'MAX_SAMPLES': TrainingConfig.MAX_SAMPLES,\n                        'BATCH_SIZE': TrainingConfig.BATCH_SIZE,\n                        'LEARNING_RATE': TrainingConfig.LEARNING_RATE,\n                        'NUM_EPOCHS': TrainingConfig.NUM_EPOCHS,\n                        'MAX_LENGTH': TrainingConfig.MAX_LENGTH,\n                        'USE_LORA': PEFT_AVAILABLE,\n                        'USE_MIXED_PRECISION': use_amp\n                    }\n                    \n                    torch.save({\n                        'global_step': global_step,\n                        'best_val_loss': best_val_loss,\n                        'training_history': training_history,\n                        'config': config_dict\n                    }, os.path.join(TrainingConfig.OUTPUT_DIR, 'training_info.pt'))\n        \n        # ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ã®çµ±è¨ˆ\n        epoch_time = time.time() - epoch_start_time\n        avg_epoch_loss = np.mean(epoch_losses)\n        \n        print(f\"\\nã‚¨ãƒãƒƒã‚¯ {epoch + 1} å®Œäº†:\")\n        print(f\"  å¹³å‡è¨“ç·´æå¤±: {avg_epoch_loss:.4f}\")\n        print(f\"  æ™‚é–“: {epoch_time:.1f}ç§’\")\n        \n        # ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ã®è©•ä¾¡\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        val_loss = evaluate_model_lora(model, val_dataloader, device, use_amp)\n        print(f\"  æ¤œè¨¼æå¤±: {val_loss:.4f}\")\n\nexcept KeyboardInterrupt:\n    print(\"\\nâ¹ï¸ å­¦ç¿’ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ\")\nexcept Exception as e:\n    print(f\"\\nâŒ å­¦ç¿’ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n    import traceback\n    traceback.print_exc()\n\nfinally:\n    # å­¦ç¿’çµ‚äº†å‡¦ç†\n    total_time = time.time() - start_time\n    print(f\"\\n=== å­¦ç¿’å®Œäº† ===\")\n    print(f\"ç·å­¦ç¿’æ™‚é–“: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†)\")\n    print(f\"ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°: {global_step}\")\n    print(f\"æœ€é«˜æ¤œè¨¼ã‚¹ã‚³ã‚¢: {best_val_loss:.4f}\")\n    \n    # æœ€çµ‚LoRAãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n    if not os.path.exists(TrainingConfig.OUTPUT_DIR):\n        os.makedirs(TrainingConfig.OUTPUT_DIR)\n    \n    if PEFT_AVAILABLE:\n        model.save_pretrained(TrainingConfig.OUTPUT_DIR)\n        print(f\"âœ… æœ€çµ‚LoRAãƒ¢ãƒ‡ãƒ«ä¿å­˜: {TrainingConfig.OUTPUT_DIR}\")\n    \n    # å­¦ç¿’å±¥æ­´ã®ä¿å­˜\n    config_dict = {\n        'DATASET_NAME': TrainingConfig.DATASET_NAME,\n        'MAX_SAMPLES': TrainingConfig.MAX_SAMPLES,\n        'BATCH_SIZE': TrainingConfig.BATCH_SIZE,\n        'LEARNING_RATE': TrainingConfig.LEARNING_RATE,\n        'NUM_EPOCHS': TrainingConfig.NUM_EPOCHS,\n        'MAX_LENGTH': TrainingConfig.MAX_LENGTH,\n        'USE_LORA': PEFT_AVAILABLE,\n        'USE_MIXED_PRECISION': use_amp\n    }\n    \n    torch.save({\n        'training_history': training_history,\n        'final_step': global_step,\n        'config': config_dict\n    }, os.path.join(TrainingConfig.OUTPUT_DIR, 'final_training_info.pt'))\n    \n    print(f\"ğŸ“Š å­¦ç¿’å±¥æ­´ä¿å­˜å®Œäº†\")\n    \n    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æœ€çµ‚ç¢ºèª\n    if torch.cuda.is_available():\n        final_memory = torch.cuda.memory_allocated() / 1e9\n        max_memory = torch.cuda.max_memory_allocated() / 1e9\n        print(f\"\\nğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚µãƒãƒªãƒ¼:\")\n        print(f\"  æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {final_memory:.1f}GB\")\n        print(f\"  æœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {max_memory:.1f}GB\")\n        print(f\"  LoRAã«ã‚ˆã‚‹ãƒ¡ãƒ¢ãƒªå‰Šæ¸›åŠ¹æœãŒç¢ºèªã§ãã¾ã—ãŸ âœ…\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’å±¥æ­´ã®å¯è¦–åŒ–\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    å­¦ç¿’å±¥æ­´ã‚’ã‚°ãƒ©ãƒ•åŒ–\n",
    "    \"\"\"\n",
    "    if not history['steps']:\n",
    "        print(\"å­¦ç¿’å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“\")\n",
    "        return\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # æå¤±ã®æ¨ç§»\n",
    "    ax1.plot(history['steps'], history['train_losses'], 'b-', label='Train Loss', alpha=0.7)\n",
    "    ax1.plot(history['steps'], history['val_losses'], 'r-', label='Validation Loss', alpha=0.7)\n",
    "    ax1.set_xlabel('Steps')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # å­¦ç¿’ç‡ã®æ¨ç§»\n",
    "    ax2.plot(history['steps'], history['learning_rates'], 'g-', alpha=0.7)\n",
    "    ax2.set_xlabel('Steps')\n",
    "    ax2.set_ylabel('Learning Rate')\n",
    "    ax2.set_title('Learning Rate Schedule')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # çµ±è¨ˆæƒ…å ±\n",
    "    if history['val_losses']:\n",
    "        final_train_loss = history['train_losses'][-1]\n",
    "        final_val_loss = history['val_losses'][-1]\n",
    "        best_val_loss = min(history['val_losses'])\n",
    "        \n",
    "        print(f\"\\nå­¦ç¿’çµæœã‚µãƒãƒªãƒ¼:\")\n",
    "        print(f\"  æœ€çµ‚è¨“ç·´æå¤±: {final_train_loss:.4f}\")\n",
    "        print(f\"  æœ€çµ‚æ¤œè¨¼æå¤±: {final_val_loss:.4f}\")\n",
    "        print(f\"  æœ€è‰¯æ¤œè¨¼æå¤±: {best_val_loss:.4f}\")\n",
    "        print(f\"  æ”¹å–„é‡: {history['val_losses'][0] - best_val_loss:.4f}\")\n",
    "\n",
    "# å­¦ç¿’å±¥æ­´ã®è¡¨ç¤º\n",
    "print(\"=== å­¦ç¿’å±¥æ­´ã®å¯è¦–åŒ– ===\")\n",
    "plot_training_history(training_history)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# LoRAå­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ã®æ—¥æœ¬èªAlpacaæŒ‡ç¤ºå¿œç­”ãƒ†ã‚¹ãƒˆ\ndef test_japanese_alpaca_generation_lora(model, tokenizer, test_cases, device, steps=16, gen_length=128):\n    \"\"\"\n    LoRAå­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§æ—¥æœ¬èªæŒ‡ç¤ºå¿œç­”ã‚’ãƒ†ã‚¹ãƒˆ\n    \"\"\"\n    model.eval()\n    \n    # LoRAå¯¾å¿œã®ç°¡æ˜“ç”Ÿæˆé–¢æ•°\n    @torch.no_grad()\n    def simple_alpaca_generate_lora(instruction, input_text=\"\"):\n        # Alpacaå½¢å¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ\n        if input_text.strip():\n            prompt = f\"{instruction}\\n\\nå…¥åŠ›: {input_text}\"\n        else:\n            prompt = instruction\n        \n        # ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é©ç”¨\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        formatted_prompt = tokenizer.apply_chat_template(\n            messages, add_generation_prompt=True, tokenize=False\n        )\n        input_ids = tokenizer(formatted_prompt)['input_ids']\n        input_ids = torch.tensor(input_ids).to(device).unsqueeze(0)\n        \n        prompt_length = input_ids.shape[1]\n        \n        # ç°¡æ˜“çš„ãªç”Ÿæˆï¼ˆ1ã‚¹ãƒ†ãƒƒãƒ—ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°ï¼‰\n        x = torch.full(\n            (1, prompt_length + gen_length), \n            TrainingConfig.MASK_ID, \n            dtype=torch.long\n        ).to(device)\n        x[:, :prompt_length] = input_ids.clone()\n        \n        # LoRAãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬\n        if PEFT_AVAILABLE and hasattr(model, 'forward'):\n            outputs = model(x)\n        else:\n            outputs = model(x)\n        \n        logits = outputs.logits\n        \n        # ãƒã‚¹ã‚¯ä½ç½®ã®äºˆæ¸¬\n        mask_positions = (x == TrainingConfig.MASK_ID)\n        predictions = torch.argmax(logits, dim=-1)\n        \n        # ãƒã‚¹ã‚¯ã‚’äºˆæ¸¬ã§ç½®æ›\n        x[mask_positions] = predictions[mask_positions]\n        \n        # çµæœã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰\n        result = tokenizer.decode(\n            x[0, prompt_length:], skip_special_tokens=True\n        )\n        \n        return result\n    \n    print(\"=== LoRAå­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ã®æ—¥æœ¬èªAlpacaæŒ‡ç¤ºå¿œç­”ãƒ†ã‚¹ãƒˆ ===\")\n    \n    results = []\n    for i, test_case in enumerate(test_cases):\n        instruction = test_case.get('instruction', '')\n        input_text = test_case.get('input', '')\n        expected_output = test_case.get('expected_output', '')\n        \n        print(f\"\\n{i+1}. ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹\")\n        print(\"-\" * 50)\n        print(f\"æŒ‡ç¤º: {instruction}\")\n        if input_text:\n            print(f\"å…¥åŠ›: {input_text}\")\n        if expected_output:\n            print(f\"æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›: {expected_output}\")\n        \n        try:\n            result = simple_alpaca_generate_lora(instruction, input_text)\n            print(f\"ç”Ÿæˆçµæœ: {result}\")\n            \n            # ç°¡æ˜“çš„ãªè©•ä¾¡\n            if expected_output:\n                # é•·ã•ã®æ¯”è¼ƒ\n                length_similarity = min(len(result), len(expected_output)) / max(len(result), len(expected_output)) if max(len(result), len(expected_output)) > 0 else 0\n                print(f\"é•·ã•é¡ä¼¼åº¦: {length_similarity:.2f}\")\n                \n                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«æœ‰åº¦ãƒã‚§ãƒƒã‚¯\n                keywords = expected_output.split()[:3]  # æœ€åˆã®3ã¤ã®å˜èª\n                keyword_matches = sum(1 for kw in keywords if kw in result)\n                keyword_score = keyword_matches / len(keywords) if keywords else 0\n                print(f\"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´åº¦: {keyword_score:.2f}\")\n            \n            results.append({\n                'instruction': instruction,\n                'input': input_text,\n                'expected': expected_output,\n                'generated': result\n            })\n        except Exception as e:\n            print(f\"ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}\")\n            results.append({\n                'instruction': instruction,\n                'input': input_text,\n                'expected': expected_output,\n                'generated': \"[ç”Ÿæˆå¤±æ•—]\"\n            })\n    \n    return results\n\n# æ—¥æœ¬èªAlpacaå½¢å¼ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ï¼ˆLoRAå‘ã‘ã«æœ€é©åŒ–ï¼‰\nalpaca_test_cases_lora = [\n    {\n        'instruction': 'æ—¥æœ¬ã®é¦–éƒ½ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚',\n        'input': '',\n        'expected_output': 'æ—¥æœ¬ã®é¦–éƒ½ã¯æ±äº¬ã§ã™ã€‚'\n    },\n    {\n        'instruction': 'ä»¥ä¸‹ã®æ–‡ç« ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚',\n        'input': 'æ¡œã¯æ—¥æœ¬ã®è±¡å¾´çš„ãªèŠ±ã¨ã—ã¦çŸ¥ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚æ˜¥ã«ãªã‚‹ã¨å…¨å›½å„åœ°ã§æ¡œãŒå’²ãã€å¤šãã®äººã€…ãŒãŠèŠ±è¦‹ã‚’æ¥½ã—ã¿ã¾ã™ã€‚',\n        'expected_output': 'æ¡œã¯æ—¥æœ¬ã®è±¡å¾´çš„ãªèŠ±ã§ã€æ˜¥ã«å…¨å›½ã§é–‹èŠ±ã—ãŠèŠ±è¦‹ãŒæ¥½ã—ã¾ã‚Œã¾ã™ã€‚'\n    },\n    {\n        'instruction': 'å¥åº·çš„ãªé£Ÿäº‹ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ã—ã¦ãã ã•ã„ã€‚',\n        'input': '',\n        'expected_output': 'ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸé£Ÿäº‹ã‚’å¿ƒãŒã‘ã€é‡èœã‚’å¤šãæ‘‚å–ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚'\n    },\n    {\n        'instruction': 'æ¬¡ã®è‹±èªã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚',\n        'input': 'Good morning',\n        'expected_output': 'ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™'\n    },\n    {\n        'instruction': 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°åˆå¿ƒè€…ã«ãŠã™ã™ã‚ã®è¨€èªã‚’æ•™ãˆã¦ãã ã•ã„ã€‚',\n        'input': '',\n        'expected_output': 'Pythonã€JavaScriptã€JavaãŒãŠã™ã™ã‚ã§ã™ã€‚'\n    }\n]\n\n# LoRAæŒ‡ç¤ºå¿œç­”ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ\nprint(f\"\\n=== LoRAå­¦ç¿’åŠ¹æœã®æ¤œè¨¼ ===\")\nif PEFT_AVAILABLE:\n    print(\"âœ… LoRAãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™\")\n    generation_results_lora = test_japanese_alpaca_generation_lora(\n        model, tokenizer, alpaca_test_cases_lora, device, gen_length=64\n    )\nelse:\n    print(\"âš ï¸ LoRAæœªä½¿ç”¨ã®ãŸã‚ã€æ¨™æº–ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™\")\n    generation_results_lora = test_japanese_alpaca_generation(\n        model, tokenizer, alpaca_test_cases_lora, device, gen_length=64\n    )\n\nprint(\"\\n=== LoRAæŒ‡ç¤ºå¿œç­”ãƒ†ã‚¹ãƒˆå®Œäº† ===\")\n\n# LoRAã®åŠ¹æœåˆ†æ\nif PEFT_AVAILABLE and training_history['val_losses']:\n    print(f\"\\n=== LoRAå­¦ç¿’åŠ¹æœã®åˆ†æ ===\")\n    \n    # å­¦ç¿’ã®æ”¹å–„åº¦\n    initial_loss = training_history['val_losses'][0] if training_history['val_losses'] else None\n    final_loss = training_history['val_losses'][-1] if training_history['val_losses'] else None\n    best_loss = min(training_history['val_losses']) if training_history['val_losses'] else None\n    \n    if initial_loss and final_loss and best_loss:\n        improvement = initial_loss - best_loss\n        improvement_pct = (improvement / initial_loss) * 100\n        \n        print(f\"ğŸ“Š å­¦ç¿’æˆæœ:\")\n        print(f\"  åˆæœŸæ¤œè¨¼æå¤±: {initial_loss:.4f}\")\n        print(f\"  æœ€çµ‚æ¤œè¨¼æå¤±: {final_loss:.4f}\")\n        print(f\"  æœ€è‰¯æ¤œè¨¼æå¤±: {best_loss:.4f}\")\n        print(f\"  æ”¹å–„é‡: {improvement:.4f} ({improvement_pct:.1f}%)\")\n        \n        if improvement > 0:\n            print(\"âœ… LoRAã«ã‚ˆã‚‹å­¦ç¿’åŠ¹æœãŒç¢ºèªã•ã‚Œã¾ã—ãŸ\")\n        else:\n            print(\"âš ï¸ æ›´ãªã‚‹å­¦ç¿’ãŒå¿…è¦ãªå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\")\n    \n    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡æ€§ã®ç¢ºèª\n    if hasattr(model, 'print_trainable_parameters'):\n        print(f\"\\nğŸ“ˆ LoRAåŠ¹ç‡æ€§:\")\n        model.print_trainable_parameters()\n    \n    print(f\"\\nğŸ’¡ ä»Šå¾Œã®æ”¹å–„æ¡ˆ:\")\n    print(f\"  1. ã‚ˆã‚Šå¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ5K-10K ã‚µãƒ³ãƒ—ãƒ«ï¼‰\")\n    print(f\"  2. ã‚ˆã‚Šé•·ã„å­¦ç¿’ï¼ˆ3-5 ã‚¨ãƒãƒƒã‚¯ï¼‰\")\n    print(f\"  3. LoRAãƒ©ãƒ³ã‚¯ã®èª¿æ•´ï¼ˆ8-32ï¼‰\")\n    print(f\"  4. å­¦ç¿’ç‡ã®æœ€é©åŒ–\")\n    print(f\"  5. ã‚ˆã‚Šè©³ç´°ãªè©•ä¾¡æŒ‡æ¨™ã®å°å…¥\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# å­¦ç¿’å‰å¾Œã®æ¯”è¼ƒï¼ˆå¯èƒ½ã§ã‚ã‚Œã°ï¼‰\ndef compare_before_after_training():\n    \"\"\"\n    å­¦ç¿’å‰å¾Œã®ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’æ¯”è¼ƒ\n    \"\"\"\n    print(\"=== å­¦ç¿’åŠ¹æœã®åˆ†æ ===\")\n    \n    # å­¦ç¿’æ›²ç·šã®åˆ†æ\n    if training_history['val_losses']:\n        initial_loss = training_history['val_losses'][0] if training_history['val_losses'] else None\n        final_loss = training_history['val_losses'][-1] if training_history['val_losses'] else None\n        best_loss = min(training_history['val_losses']) if training_history['val_losses'] else None\n        \n        if initial_loss and final_loss:\n            improvement = initial_loss - best_loss\n            improvement_pct = (improvement / initial_loss) * 100\n            \n            print(f\"\\næå¤±ã®æ”¹å–„:\")\n            print(f\"  åˆæœŸæ¤œè¨¼æå¤±: {initial_loss:.4f}\")\n            print(f\"  æœ€çµ‚æ¤œè¨¼æå¤±: {final_loss:.4f}\")\n            print(f\"  æœ€è‰¯æ¤œè¨¼æå¤±: {best_loss:.4f}\")\n            print(f\"  æ”¹å–„é‡: {improvement:.4f} ({improvement_pct:.1f}%)\")\n            \n            if improvement > 0:\n                print(\"âœ… ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ãŒå‘ä¸Šã—ã¾ã—ãŸ\")\n            else:\n                print(\"âš ï¸ ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½å‘ä¸ŠãŒè¦‹ã‚‰ã‚Œã¾ã›ã‚“\")\n    \n    # ç”Ÿæˆçµæœã®è©•ä¾¡ï¼ˆå®‰å…¨ãªå¤‰æ•°ãƒã‚§ãƒƒã‚¯ï¼‰\n    print(f\"\\nç”Ÿæˆçµæœã®è©•ä¾¡:\")\n    \n    # åˆ©ç”¨å¯èƒ½ãªç”Ÿæˆçµæœã‚’ç¢ºèª\n    results_available = False\n    results_to_use = None\n    \n    if 'generation_results_lora' in globals():\n        results_to_use = generation_results_lora\n        results_available = True\n        print(\"âœ… LoRAç”Ÿæˆçµæœã‚’ä½¿ç”¨\")\n    elif 'generation_results' in globals():\n        results_to_use = generation_results\n        results_available = True\n        print(\"âœ… æ¨™æº–ç”Ÿæˆçµæœã‚’ä½¿ç”¨\")\n    else:\n        print(\"âš ï¸ ç”ŸæˆçµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n        print(\"   ç”Ÿæˆãƒ†ã‚¹ãƒˆã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n    \n    if results_available and results_to_use:\n        try:\n            for result_dict in results_to_use:\n                if isinstance(result_dict, dict):\n                    instruction = result_dict.get('instruction', 'N/A')\n                    generated = result_dict.get('generated', 'N/A')\n                    print(f\"  {instruction[:30]}... â†’ {generated[:50]}{'...' if len(generated) > 50 else ''}\")\n                else:\n                    # å¤ã„å½¢å¼ã®å ´åˆ\n                    print(f\"  çµæœ: {str(result_dict)[:70]}...\")\n        except Exception as e:\n            print(f\"âš ï¸ ç”Ÿæˆçµæœã®è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}\")\n    \n    # LoRAç‰¹æœ‰ã®åˆ†æ\n    if PEFT_AVAILABLE:\n        print(f\"\\n=== LoRAå­¦ç¿’ã®åŠ¹æœ ===\")\n        \n        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡æ€§ã®ç¢ºèª\n        if hasattr(model, 'print_trainable_parameters'):\n            print(\"ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡æ€§:\")\n            try:\n                model.print_trainable_parameters()\n            except:\n                trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n                all_params = sum(p.numel() for p in model.parameters())\n                print(f\"  å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {trainable_params / 1e6:.1f}M\")\n                print(f\"  å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {all_params / 1e9:.1f}B\") \n                print(f\"  åŠ¹ç‡æ€§: {trainable_params / all_params * 100:.2f}%\")\n        \n        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§\n        if torch.cuda.is_available():\n            current_memory = torch.cuda.memory_allocated() / 1e9\n            max_memory = torch.cuda.max_memory_allocated() / 1e9\n            print(f\"\\nğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨åŠ¹ç‡:\")\n            print(f\"  ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {current_memory:.1f}GB\")\n            print(f\"  æœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {max_memory:.1f}GB\")\n            print(f\"  LoRAã«ã‚ˆã‚‹å¤§å¹…ãªãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã‚’å®Ÿç¾ âœ…\")\n    \n    # æ¨å¥¨äº‹é …\n    print(f\"\\n=== ä»Šå¾Œã®æ”¹å–„ææ¡ˆ ===\")\n    print(f\"  1. ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã§ã®å­¦ç¿’ï¼ˆç¾åœ¨: {TrainingConfig.MAX_SAMPLES}ã‚µãƒ³ãƒ—ãƒ«ï¼‰\")\n    print(f\"  2. ã‚ˆã‚Šé•·ã„ã‚¨ãƒãƒƒã‚¯æ•°ã§ã®å­¦ç¿’ï¼ˆç¾åœ¨: {TrainingConfig.NUM_EPOCHS}ã‚¨ãƒãƒƒã‚¯ï¼‰\")\n    print(f\"  3. ç•°ãªã‚‹å­¦ç¿’ç‡ã§ã®å®Ÿé¨“ï¼ˆç¾åœ¨: {TrainingConfig.LEARNING_RATE}ï¼‰\")\n    print(f\"  4. ã‚ˆã‚Šå¤šæ§˜ãªæ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½¿ç”¨\")\n    print(f\"  5. å°‚é–€åˆ†é‡ï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹ã€å°èª¬ãªã©ï¼‰ã«ç‰¹åŒ–ã—ãŸãƒ‡ãƒ¼ã‚¿ã§ã®å­¦ç¿’\")\n    \n    if PEFT_AVAILABLE:\n        print(f\"\\n=== LoRAå›ºæœ‰ã®æ”¹å–„æ¡ˆ ===\")\n        print(f\"  1. LoRAãƒ©ãƒ³ã‚¯ã®èª¿æ•´ï¼ˆç¾åœ¨: 16 â†’ 8, 32, 64ã§å®Ÿé¨“ï¼‰\")\n        print(f\"  2. LoRAã‚¢ãƒ«ãƒ•ã‚¡ã®æœ€é©åŒ–ï¼ˆç¾åœ¨: 32ï¼‰\")\n        print(f\"  3. å¯¾è±¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æ‹¡å¼µï¼ˆç¾åœ¨: attention layers ã®ã¿ï¼‰\")\n        print(f\"  4. è¤‡æ•°LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®çµ„ã¿åˆã‚ã›\")\n        print(f\"  5. QLoRAï¼ˆ4bité‡å­åŒ–ï¼‰ã§ã®æ›´ãªã‚‹åŠ¹ç‡åŒ–\")\n\n# å­¦ç¿’åŠ¹æœåˆ†æã‚’å®Ÿè¡Œ\ncompare_before_after_training()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã¨æ´»ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# LoRAå¯¾å¿œ å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã¨æ´»ç”¨æ–¹æ³•\ndef save_and_document_lora_model():\n    \"\"\"\n    LoRAå­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã¨ä½¿ç”¨æ–¹æ³•ã®èª¬æ˜\n    \"\"\"\n    print(\"=== LoRAå­¦ç¿’æ¸ˆã¿LLaDAãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã¨æ´»ç”¨ ===\")\n    \n    # ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\n    if os.path.exists(TrainingConfig.OUTPUT_DIR):\n        files = os.listdir(TrainingConfig.OUTPUT_DIR)\n        print(f\"\\nä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ{TrainingConfig.OUTPUT_DIR}ï¼‰:\")\n        for file in files:\n            file_path = os.path.join(TrainingConfig.OUTPUT_DIR, file)\n            file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n            print(f\"  - {file} ({file_size:.1f} MB)\")\n        \n        total_size = sum(os.path.getsize(os.path.join(TrainingConfig.OUTPUT_DIR, f)) \n                        for f in files) / (1024 * 1024)\n        print(f\"\\nğŸ’¾ LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ç·ã‚µã‚¤ã‚º: {total_size:.1f} MB\")\n        print(\"âœ… ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼ˆæ•°GBï¼‰ã¨æ¯”è¼ƒã—ã¦å¤§å¹…ã«ã‚µã‚¤ã‚ºå‰Šæ¸›ã•ã‚Œã¾ã—ãŸ\")\n    \n    # LoRAãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æ–¹æ³•ã®ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰\n    print(f\"\\n=== LoRAãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æ–¹æ³• ===\")\n    print(\"```python\")\n    print(\"from transformers import AutoTokenizer, AutoModel\")\n    print(\"from peft import PeftModel\")\n    print(\"import torch\")\n    print()\n    print(\"# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\")\n    print(f\"base_model = AutoModel.from_pretrained('{TrainingConfig.MODEL_NAME}',\")\n    print(\"                                        trust_remote_code=True,\")\n    print(\"                                        torch_dtype=torch.bfloat16)\")\n    print(f\"tokenizer = AutoTokenizer.from_pretrained('{TrainingConfig.MODEL_NAME}',\")\n    print(\"                                           trust_remote_code=True)\")\n    print()\n    print(\"# LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®èª­ã¿è¾¼ã¿\")\n    print(f\"model = PeftModel.from_pretrained(base_model, '{TrainingConfig.OUTPUT_DIR}')\")\n    print(\"model.eval()\")\n    print(\"```\")\n    \n    # ä½¿ç”¨ä¾‹\n    print(f\"\\n=== ä½¿ç”¨ä¾‹ ===\")\n    print(\"```python\")\n    print(\"# æ—¥æœ¬èªæŒ‡ç¤ºå¿œç­”ã§ã®ç”Ÿæˆ\")\n    print(\"instruction = \\\"æ—¥æœ¬ã®ç¾ã—ã„å­£ç¯€ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„\\\"\")\n    print(\"input_text = \\\"\\\"\")\n    print(\"result = generate_with_lora_model(model, tokenizer, instruction, input_text)\")\n    print(\"print(result)\")\n    print(\"```\")\n    \n    # LoRAã®åˆ©ç‚¹\n    print(f\"\\n=== LoRAã®åˆ©ç‚¹ ===\")\n    print(\"ğŸš€ **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§**\")\n    print(\"   - å­¦ç¿’æ™‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: 90%ä»¥ä¸Šå‰Šæ¸›\")\n    print(\"   - æ¨è«–æ™‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ã»ã¼åŒç­‰\")\n    print(\"   - ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°: é«˜é€ŸåŒ–\")\n    print()\n    print(\"ğŸ’¾ **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸åŠ¹ç‡æ€§**\")\n    print(\"   - ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚µã‚¤ã‚º: æ•°åMBï¼ˆãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«: æ•°GBï¼‰\")\n    print(\"   - è¤‡æ•°ã‚¿ã‚¹ã‚¯å¯¾å¿œ: è¤‡æ•°LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®åˆ‡ã‚Šæ›¿ãˆå¯èƒ½\")\n    print(\"   - ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†: è»½é‡ã§ç®¡ç†ã—ã‚„ã™ã„\")\n    print()\n    print(\"ğŸ¯ **å­¦ç¿’åŠ¹ç‡æ€§**\")\n    print(\"   - å­¦ç¿’æ™‚é–“: å¤§å¹…çŸ­ç¸®\")\n    print(\"   - éå­¦ç¿’ãƒªã‚¹ã‚¯: ä½æ¸›\")\n    print(\"   - ã‚¿ã‚¹ã‚¯é©å¿œ: è¿…é€Ÿãªç‰¹åŒ–å­¦ç¿’\")\n    \n    # æ³¨æ„äº‹é …ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹\n    print(f\"\\n=== æ³¨æ„äº‹é …ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ ===\")\n    print(\"âš ï¸ **é‡è¦ãªæ³¨æ„ç‚¹**\")\n    print(\"1. ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¯å¿…ãšã‚»ãƒƒãƒˆã§ç®¡ç†\")\n    print(\"2. ç•°ãªã‚‹ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨ã®äº’æ›æ€§ã«æ³¨æ„\")\n    print(\"3. æœ¬æ ¼é‹ç”¨å‰ã«ã¯ååˆ†ãªè©•ä¾¡ã¨ãƒ†ã‚¹ãƒˆã‚’å®Ÿæ–½\")\n    print(\"4. ãƒ©ã‚¤ã‚»ãƒ³ã‚¹è¦ä»¶ï¼ˆLLaMA3ãƒ™ãƒ¼ã‚¹ï¼‰ã®ç¢ºèª\")\n    print()\n    print(\"ğŸ’¡ **ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹**\")\n    print(\"1. **æ®µéšçš„å­¦ç¿’**: å°â†’ä¸­â†’å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã®é †æ¬¡å­¦ç¿’\")\n    print(\"2. **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´**: rankã€alphaã€å­¦ç¿’ç‡ã®æœ€é©åŒ–\")\n    print(\"3. **è©•ä¾¡æŒ‡æ¨™**: å®šé‡çš„ãƒ»å®šæ€§çš„è©•ä¾¡ã®çµ„ã¿åˆã‚ã›\")\n    print(\"4. **ç¶™ç¶šå­¦ç¿’**: æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã§ã®è¿½åŠ å­¦ç¿’\")\n    \n    # ä»Šå¾Œã®æ‹¡å¼µæ¡ˆ\n    print(f\"\\n=== ä»Šå¾Œã®æ‹¡å¼µæ¡ˆ ===\")\n    print(\"ğŸ”„ **å­¦ç¿’ã®æ‹¡å¼µ**\")\n    print(\"1. **å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿**: 10K-50Kã‚µãƒ³ãƒ—ãƒ«ã§ã®æœ¬æ ¼å­¦ç¿’\")\n    print(\"2. **ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯**: è¤‡æ•°ã®æ—¥æœ¬èªã‚¿ã‚¹ã‚¯ã§ã®åŒæ™‚å­¦ç¿’\")\n    print(\"3. **ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–**: åŒ»ç™‚ã€æ³•å¾‹ã€æŠ€è¡“åˆ†é‡ã¸ã®ç‰¹åŒ–\")\n    print(\"4. **ç¶™ç¶šå­¦ç¿’**: æ–°ã—ã„çŸ¥è­˜ã®ç¶™ç¶šçš„ãªè¿½åŠ \")\n    print()\n    print(\"ğŸ› ï¸ **æŠ€è¡“çš„æ”¹å–„**\")\n    print(\"1. **QLoRA**: 4bité‡å­åŒ–ã§ã®æ›´ãªã‚‹ãƒ¡ãƒ¢ãƒªå‰Šæ¸›\")\n    print(\"2. **AdaLoRA**: é©å¿œçš„ãƒ©ãƒ³ã‚¯èª¿æ•´\")\n    print(\"3. **DoRA**: é‡ã¿åˆ†è§£LoRA\")\n    print(\"4. **åˆ†æ•£å­¦ç¿’**: è¤‡æ•°GPU/ãƒãƒ¼ãƒ‰ã§ã®é«˜é€ŸåŒ–\")\n    print()\n    print(\"ğŸ“Š **è©•ä¾¡ã®é«˜åº¦åŒ–**\")\n    print(\"1. **è‡ªå‹•è©•ä¾¡**: BLEUã€ROUGEã€BERTScoreãªã©\")\n    print(\"2. **äººé–“è©•ä¾¡**: æœ‰ç”¨æ€§ã€å®‰å…¨æ€§ã€å€«ç†æ€§\")\n    print(\"3. **A/Bãƒ†ã‚¹ãƒˆ**: å®Ÿç”¨ç’°å¢ƒã§ã®æ¯”è¼ƒè©•ä¾¡\")\n    print(\"4. **ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**: æ¨™æº–çš„æ—¥æœ¬èªNLPã‚¿ã‚¹ã‚¯ã§ã®è©•ä¾¡\")\n    \n    return True\n\n# å®Ÿè¡Œ\nsave_and_document_lora_model()\n\n# LoRAå­¦ç¿’å®Ÿé¨“ã®ã‚µãƒãƒªãƒ¼ä¿å­˜\nif PEFT_AVAILABLE:\n    summary = {\n        'experiment_type': 'LoRA Fine-tuning',\n        'base_model': TrainingConfig.MODEL_NAME,\n        'dataset': TrainingConfig.DATASET_NAME,\n        'samples': TrainingConfig.MAX_SAMPLES,\n        'epochs': TrainingConfig.NUM_EPOCHS,\n        'batch_size': TrainingConfig.BATCH_SIZE,\n        'learning_rate': TrainingConfig.LEARNING_RATE,\n        'max_length': TrainingConfig.MAX_LENGTH,\n        'use_lora': True,\n        'use_mixed_precision': TrainingConfig.USE_MIXED_PRECISION,\n        'lora_config': {\n            'rank': 16,\n            'alpha': 32,\n            'dropout': 0.1\n        },\n        'training_history': training_history,\n        'generation_examples': generation_results_lora if 'generation_results_lora' in locals() else []\n    }\nelse:\n    summary = {\n        'experiment_type': 'Standard Fine-tuning',\n        'note': 'LoRA was not available for this experiment'\n    }\n\n# ã‚µãƒãƒªãƒ¼ã‚’JSONã§ä¿å­˜\nif os.path.exists(TrainingConfig.OUTPUT_DIR):\n    with open(os.path.join(TrainingConfig.OUTPUT_DIR, 'lora_experiment_summary.json'), 'w', encoding='utf-8') as f:\n        json.dump(summary, f, ensure_ascii=False, indent=2)\n    print(f\"\\nğŸ“‹ LoRAå®Ÿé¨“ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {TrainingConfig.OUTPUT_DIR}/lora_experiment_summary.json\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"ğŸ‰ LoRAå¯¾å¿œ LLaDAæ—¥æœ¬èªAlpacaå­¦ç¿’å®Ÿé¨“å®Œäº†ï¼\")\nprint(\"=\" * 80)\n\nif PEFT_AVAILABLE:\n    print(\"\\nğŸ† **å®Ÿé¨“æˆæœ**\")\n    print(\"âœ… LoRAã«ã‚ˆã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡çš„å­¦ç¿’ã®å®Ÿè£…ã«æˆåŠŸ\")\n    print(\"âœ… ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å¤§å¹…å‰Šæ¸›ã‚’å®Ÿç¾\")\n    print(\"âœ… æ—¥æœ¬èªAlpacaæŒ‡ç¤ºå¿œç­”ãƒ‡ãƒ¼ã‚¿ã§ã®å­¦ç¿’å®Œäº†\")\n    print(\"âœ… å­¦ç¿’æ¸ˆã¿LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ä¿å­˜ãƒ»æ´»ç”¨æ–¹æ³•ã‚’ç¢ºç«‹\")\n    \n    print(\"\\nğŸš€ **æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**\")\n    print(\"1. ã‚ˆã‚Šå¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®æœ¬æ ¼å­¦ç¿’\")\n    print(\"2. è©³ç´°ãªè©•ä¾¡æŒ‡æ¨™ã«ã‚ˆã‚‹æ€§èƒ½æ¸¬å®š\")\n    print(\"3. å®Ÿç”¨ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã®å‹•ä½œç¢ºèª\")\n    print(\"4. ç¶™ç¶šçš„ãªæ”¹å–„ã¨ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\")\n    \n    print(\"\\nğŸ’¡ ã“ã®å®Ÿé¨“ã«ã‚ˆã‚Šã€é™ã‚‰ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ã§ã‚‚åŠ¹ç‡çš„ã«\")\n    print(\"   å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’æ—¥æœ¬èªã‚¿ã‚¹ã‚¯ã«é©å¿œã•ã›ã‚‹\")\n    print(\"   åŸºç›¤æŠ€è¡“ãŒç¢ºç«‹ã•ã‚Œã¾ã—ãŸã€‚\")\nelse:\n    print(\"\\nâš ï¸ LoRAãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã§ã—ãŸãŒã€\")\n    print(\"   åŸºæœ¬çš„ãªå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯æ§‹ç¯‰ã•ã‚Œã¾ã—ãŸã€‚\")\n    print(\"   PEFT ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦å†å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚\")\n\nprint(\"\\nãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼ ğŸŠ\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å®Ÿé¨“å®Œäº†ãƒ»ã¾ã¨ã‚\n",
    "\n",
    "### ğŸ¯ å®Ÿé¨“ã®æˆæœ\n",
    "\n",
    "1. **LLaDAæ‹¡æ•£å­¦ç¿’ã®å®Ÿè£…**: æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®è¿½åŠ å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰\n",
    "2. **ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥**: ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¹ã‚¯æ¯”ç‡ã§ã®æ‹¡æ•£å­¦ç¿’ã‚’å®Ÿè£…\n",
    "3. **å­¦ç¿’ç›£è¦–**: æå¤±æ¨ç§»ã¨æ¤œè¨¼ã‚¹ã‚³ã‚¢ã®è¿½è·¡\n",
    "4. **ç”Ÿæˆè©•ä¾¡**: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ã®æ—¥æœ¬èªç”Ÿæˆãƒ†ã‚¹ãƒˆ\n",
    "\n",
    "### ğŸ“Š æŠ€è¡“çš„è¦ç‚¹\n",
    "\n",
    "- **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: è¤‡æ•°ã®æ—¥æœ¬èªã‚³ãƒ¼ãƒ‘ã‚¹å¯¾å¿œï¼ˆWikipediaã€OSCARç­‰ï¼‰\n",
    "- **å­¦ç¿’æ‰‹æ³•**: ãƒã‚¹ã‚¯ãƒ‰è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ãƒ™ãƒ¼ã‚¹ã®æ‹¡æ•£å­¦ç¿’\n",
    "- **æœ€é©åŒ–**: AdamW + ç·šå½¢ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼\n",
    "- **è©•ä¾¡**: è¨“ç·´/æ¤œè¨¼æå¤±ã«ã‚ˆã‚‹æ€§èƒ½è¿½è·¡\n",
    "\n",
    "### ğŸš€ ä»Šå¾Œã®ç™ºå±•æ–¹å‘\n",
    "\n",
    "1. **ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—**: ã‚ˆã‚Šå¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨é•·æ™‚é–“å­¦ç¿’\n",
    "2. **å°‚é–€åŒ–**: ç‰¹å®šãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆåŒ»ç™‚ã€æ³•å¾‹ã€æŠ€è¡“ï¼‰ã¸ã®ç‰¹åŒ–\n",
    "3. **åŠ¹ç‡åŒ–**: LoRAã‚„QLoRAã«ã‚ˆã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡çš„å­¦ç¿’\n",
    "4. **è©•ä¾¡å¼·åŒ–**: è‡ªå‹•è©•ä¾¡æŒ‡æ¨™ã¨ãƒ’ãƒ¥ãƒ¼ãƒãƒ³è©•ä¾¡ã®å°å…¥\n",
    "\n",
    "### âš ï¸ åˆ¶é™äº‹é …\n",
    "\n",
    "- æœ¬å®Ÿé¨“ã¯æ•™è‚²ç›®çš„ã§ã®æ¦‚å¿µå®Ÿè¨¼ã§ã™\n",
    "- å®Ÿç”¨ãƒ¬ãƒ™ãƒ«ã«ã¯å¤§è¦æ¨¡ãªè¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã¨æ™‚é–“ãŒå¿…è¦ã§ã™\n",
    "- ãƒ¢ãƒ‡ãƒ«ã®å•†ç”¨åˆ©ç”¨å‰ã«ã¯ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ç¢ºèªãŒå¿…è¦ã§ã™\n",
    "\n",
    "**å®Ÿé¨“ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼** æ—¥æœ¬èªLLaDAã®å­¦ç¿’åŸºç›¤ãŒæ§‹ç¯‰ã§ãã¾ã—ãŸã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}