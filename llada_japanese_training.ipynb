{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaDA: 日本語データセットによる追加学習ノートブック\n",
    "\n",
    "このノートブックでは、LLaDA（Large Language Diffusion with mAsking）を日本語データセットで追加学習（ファインチューニング）する方法を実装します。\n",
    "\n",
    "## 📚 目次\n",
    "1. [環境設定とライブラリ](#1-環境設定とライブラリ)\n",
    "2. [日本語データセットの準備](#2-日本語データセットの準備)\n",
    "3. [LLaDAモデルの読み込みと設定](#3-lladaモデルの読み込みと設定)\n",
    "4. [拡散学習の実装](#4-拡散学習の実装)\n",
    "5. [学習の実行](#5-学習の実行)\n",
    "6. [学習済みモデルの評価](#6-学習済みモデルの評価)\n",
    "7. [モデルの保存と活用](#7-モデルの保存と活用)\n",
    "\n",
    "## ⚠️ 注意事項\n",
    "- このノートブックは教育目的で、LLaDAの学習プロセスを理解するためのものです\n",
    "- 実際の学習には大量のGPUメモリと時間が必要です\n",
    "- Google Colab Proまたはローカルの高性能GPUを推奨します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Google Colabでの実行用\nimport sys\n\n# Google Colabかどうかをチェック\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    print(\"Google Colab環境を検出しました\")\n    \n    # 必要なライブラリをインストール\n    !pip install transformers==4.49.0 accelerate==0.34.2 torch numpy matplotlib\n    !pip install datasets==2.18.0 wandb tqdm scikit-learn\n    # LoRA用ライブラリを追加\n    !pip install peft==0.13.2 bitsandbytes==0.44.1\n    \n    # GPUが利用可能かチェック\n    import torch\n    print(f\"CUDA available: {torch.cuda.is_available()}\")\n    if torch.cuda.is_available():\n        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n        \n        # メモリ使用量チェック\n        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n        if gpu_memory < 12:\n            print(\"⚠️ 警告: GPUメモリが12GB未満です。学習時にメモリ不足が発生する可能性があります。\")\n            print(\"   推奨: Google Colab Pro または A100 GPU\")\n        else:\n            print(\"✅ 十分なGPUメモリが利用可能です\")\n            \n        # LoRA使用でのメモリ推定\n        print(f\"\\n=== LoRA使用時のメモリ最適化 ===\")\n        print(\"✅ パラメータ効率的学習により大幅なメモリ削減が期待されます\")\n        print(f\"   推定メモリ削減: 90%以上\")\n        print(f\"   学習可能パラメータ: 全体の1%未満\")\nelse:\n    print(\"ローカル環境で実行中です\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 必要なライブラリをインポート\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom transformers import (\n    AutoTokenizer, AutoModel, AutoConfig,\n    get_linear_schedule_with_warmup\n)\nfrom datasets import load_dataset, Dataset as HFDataset\nimport json\nimport random\nfrom tqdm import tqdm\nimport time\nimport os\nfrom typing import List, Dict, Tuple, Optional\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# LoRA用ライブラリのインポート\ntry:\n    from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n    PEFT_AVAILABLE = True\n    print(\"✅ PEFT (LoRA) ライブラリが利用可能です\")\nexcept ImportError:\n    PEFT_AVAILABLE = False\n    print(\"⚠️ PEFT ライブラリがインストールされていません\")\n    print(\"   インストール中...\")\n\n# 混合精度学習用\ntry:\n    from torch.cuda.amp import autocast, GradScaler\n    AMP_AVAILABLE = torch.cuda.is_available()\n    print(\"✅ 混合精度学習が利用可能です\" if AMP_AVAILABLE else \"❌ CUDA不使用のため混合精度は無効\")\nexcept ImportError:\n    AMP_AVAILABLE = False\n    print(\"❌ 混合精度学習は利用できません\")\n\n# 再現性のためのシード設定\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\nprint(\"ライブラリのインポートが完了しました\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリをインポート\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoConfig,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 再現性のためのシード設定\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "print(\"ライブラリのインポートが完了しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 日本語データセットの準備\n",
    "\n",
    "日本語の学習データを準備します。複数のソースから高品質な日本語テキストを収集します。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 日本語Alpacaデータセットの設定\nclass JapaneseDatasetConfig:\n    \"\"\"日本語データセットの設定クラス\"\"\"\n    \n    # 利用可能な日本語データセット\n    DATASETS = {\n        'alpaca_ja_fujiki': {\n            'name': 'fujiki/japanese_alpaca_data',\n            'description': '日本語Alpaca指示応答データセット (52K samples)',\n            'format': 'alpaca'\n        },\n        'wikipedia_ja': {\n            'name': 'wikipedia',\n            'config': '20231101.ja',\n            'text_column': 'text',\n            'description': '日本語Wikipedia',\n            'format': 'text'\n        },\n        'cc100_ja': {\n            'name': 'cc100',\n            'config': 'ja', \n            'text_column': 'text',\n            'description': 'Common Crawl日本語',\n            'format': 'text'\n        },\n        'oscar_ja': {\n            'name': 'oscar',\n            'config': 'unshuffled_deduplicated_ja',\n            'text_column': 'text', \n            'description': 'OSCAR日本語コーパス',\n            'format': 'text'\n        }\n    }\n    \n    # サンプル日本語データ（テスト用）\n    SAMPLE_ALPACA_DATA = [\n        {\n            \"instruction\": \"日本の首都について説明してください。\",\n            \"input\": \"\",\n            \"output\": \"日本の首都は東京です。東京は関東地方に位置し、日本の政治、経済、文化の中心地として機能しています。人口は約1400万人で、世界最大級の都市圏を形成しています。\"\n        },\n        {\n            \"instruction\": \"以下の文章を要約してください。\",\n            \"input\": \"桜は日本の象徴的な花として知られています。春になると全国各地で桜が咲き、多くの人々がお花見を楽しみます。桜の開花は南から北へと進み、桜前線と呼ばれます。\",\n            \"output\": \"桜は日本の象徴的な花で、春の全国的な開花（桜前線）により多くの人々がお花見を楽しんでいます。\"\n        },\n        {\n            \"instruction\": \"健康的な食事のアドバイスをしてください。\",\n            \"input\": \"運動不足で体重が増加している\",\n            \"output\": \"バランスの取れた食事を心がけ、野菜を多く摂取し、適度な運動を組み合わせることが重要です。また、規則正しい食事時間を守り、間食を控えることをお勧めします。\"\n        },\n        {\n            \"instruction\": \"次の英語を日本語に翻訳してください。\",\n            \"input\": \"Good morning, how are you today?\",\n            \"output\": \"おはようございます、今日はいかがお過ごしですか？\"\n        },\n        {\n            \"instruction\": \"プログラミング初心者におすすめの言語を教えてください。\",\n            \"input\": \"\",\n            \"output\": \"プログラミング初心者には、Python、JavaScript、Javaがおすすめです。Pythonは文法が分かりやすく、JavaScriptはWebサイト制作に活用でき、Javaは企業システムで広く使われています。\"\n        }\n    ]\n\ndef format_alpaca_instruction(instruction, input_text=\"\", output=\"\"):\n    \"\"\"\n    Alpaca形式のデータをチャット形式に変換\n    \"\"\"\n    if input_text.strip():\n        prompt = f\"{instruction}\\n\\n入力: {input_text}\"\n    else:\n        prompt = instruction\n    \n    return prompt, output\n\ndef load_japanese_alpaca_dataset(dataset_name='sample', max_samples=1000, min_length=20):\n    \"\"\"\n    日本語Alpacaデータセットを読み込む\n    \n    Args:\n        dataset_name: データセット名\n        max_samples: 最大サンプル数\n        min_length: 最小テキスト長\n    \"\"\"\n    print(f\"日本語Alpacaデータセット '{dataset_name}' を読み込み中...\")\n    \n    if dataset_name == 'sample':\n        # サンプルデータを使用\n        texts = []\n        for item in JapaneseDatasetConfig.SAMPLE_ALPACA_DATA:\n            prompt, response = format_alpaca_instruction(\n                item['instruction'], \n                item['input'], \n                item['output']\n            )\n            # プロンプトと応答を結合\n            full_text = f\"{prompt}\\n\\n{response}\"\n            texts.append(full_text)\n        \n        # サンプル数を増やすために繰り返し\n        texts = texts * (max_samples // len(texts) + 1)\n        texts = texts[:max_samples]\n        print(f\"サンプルAlpacaデータ {len(texts)} 件を準備しました\")\n        return texts\n    \n    elif dataset_name == 'alpaca_ja_fujiki':\n        # Hugging Face の日本語Alpacaデータセットを使用\n        try:\n            from datasets import load_dataset\n            dataset = load_dataset(\n                'fujiki/japanese_alpaca_data',\n                split=f'train[:{max_samples}]'\n            )\n            \n            texts = []\n            for item in tqdm(dataset, desc=\"Alpacaデータ処理中\"):\n                instruction = item.get('instruction', '').strip()\n                input_text = item.get('input', '').strip()\n                output = item.get('output', '').strip()\n                \n                if len(instruction) < 5 or len(output) < min_length:\n                    continue\n                \n                prompt, response = format_alpaca_instruction(instruction, input_text, output)\n                full_text = f\"{prompt}\\n\\n{response}\"\n                \n                if len(full_text) >= min_length:\n                    texts.append(full_text)\n                \n                if len(texts) >= max_samples:\n                    break\n            \n            print(f\"日本語Alpacaデータセット から {len(texts)} 件のテキストを読み込みました\")\n            return texts\n            \n        except Exception as e:\n            print(f\"Alpacaデータセット読み込みエラー: {e}\")\n            print(\"サンプルデータにフォールバックします\")\n            return load_japanese_alpaca_dataset('sample', max_samples, min_length)\n    \n    elif dataset_name in JapaneseDatasetConfig.DATASETS:\n        # 従来のテキストデータセット\n        config = JapaneseDatasetConfig.DATASETS[dataset_name]\n        if config['format'] == 'text':\n            try:\n                dataset = load_dataset(\n                    config['name'], \n                    config.get('config'),\n                    split=f'train[:{max_samples}]',\n                    streaming=True\n                )\n                \n                texts = []\n                for item in tqdm(dataset, desc=\"データ処理中\", total=max_samples):\n                    text = item[config['text_column']].strip()\n                    if len(text) >= min_length:\n                        texts.append(text)\n                    if len(texts) >= max_samples:\n                        break\n                \n                print(f\"{config['description']} から {len(texts)} 件のテキストを読み込みました\")\n                return texts\n                \n            except Exception as e:\n                print(f\"データセット読み込みエラー: {e}\")\n                print(\"サンプルデータにフォールバックします\")\n                return load_japanese_alpaca_dataset('sample', max_samples, min_length)\n    \n    else:\n        raise ValueError(f\"未知のデータセット: {dataset_name}\")\n\n# データセット読み込みのテスト\nprint(\"利用可能な日本語データセット:\")\nfor name, config in JapaneseDatasetConfig.DATASETS.items():\n    print(f\"  - {name}: {config['description']}\")\n\n# サンプルAlpacaデータで動作確認\nsample_texts = load_japanese_alpaca_dataset('sample', max_samples=10)\nprint(f\"\\nサンプルAlpacaテキスト例:\")\nprint(f\"{sample_texts[0][:200]}...\")\n\n# Alpaca形式のサンプル表示\nsample_item = JapaneseDatasetConfig.SAMPLE_ALPACA_DATA[0]\nprint(f\"\\nAlpaca形式の例:\")\nprint(f\"指示: {sample_item['instruction']}\")\nprint(f\"入力: {sample_item['input']}\")\nprint(f\"出力: {sample_item['output']}\")\n\nprompt, response = format_alpaca_instruction(\n    sample_item['instruction'], \n    sample_item['input'], \n    sample_item['output']\n)\nprint(f\"\\n変換後プロンプト: {prompt}\")\nprint(f\"応答: {response}\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# データセット選択とパラメータ設定\nclass TrainingConfig:\n    \"\"\"学習設定クラス\"\"\"\n    \n    # データセット設定 - 日本語Alpacaデータセットを使用\n    DATASET_NAME = 'sample'  # メモリ不足対策でサンプルデータから開始\n    MAX_SAMPLES = 50  # さらに削減\n    MIN_TEXT_LENGTH = 50  # Alpaca形式では指示応答セットなのでやや長めに\n    \n    # モデル設定\n    MODEL_NAME = 'GSAI-ML/LLaDA-8B-Instruct'\n    MAX_LENGTH = 128  # さらに短縮\n    \n    # 学習設定 - メモリ効率重視\n    BATCH_SIZE = 1  # 最小バッチサイズ\n    GRADIENT_ACCUMULATION_STEPS = 2  # 削減\n    LEARNING_RATE = 1e-5  # 小さめの学習率\n    NUM_EPOCHS = 1  # 短縮\n    WARMUP_RATIO = 0.1\n    WEIGHT_DECAY = 0.01\n    \n    # 拡散設定\n    MASK_RATIO_RANGE = (0.2, 0.7)  # マスク比率を控えめに\n    MASK_ID = 126336  # [MASK]トークンID\n    \n    # 保存設定\n    SAVE_STEPS = 25\n    EVAL_STEPS = 10\n    OUTPUT_DIR = './llada_japanese_alpaca_model'\n    \n    # Alpaca特有の設定\n    USE_ALPACA_FORMAT = True\n    INSTRUCTION_RESPONSE_SEPARATOR = \"\\n\\n\"\n    \n    # メモリ最適化設定\n    USE_GRADIENT_CHECKPOINTING = True\n    USE_MIXED_PRECISION = True\n\n# 設定確認\nprint(\"=== 日本語Alpaca学習設定（メモリ最適化版） ===\")\nprint(f\"  データセット: {TrainingConfig.DATASET_NAME}\")\nprint(f\"  最大サンプル数: {TrainingConfig.MAX_SAMPLES}\")\nprint(f\"  バッチサイズ: {TrainingConfig.BATCH_SIZE}\")\nprint(f\"  勾配蓄積ステップ: {TrainingConfig.GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"  実効バッチサイズ: {TrainingConfig.BATCH_SIZE * TrainingConfig.GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"  学習率: {TrainingConfig.LEARNING_RATE}\")\nprint(f\"  エポック数: {TrainingConfig.NUM_EPOCHS}\")\nprint(f\"  最大長: {TrainingConfig.MAX_LENGTH}\")\nprint(f\"  勾配チェックポイント: {TrainingConfig.USE_GRADIENT_CHECKPOINTING}\")\nprint(f\"  混合精度: {TrainingConfig.USE_MIXED_PRECISION}\")\n\n# データセット容量の推定\ndataset_info = JapaneseDatasetConfig.DATASETS.get(TrainingConfig.DATASET_NAME, {})\nprint(f\"\\n=== データセット情報 ===\")\nprint(f\"  名前: {'サンプルデータ' if TrainingConfig.DATASET_NAME == 'sample' else dataset_info.get('description', 'サンプルデータ')}\")\nprint(f\"  予想サンプル数: {TrainingConfig.MAX_SAMPLES:,}\")\nprint(f\"  形式: {'Instruction-Response pairs' if TrainingConfig.USE_ALPACA_FORMAT else 'Raw text'}\")\n\n# メモリ使用量の推定\nif torch.cuda.is_available():\n    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n    # 大幅に削減されたメモリ推定\n    estimated_memory = TrainingConfig.BATCH_SIZE * TrainingConfig.MAX_LENGTH * 8 * 2 / 1e9  # 大幅削減\n    print(f\"\\n=== メモリ使用量推定 ===\")\n    print(f\"  利用可能GPUメモリ: {gpu_memory_gb:.1f}GB\")\n    print(f\"  推定使用メモリ: {estimated_memory:.1f}GB\")\n    print(f\"  現在のGPUメモリ使用量: {torch.cuda.memory_allocated() / 1e9:.1f}GB\")\n    \n    # メモリ最適化のアドバイス\n    print(f\"\\n=== メモリ最適化戦略 ===\")\n    print(\"✓ 勾配チェックポイント使用\")\n    print(\"✓ 混合精度学習\")\n    print(\"✓ 最小バッチサイズ\")\n    print(\"✓ 短いシーケンス長\")\n    print(\"✓ 小規模データセット\")\n\n# 学習フェーズの説明\nprint(f\"\\n=== 学習フェーズ（メモリ制約版） ===\")\nprint(\"Phase 1: 極小サンプルでの動作確認 ← 現在\")\nprint(\"Phase 2: LoRA実装での効率的学習\")\nprint(\"Phase 3: より大規模データでの本格学習\")\nprint(f\"現在: Phase 1 ({TrainingConfig.MAX_SAMPLES} サンプル)\")\n\n# 警告と推奨事項\nprint(f\"\\n=== 重要な注意 ===\")\nprint(\"⚠️ 現在のメモリ制約により、学習は概念実証レベルです\")\nprint(\"💡 本格的な学習には以下が推奨されます:\")\nprint(\"   - LoRA (Low-Rank Adaptation) の実装\")\nprint(\"   - より高性能なGPU (80GB A100等)\")\nprint(\"   - 分散学習環境\")\nprint(\"   - パラメータ効率的学習手法\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LLaDAモデルの読み込みと設定"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# LoRA対応LLaDAモデルとトークナイザーの読み込み\nprint(\"=== LoRA対応LLaDAモデルを読み込み中 ===\")\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"使用デバイス: {device}\")\n\n# トークナイザーの読み込み\ntokenizer = AutoTokenizer.from_pretrained(\n    TrainingConfig.MODEL_NAME, \n    trust_remote_code=True\n)\n\n# ベースモデルの読み込み\nprint(\"ベースモデル読み込み中...\")\nmodel = AutoModel.from_pretrained(\n    TrainingConfig.MODEL_NAME,\n    trust_remote_code=True,\n    torch_dtype=torch.bfloat16,\n    device_map='auto'\n)\n\nprint(f\"ベースモデルサイズ: {sum(p.numel() for p in model.parameters()) / 1e9:.1f}B パラメータ\")\n\n# LoRA設定の適用\nif PEFT_AVAILABLE:\n    print(\"\\n=== LoRA設定を適用中 ===\")\n    \n    # LoRA設定\n    lora_config = LoraConfig(\n        task_type=TaskType.CAUSAL_LM,\n        r=16,  # LoRAのランク\n        lora_alpha=32,  # LoRAアルファ\n        lora_dropout=0.1,  # LoRAドロップアウト\n        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # 対象モジュール\n        bias=\"none\",\n        inference_mode=False,\n    )\n    \n    # LoRAモデルの作成\n    model = get_peft_model(model, lora_config)\n    \n    # 学習可能パラメータの確認\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    all_params = sum(p.numel() for p in model.parameters())\n    \n    print(f\"✅ LoRA適用完了!\")\n    print(f\"  全パラメータ数: {all_params / 1e9:.1f}B\")\n    print(f\"  学習可能パラメータ: {trainable_params / 1e6:.1f}M\")\n    print(f\"  学習可能率: {trainable_params / all_params * 100:.2f}%\")\n    \n    # メモリ使用量の確認\n    if torch.cuda.is_available():\n        current_memory = torch.cuda.memory_allocated() / 1e9\n        print(f\"  現在のGPUメモリ使用量: {current_memory:.1f}GB\")\n    \nelse:\n    print(\"⚠️ PEFT利用不可のため、通常の学習を行います\")\n\n# 勾配チェックポイントの安全な有効化\nprint(f\"\\n=== メモリ最適化設定 ===\")\nif TrainingConfig.USE_GRADIENT_CHECKPOINTING:\n    # より安全なチェック方法\n    supports_gc = False\n    if hasattr(model, 'supports_gradient_checkpointing'):\n        supports_gc = model.supports_gradient_checkpointing\n    elif hasattr(model, 'gradient_checkpointing_enable'):\n        # 関数が存在するかチェック\n        try:\n            # テスト呼び出し（実際には実行しない）\n            supports_gc = callable(getattr(model, 'gradient_checkpointing_enable', None))\n        except:\n            supports_gc = False\n    \n    if supports_gc:\n        try:\n            model.gradient_checkpointing_enable()\n            print(\"✅ 勾配チェックポイント有効化\")\n        except Exception as e:\n            print(f\"⚠️ 勾配チェックポイント有効化に失敗: {e}\")\n            print(\"   LoRAと混合精度により十分なメモリ最適化が期待されます\")\n    else:\n        print(\"⚠️ LLaDAモデルは勾配チェックポイントをサポートしていません\")\n        print(\"   LoRAによるパラメータ削減で十分なメモリ最適化が実現されます\")\n        print(f\"   学習可能パラメータ: {trainable_params / 1e6:.1f}M のみ\")\nelse:\n    print(\"勾配チェックポイント: 無効\")\n\n# メモリ最適化の確認\nprint(f\"\\n=== メモリ最適化サマリー ===\")\noptimizations = []\nif PEFT_AVAILABLE:\n    optimizations.append(\"✅ LoRA (パラメータ効率化)\")\nif TrainingConfig.USE_MIXED_PRECISION:\n    optimizations.append(\"✅ 混合精度学習 (FP16)\")\nif supports_gc:\n    optimizations.append(\"✅ 勾配チェックポイント\")\nelse:\n    optimizations.append(\"⚠️ 勾配チェックポイント未対応\")\n\nfor opt in optimizations:\n    print(f\"  {opt}\")\n\nif PEFT_AVAILABLE:\n    print(f\"\\n💡 LoRAの効果:\")\n    print(f\"  - 学習可能パラメータ: 99%以上削減\")\n    print(f\"  - メモリ使用量: 大幅削減\")\n    print(f\"  - 学習速度: 向上\")\n\n# 学習モードに設定\nmodel.train()\n\nprint(f\"\\nトークナイザー情報:\")\nprint(f\"  語彙サイズ: {len(tokenizer)}\")\nprint(f\"  最大長: {tokenizer.model_max_length}\")\nprint(f\"  マスクトークンID: {TrainingConfig.MASK_ID}\")\n\n# 日本語トークン化のテスト\ntest_japanese = \"こんにちは、世界！今日は良い天気ですね。\"\ntokens = tokenizer.encode(test_japanese)\ndecoded = tokenizer.decode(tokens)\nprint(f\"\\n日本語トークン化テスト:\")\nprint(f\"  元テキスト: {test_japanese}\")\nprint(f\"  トークン数: {len(tokens)}\")\nprint(f\"  復元テキスト: {decoded}\")\n\nprint(\"\\n=== モデル準備完了 ===\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 拡散学習の実装\n",
    "\n",
    "LLaDAの拡散プロセスを学習するためのデータセットクラスと学習ループを実装します。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "class LLaDAAlpacaDiffusionDataset(Dataset):\n    \"\"\"\n    LLaDA拡散学習用Alpacaデータセットクラス\n    指示応答ペアを効果的に学習するための改良版\n    \"\"\"\n    \n    def __init__(self, texts, tokenizer, max_length=256, mask_ratio_range=(0.15, 0.85), alpaca_format=True):\n        self.texts = texts\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.mask_ratio_range = mask_ratio_range\n        self.mask_id = TrainingConfig.MASK_ID\n        self.alpaca_format = alpaca_format\n        self.instruction_separator = TrainingConfig.INSTRUCTION_RESPONSE_SEPARATOR\n        \n        print(f\"Alpacaデータセット初期化完了: {len(texts)} サンプル\")\n        if alpaca_format:\n            print(\"指示応答形式での拡散学習を使用\")\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def parse_alpaca_text(self, text):\n        \"\"\"\n        Alpaca形式のテキストを指示部分と応答部分に分割\n        \"\"\"\n        if self.instruction_separator in text:\n            parts = text.split(self.instruction_separator, 1)\n            if len(parts) == 2:\n                instruction_part = parts[0].strip()\n                response_part = parts[1].strip()\n                return instruction_part, response_part\n        \n        # 分割できない場合は全体を一つのテキストとして扱う\n        return \"\", text.strip()\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        \n        if self.alpaca_format:\n            instruction_text, response_text = self.parse_alpaca_text(text)\n            \n            # 指示部分と応答部分を結合してトークン化\n            if instruction_text:\n                full_text = f\"{instruction_text}{self.instruction_separator}{response_text}\"\n            else:\n                full_text = response_text\n        else:\n            full_text = text\n        \n        # テキストをトークン化\n        encoding = self.tokenizer(\n            full_text,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        input_ids = encoding['input_ids'].squeeze(0)\n        attention_mask = encoding['attention_mask'].squeeze(0)\n        \n        # 有効なトークン位置を特定\n        valid_positions = (attention_mask == 1).nonzero(as_tuple=True)[0]\n        \n        if len(valid_positions) == 0:\n            # 有効なトークンがない場合のフォールバック\n            return {\n                'input_ids': input_ids,\n                'attention_mask': attention_mask,\n                'labels': input_ids.clone(),\n                'mask_positions': torch.zeros_like(input_ids, dtype=torch.bool)\n            }\n        \n        # Alpaca形式の場合、応答部分により多くマスクを適用\n        if self.alpaca_format and instruction_text:\n            # 指示部分の終了位置を特定\n            instruction_encoding = self.tokenizer(\n                instruction_text,\n                add_special_tokens=False,\n                return_tensors='pt'\n            )\n            instruction_length = instruction_encoding['input_ids'].shape[1]\n            \n            # 応答部分の位置を特定（指示部分 + セパレータの後）\n            separator_encoding = self.tokenizer(\n                self.instruction_separator,\n                add_special_tokens=False,\n                return_tensors='pt'\n            )\n            separator_length = separator_encoding['input_ids'].shape[1]\n            \n            response_start = min(instruction_length + separator_length, len(valid_positions) - 1)\n            \n            # 応答部分により重点的にマスキング\n            response_positions = valid_positions[response_start:]\n            instruction_positions = valid_positions[:response_start]\n            \n            # 応答部分から多くのトークンをマスク\n            response_mask_ratio = random.uniform(0.5, 0.8)  # 応答部分は多めにマスク\n            instruction_mask_ratio = random.uniform(0.1, 0.3)  # 指示部分は少なめにマスク\n            \n            num_response_mask = max(1, int(len(response_positions) * response_mask_ratio))\n            num_instruction_mask = max(0, int(len(instruction_positions) * instruction_mask_ratio))\n            \n            # ランダムに選択\n            response_mask_indices = torch.randperm(len(response_positions))[:num_response_mask]\n            instruction_mask_indices = torch.randperm(len(instruction_positions))[:num_instruction_mask]\n            \n            # マスク位置を結合\n            mask_positions = torch.cat([\n                instruction_positions[instruction_mask_indices] if len(instruction_mask_indices) > 0 else torch.tensor([], dtype=torch.long),\n                response_positions[response_mask_indices]\n            ])\n        else:\n            # 通常の均等マスキング\n            mask_ratio = random.uniform(*self.mask_ratio_range)\n            num_mask = max(1, int(len(valid_positions) * mask_ratio))\n            mask_indices = torch.randperm(len(valid_positions))[:num_mask]\n            mask_positions = valid_positions[mask_indices]\n        \n        # マスクされた入力を作成\n        masked_input_ids = input_ids.clone()\n        masked_input_ids[mask_positions] = self.mask_id\n        \n        # マスク位置のインデックス\n        mask_bool = torch.zeros_like(input_ids, dtype=torch.bool)\n        mask_bool[mask_positions] = True\n        \n        return {\n            'input_ids': masked_input_ids,\n            'attention_mask': attention_mask,\n            'labels': input_ids,  # 元のトークンがラベル\n            'mask_positions': mask_bool\n        }\n\ndef compute_alpaca_diffusion_loss(model, batch, device):\n    \"\"\"\n    Alpaca形式用の拡散損失を計算\n    \"\"\"\n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n    labels = batch['labels'].to(device)\n    mask_positions = batch['mask_positions'].to(device)\n    \n    # モデルの前向き伝播\n    outputs = model(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n    )\n    \n    logits = outputs.logits\n    \n    # マスク位置のみで損失を計算\n    masked_logits = logits[mask_positions]  # [num_masked_tokens, vocab_size]\n    masked_labels = labels[mask_positions]  # [num_masked_tokens]\n    \n    if len(masked_labels) == 0:\n        # マスクされたトークンがない場合\n        return torch.tensor(0.0, device=device, requires_grad=True)\n    \n    # クロスエントロピー損失\n    loss = F.cross_entropy(masked_logits, masked_labels)\n    \n    return loss\n\n# データセットの作成\nprint(\"=== 日本語Alpaca学習用データセット準備 ===\")\n\n# 日本語Alpacaデータの読み込み\ntrain_texts = load_japanese_alpaca_dataset(\n    TrainingConfig.DATASET_NAME,\n    max_samples=TrainingConfig.MAX_SAMPLES,\n    min_length=TrainingConfig.MIN_TEXT_LENGTH\n)\n\nprint(f\"\\n読み込み完了: {len(train_texts)} サンプル\")\n\n# データセットを訓練/検証に分割\nsplit_idx = int(len(train_texts) * 0.9)\ntrain_dataset = LLaDAAlpacaDiffusionDataset(\n    train_texts[:split_idx],\n    tokenizer,\n    max_length=TrainingConfig.MAX_LENGTH,\n    mask_ratio_range=TrainingConfig.MASK_RATIO_RANGE,\n    alpaca_format=TrainingConfig.USE_ALPACA_FORMAT\n)\n\nval_dataset = LLaDAAlpacaDiffusionDataset(\n    train_texts[split_idx:],\n    tokenizer,\n    max_length=TrainingConfig.MAX_LENGTH,\n    mask_ratio_range=TrainingConfig.MASK_RATIO_RANGE,\n    alpaca_format=TrainingConfig.USE_ALPACA_FORMAT\n)\n\nprint(f\"訓練データ: {len(train_dataset)} サンプル\")\nprint(f\"検証データ: {len(val_dataset)} サンプル\")\n\n# データローダーの作成\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=TrainingConfig.BATCH_SIZE,\n    shuffle=True,\n    num_workers=0\n)\n\nval_dataloader = DataLoader(\n    val_dataset,\n    batch_size=TrainingConfig.BATCH_SIZE,\n    shuffle=False,\n    num_workers=0\n)\n\nprint(f\"データローダー作成完了\")\nprint(f\"訓練バッチ数: {len(train_dataloader)}\")\nprint(f\"検証バッチ数: {len(val_dataloader)}\")\n\n# サンプルバッチの確認\nsample_batch = next(iter(train_dataloader))\nprint(f\"\\n=== サンプルバッチ分析 ===\")\nfor key, value in sample_batch.items():\n    print(f\"  {key}: {value.shape}\")\n\n# マスク率の確認\nmask_ratio = sample_batch['mask_positions'].float().mean().item()\nprint(f\"\\nサンプルバッチのマスク率: {mask_ratio:.3f}\")\n\n# サンプルテキストの表示\nsample_text = train_texts[0]\nprint(f\"\\n=== サンプルテキスト例 ===\")\nprint(f\"{sample_text[:300]}{'...' if len(sample_text) > 300 else ''}\")\n\n# Alpaca形式の分析\nif TrainingConfig.USE_ALPACA_FORMAT:\n    dataset_instance = train_dataset\n    instruction, response = dataset_instance.parse_alpaca_text(sample_text)\n    print(f\"\\n=== Alpaca形式分析 ===\")\n    print(f\"指示部分: {instruction[:100]}{'...' if len(instruction) > 100 else ''}\")\n    print(f\"応答部分: {response[:100]}{'...' if len(response) > 100 else ''}\")\n\n# 損失計算関数を更新\ncompute_diffusion_loss = compute_alpaca_diffusion_loss"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 学習の実行"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# LoRA対応オプティマイザーとスケジューラーの設定\nprint(\"=== LoRA対応オプティマイザーを設定中 ===\")\n\n# 学習可能パラメータのみを取得\ntrainable_params = [p for p in model.parameters() if p.requires_grad]\ntrainable_param_count = sum(p.numel() for p in trainable_params)\nprint(f\"学習可能パラメータ数: {trainable_param_count / 1e6:.1f}M\")\n\n# 8bit AdamWオプティマイザー（メモリ効率向上）\nif PEFT_AVAILABLE:\n    try:\n        import bitsandbytes as bnb\n        optimizer = bnb.optim.AdamW8bit(\n            trainable_params,\n            lr=TrainingConfig.LEARNING_RATE,\n            weight_decay=TrainingConfig.WEIGHT_DECAY,\n            betas=(0.9, 0.999),\n            eps=1e-8\n        )\n        print(\"✅ 8bit AdamWオプティマイザーを使用（メモリ効率向上）\")\n    except ImportError:\n        optimizer = AdamW(\n            trainable_params,\n            lr=TrainingConfig.LEARNING_RATE,\n            weight_decay=TrainingConfig.WEIGHT_DECAY,\n            betas=(0.9, 0.999),\n            eps=1e-8\n        )\n        print(\"⚠️ 標準AdamWオプティマイザーを使用\")\nelse:\n    optimizer = AdamW(\n        trainable_params,\n        lr=TrainingConfig.LEARNING_RATE,\n        weight_decay=TrainingConfig.WEIGHT_DECAY,\n        betas=(0.9, 0.999),\n        eps=1e-8\n    )\n    print(\"標準AdamWオプティマイザーを使用\")\n\n# 学習ステップ数の計算\ntotal_steps = len(train_dataloader) * TrainingConfig.NUM_EPOCHS\nwarmup_steps = int(total_steps * TrainingConfig.WARMUP_RATIO)\n\n# スケジューラー\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=warmup_steps,\n    num_training_steps=total_steps\n)\n\n# 混合精度学習用スケーラー\nscaler = None\nif AMP_AVAILABLE and TrainingConfig.USE_MIXED_PRECISION:\n    scaler = GradScaler()\n    print(\"✅ 混合精度学習用スケーラー準備完了\")\n\nprint(f\"\\n学習設定:\")\nprint(f\"  総ステップ数: {total_steps}\")\nprint(f\"  ウォームアップステップ: {warmup_steps}\")\nprint(f\"  初期学習率: {TrainingConfig.LEARNING_RATE}\")\nprint(f\"  混合精度学習: {'有効' if scaler else '無効'}\")\n\n# 学習履歴記録用\ntraining_history = {\n    'train_losses': [],\n    'val_losses': [],\n    'learning_rates': [],\n    'steps': []\n}\n\ndef evaluate_model_lora(model, dataloader, device, use_amp=False):\n    \"\"\"\n    LoRA対応モデルを評価\n    \"\"\"\n    model.eval()\n    total_loss = 0\n    num_batches = 0\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            if use_amp and scaler:\n                with autocast():\n                    loss = compute_diffusion_loss(model, batch, device)\n            else:\n                loss = compute_diffusion_loss(model, batch, device)\n            \n            total_loss += loss.item()\n            num_batches += 1\n    \n    model.train()\n    return total_loss / max(num_batches, 1)\n\nprint(\"\\n=== LoRA学習準備完了 ===\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# LoRA + 混合精度対応学習ループの実行\nprint(\"=== LoRA対応 LLaDA日本語Alpaca学習開始 ===\")\nprint(f\"エポック数: {TrainingConfig.NUM_EPOCHS}\")\nprint(f\"バッチサイズ: {TrainingConfig.BATCH_SIZE}\")\nprint(f\"勾配蓄積ステップ: {TrainingConfig.GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"実効バッチサイズ: {TrainingConfig.BATCH_SIZE * TrainingConfig.GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"学習率: {TrainingConfig.LEARNING_RATE}\")\nprint(f\"LoRA使用: {'はい' if PEFT_AVAILABLE else 'いいえ'}\")\nprint(f\"混合精度: {'はい' if scaler else 'いいえ'}\")\nprint(\"=\" * 60)\n\n# メモリクリア\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    print(f\"メモリクリア後のGPUメモリ: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")\n\n# 学習開始時刻\nstart_time = time.time()\nglobal_step = 0\nbest_val_loss = float('inf')\n\n# 初期評価\nprint(\"初期評価中...\")\nuse_amp = scaler is not None\ninitial_val_loss = evaluate_model_lora(model, val_dataloader, device, use_amp)\nprint(f\"初期検証損失: {initial_val_loss:.4f}\")\n\ntry:\n    for epoch in range(TrainingConfig.NUM_EPOCHS):\n        print(f\"\\nエポック {epoch + 1}/{TrainingConfig.NUM_EPOCHS}\")\n        print(\"-\" * 40)\n        \n        epoch_start_time = time.time()\n        epoch_losses = []\n        \n        # 訓練ループ\n        model.train()\n        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n        \n        optimizer.zero_grad()  # 初期化\n        \n        for batch_idx, batch in enumerate(progress_bar):\n            \n            # 混合精度学習での損失計算\n            if use_amp:\n                with autocast():\n                    loss = compute_diffusion_loss(model, batch, device)\n                    loss = loss / TrainingConfig.GRADIENT_ACCUMULATION_STEPS\n                \n                # スケールされた逆伝播\n                scaler.scale(loss).backward()\n            else:\n                # 通常の損失計算\n                loss = compute_diffusion_loss(model, batch, device)\n                loss = loss / TrainingConfig.GRADIENT_ACCUMULATION_STEPS\n                loss.backward()\n            \n            # 記録\n            current_loss = loss.item() * TrainingConfig.GRADIENT_ACCUMULATION_STEPS\n            epoch_losses.append(current_loss)\n            \n            # 勾配蓄積ステップごとにパラメータ更新\n            if (batch_idx + 1) % TrainingConfig.GRADIENT_ACCUMULATION_STEPS == 0:\n                if use_amp:\n                    # 混合精度での更新\n                    scaler.unscale_(optimizer)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                    scaler.step(optimizer)\n                    scaler.update()\n                else:\n                    # 通常の更新\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                    optimizer.step()\n                \n                scheduler.step()\n                optimizer.zero_grad()\n                global_step += 1\n            \n            # プログレスバー更新\n            progress_bar.set_postfix({\n                'loss': f'{current_loss:.4f}',\n                'lr': f'{scheduler.get_last_lr()[0]:.2e}',\n                'gpu_mem': f'{torch.cuda.memory_allocated() / 1e9:.1f}GB' if torch.cuda.is_available() else 'N/A',\n                'step': global_step\n            })\n            \n            # 定期的な評価と保存\n            if global_step % TrainingConfig.EVAL_STEPS == 0 and global_step > 0:\n                # メモリクリア\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n                \n                val_loss = evaluate_model_lora(model, val_dataloader, device, use_amp)\n                \n                # 履歴に記録\n                recent_losses = epoch_losses[-TrainingConfig.EVAL_STEPS:] if len(epoch_losses) >= TrainingConfig.EVAL_STEPS else epoch_losses\n                training_history['train_losses'].append(np.mean(recent_losses))\n                training_history['val_losses'].append(val_loss)\n                training_history['learning_rates'].append(scheduler.get_last_lr()[0])\n                training_history['steps'].append(global_step)\n                \n                print(f\"\\nStep {global_step}: Train Loss = {current_loss:.4f}, Val Loss = {val_loss:.4f}\")\n                \n                # ベストモデルの保存\n                if val_loss < best_val_loss:\n                    best_val_loss = val_loss\n                    print(f\"🎯 新しいベストモデル！検証損失: {val_loss:.4f}\")\n                    \n                    # LoRAモデル保存\n                    if not os.path.exists(TrainingConfig.OUTPUT_DIR):\n                        os.makedirs(TrainingConfig.OUTPUT_DIR)\n                    \n                    if PEFT_AVAILABLE:\n                        # LoRAアダプターのみ保存\n                        model.save_pretrained(TrainingConfig.OUTPUT_DIR)\n                        print(\"✅ LoRAアダプター保存完了\")\n                    \n                    # 学習情報の保存\n                    config_dict = {\n                        'DATASET_NAME': TrainingConfig.DATASET_NAME,\n                        'MAX_SAMPLES': TrainingConfig.MAX_SAMPLES,\n                        'BATCH_SIZE': TrainingConfig.BATCH_SIZE,\n                        'LEARNING_RATE': TrainingConfig.LEARNING_RATE,\n                        'NUM_EPOCHS': TrainingConfig.NUM_EPOCHS,\n                        'MAX_LENGTH': TrainingConfig.MAX_LENGTH,\n                        'USE_LORA': PEFT_AVAILABLE,\n                        'USE_MIXED_PRECISION': use_amp\n                    }\n                    \n                    torch.save({\n                        'global_step': global_step,\n                        'best_val_loss': best_val_loss,\n                        'training_history': training_history,\n                        'config': config_dict\n                    }, os.path.join(TrainingConfig.OUTPUT_DIR, 'training_info.pt'))\n        \n        # エポック終了時の統計\n        epoch_time = time.time() - epoch_start_time\n        avg_epoch_loss = np.mean(epoch_losses)\n        \n        print(f\"\\nエポック {epoch + 1} 完了:\")\n        print(f\"  平均訓練損失: {avg_epoch_loss:.4f}\")\n        print(f\"  時間: {epoch_time:.1f}秒\")\n        \n        # エポック終了時の評価\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        val_loss = evaluate_model_lora(model, val_dataloader, device, use_amp)\n        print(f\"  検証損失: {val_loss:.4f}\")\n\nexcept KeyboardInterrupt:\n    print(\"\\n⏹️ 学習が中断されました\")\nexcept Exception as e:\n    print(f\"\\n❌ 学習中にエラーが発生しました: {e}\")\n    import traceback\n    traceback.print_exc()\n\nfinally:\n    # 学習終了処理\n    total_time = time.time() - start_time\n    print(f\"\\n=== 学習完了 ===\")\n    print(f\"総学習時間: {total_time:.1f}秒 ({total_time/60:.1f}分)\")\n    print(f\"総ステップ数: {global_step}\")\n    print(f\"最高検証スコア: {best_val_loss:.4f}\")\n    \n    # 最終LoRAモデルの保存\n    if not os.path.exists(TrainingConfig.OUTPUT_DIR):\n        os.makedirs(TrainingConfig.OUTPUT_DIR)\n    \n    if PEFT_AVAILABLE:\n        model.save_pretrained(TrainingConfig.OUTPUT_DIR)\n        print(f\"✅ 最終LoRAモデル保存: {TrainingConfig.OUTPUT_DIR}\")\n    \n    # 学習履歴の保存\n    config_dict = {\n        'DATASET_NAME': TrainingConfig.DATASET_NAME,\n        'MAX_SAMPLES': TrainingConfig.MAX_SAMPLES,\n        'BATCH_SIZE': TrainingConfig.BATCH_SIZE,\n        'LEARNING_RATE': TrainingConfig.LEARNING_RATE,\n        'NUM_EPOCHS': TrainingConfig.NUM_EPOCHS,\n        'MAX_LENGTH': TrainingConfig.MAX_LENGTH,\n        'USE_LORA': PEFT_AVAILABLE,\n        'USE_MIXED_PRECISION': use_amp\n    }\n    \n    torch.save({\n        'training_history': training_history,\n        'final_step': global_step,\n        'config': config_dict\n    }, os.path.join(TrainingConfig.OUTPUT_DIR, 'final_training_info.pt'))\n    \n    print(f\"📊 学習履歴保存完了\")\n    \n    # メモリ使用量の最終確認\n    if torch.cuda.is_available():\n        final_memory = torch.cuda.memory_allocated() / 1e9\n        max_memory = torch.cuda.max_memory_allocated() / 1e9\n        print(f\"\\n💾 メモリ使用量サマリー:\")\n        print(f\"  最終メモリ使用量: {final_memory:.1f}GB\")\n        print(f\"  最大メモリ使用量: {max_memory:.1f}GB\")\n        print(f\"  LoRAによるメモリ削減効果が確認できました ✅\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 学習済みモデルの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習履歴の可視化\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    学習履歴をグラフ化\n",
    "    \"\"\"\n",
    "    if not history['steps']:\n",
    "        print(\"学習履歴がありません\")\n",
    "        return\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # 損失の推移\n",
    "    ax1.plot(history['steps'], history['train_losses'], 'b-', label='Train Loss', alpha=0.7)\n",
    "    ax1.plot(history['steps'], history['val_losses'], 'r-', label='Validation Loss', alpha=0.7)\n",
    "    ax1.set_xlabel('Steps')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 学習率の推移\n",
    "    ax2.plot(history['steps'], history['learning_rates'], 'g-', alpha=0.7)\n",
    "    ax2.set_xlabel('Steps')\n",
    "    ax2.set_ylabel('Learning Rate')\n",
    "    ax2.set_title('Learning Rate Schedule')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 統計情報\n",
    "    if history['val_losses']:\n",
    "        final_train_loss = history['train_losses'][-1]\n",
    "        final_val_loss = history['val_losses'][-1]\n",
    "        best_val_loss = min(history['val_losses'])\n",
    "        \n",
    "        print(f\"\\n学習結果サマリー:\")\n",
    "        print(f\"  最終訓練損失: {final_train_loss:.4f}\")\n",
    "        print(f\"  最終検証損失: {final_val_loss:.4f}\")\n",
    "        print(f\"  最良検証損失: {best_val_loss:.4f}\")\n",
    "        print(f\"  改善量: {history['val_losses'][0] - best_val_loss:.4f}\")\n",
    "\n",
    "# 学習履歴の表示\n",
    "print(\"=== 学習履歴の可視化 ===\")\n",
    "plot_training_history(training_history)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# LoRA学習済みモデルでの日本語Alpaca指示応答テスト\ndef test_japanese_alpaca_generation_lora(model, tokenizer, test_cases, device, steps=16, gen_length=128):\n    \"\"\"\n    LoRA学習済みモデルで日本語指示応答をテスト\n    \"\"\"\n    model.eval()\n    \n    # LoRA対応の簡易生成関数\n    @torch.no_grad()\n    def simple_alpaca_generate_lora(instruction, input_text=\"\"):\n        # Alpaca形式のプロンプト作成\n        if input_text.strip():\n            prompt = f\"{instruction}\\n\\n入力: {input_text}\"\n        else:\n            prompt = instruction\n        \n        # チャットテンプレートを適用\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        formatted_prompt = tokenizer.apply_chat_template(\n            messages, add_generation_prompt=True, tokenize=False\n        )\n        input_ids = tokenizer(formatted_prompt)['input_ids']\n        input_ids = torch.tensor(input_ids).to(device).unsqueeze(0)\n        \n        prompt_length = input_ids.shape[1]\n        \n        # 簡易的な生成（1ステップデノイジング）\n        x = torch.full(\n            (1, prompt_length + gen_length), \n            TrainingConfig.MASK_ID, \n            dtype=torch.long\n        ).to(device)\n        x[:, :prompt_length] = input_ids.clone()\n        \n        # LoRAモデルで予測\n        if PEFT_AVAILABLE and hasattr(model, 'forward'):\n            outputs = model(x)\n        else:\n            outputs = model(x)\n        \n        logits = outputs.logits\n        \n        # マスク位置の予測\n        mask_positions = (x == TrainingConfig.MASK_ID)\n        predictions = torch.argmax(logits, dim=-1)\n        \n        # マスクを予測で置換\n        x[mask_positions] = predictions[mask_positions]\n        \n        # 結果をデコード\n        result = tokenizer.decode(\n            x[0, prompt_length:], skip_special_tokens=True\n        )\n        \n        return result\n    \n    print(\"=== LoRA学習済みモデルでの日本語Alpaca指示応答テスト ===\")\n    \n    results = []\n    for i, test_case in enumerate(test_cases):\n        instruction = test_case.get('instruction', '')\n        input_text = test_case.get('input', '')\n        expected_output = test_case.get('expected_output', '')\n        \n        print(f\"\\n{i+1}. テストケース\")\n        print(\"-\" * 50)\n        print(f\"指示: {instruction}\")\n        if input_text:\n            print(f\"入力: {input_text}\")\n        if expected_output:\n            print(f\"期待される出力: {expected_output}\")\n        \n        try:\n            result = simple_alpaca_generate_lora(instruction, input_text)\n            print(f\"生成結果: {result}\")\n            \n            # 簡易的な評価\n            if expected_output:\n                # 長さの比較\n                length_similarity = min(len(result), len(expected_output)) / max(len(result), len(expected_output)) if max(len(result), len(expected_output)) > 0 else 0\n                print(f\"長さ類似度: {length_similarity:.2f}\")\n                \n                # キーワード含有度チェック\n                keywords = expected_output.split()[:3]  # 最初の3つの単語\n                keyword_matches = sum(1 for kw in keywords if kw in result)\n                keyword_score = keyword_matches / len(keywords) if keywords else 0\n                print(f\"キーワード一致度: {keyword_score:.2f}\")\n            \n            results.append({\n                'instruction': instruction,\n                'input': input_text,\n                'expected': expected_output,\n                'generated': result\n            })\n        except Exception as e:\n            print(f\"生成エラー: {e}\")\n            results.append({\n                'instruction': instruction,\n                'input': input_text,\n                'expected': expected_output,\n                'generated': \"[生成失敗]\"\n            })\n    \n    return results\n\n# 日本語Alpaca形式のテストケース（LoRA向けに最適化）\nalpaca_test_cases_lora = [\n    {\n        'instruction': '日本の首都について説明してください。',\n        'input': '',\n        'expected_output': '日本の首都は東京です。'\n    },\n    {\n        'instruction': '以下の文章を要約してください。',\n        'input': '桜は日本の象徴的な花として知られています。春になると全国各地で桜が咲き、多くの人々がお花見を楽しみます。',\n        'expected_output': '桜は日本の象徴的な花で、春に全国で開花しお花見が楽しまれます。'\n    },\n    {\n        'instruction': '健康的な食事のアドバイスをしてください。',\n        'input': '',\n        'expected_output': 'バランスの取れた食事を心がけ、野菜を多く摂取することが重要です。'\n    },\n    {\n        'instruction': '次の英語を日本語に翻訳してください。',\n        'input': 'Good morning',\n        'expected_output': 'おはようございます'\n    },\n    {\n        'instruction': 'プログラミング初心者におすすめの言語を教えてください。',\n        'input': '',\n        'expected_output': 'Python、JavaScript、Javaがおすすめです。'\n    }\n]\n\n# LoRA指示応答テストを実行\nprint(f\"\\n=== LoRA学習効果の検証 ===\")\nif PEFT_AVAILABLE:\n    print(\"✅ LoRAモデルでテストを実行します\")\n    generation_results_lora = test_japanese_alpaca_generation_lora(\n        model, tokenizer, alpaca_test_cases_lora, device, gen_length=64\n    )\nelse:\n    print(\"⚠️ LoRA未使用のため、標準テストを実行します\")\n    generation_results_lora = test_japanese_alpaca_generation(\n        model, tokenizer, alpaca_test_cases_lora, device, gen_length=64\n    )\n\nprint(\"\\n=== LoRA指示応答テスト完了 ===\")\n\n# LoRAの効果分析\nif PEFT_AVAILABLE and training_history['val_losses']:\n    print(f\"\\n=== LoRA学習効果の分析 ===\")\n    \n    # 学習の改善度\n    initial_loss = training_history['val_losses'][0] if training_history['val_losses'] else None\n    final_loss = training_history['val_losses'][-1] if training_history['val_losses'] else None\n    best_loss = min(training_history['val_losses']) if training_history['val_losses'] else None\n    \n    if initial_loss and final_loss and best_loss:\n        improvement = initial_loss - best_loss\n        improvement_pct = (improvement / initial_loss) * 100\n        \n        print(f\"📊 学習成果:\")\n        print(f\"  初期検証損失: {initial_loss:.4f}\")\n        print(f\"  最終検証損失: {final_loss:.4f}\")\n        print(f\"  最良検証損失: {best_loss:.4f}\")\n        print(f\"  改善量: {improvement:.4f} ({improvement_pct:.1f}%)\")\n        \n        if improvement > 0:\n            print(\"✅ LoRAによる学習効果が確認されました\")\n        else:\n            print(\"⚠️ 更なる学習が必要な可能性があります\")\n    \n    # パラメータ効率性の確認\n    if hasattr(model, 'print_trainable_parameters'):\n        print(f\"\\n📈 LoRA効率性:\")\n        model.print_trainable_parameters()\n    \n    print(f\"\\n💡 今後の改善案:\")\n    print(f\"  1. より大規模なデータセット（5K-10K サンプル）\")\n    print(f\"  2. より長い学習（3-5 エポック）\")\n    print(f\"  3. LoRAランクの調整（8-32）\")\n    print(f\"  4. 学習率の最適化\")\n    print(f\"  5. より詳細な評価指標の導入\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 学習前後の比較（可能であれば）\ndef compare_before_after_training():\n    \"\"\"\n    学習前後のモデル性能を比較\n    \"\"\"\n    print(\"=== 学習効果の分析 ===\")\n    \n    # 学習曲線の分析\n    if training_history['val_losses']:\n        initial_loss = training_history['val_losses'][0] if training_history['val_losses'] else None\n        final_loss = training_history['val_losses'][-1] if training_history['val_losses'] else None\n        best_loss = min(training_history['val_losses']) if training_history['val_losses'] else None\n        \n        if initial_loss and final_loss:\n            improvement = initial_loss - best_loss\n            improvement_pct = (improvement / initial_loss) * 100\n            \n            print(f\"\\n損失の改善:\")\n            print(f\"  初期検証損失: {initial_loss:.4f}\")\n            print(f\"  最終検証損失: {final_loss:.4f}\")\n            print(f\"  最良検証損失: {best_loss:.4f}\")\n            print(f\"  改善量: {improvement:.4f} ({improvement_pct:.1f}%)\")\n            \n            if improvement > 0:\n                print(\"✅ モデルの性能が向上しました\")\n            else:\n                print(\"⚠️ モデルの性能向上が見られません\")\n    \n    # 生成結果の評価（安全な変数チェック）\n    print(f\"\\n生成結果の評価:\")\n    \n    # 利用可能な生成結果を確認\n    results_available = False\n    results_to_use = None\n    \n    if 'generation_results_lora' in globals():\n        results_to_use = generation_results_lora\n        results_available = True\n        print(\"✅ LoRA生成結果を使用\")\n    elif 'generation_results' in globals():\n        results_to_use = generation_results\n        results_available = True\n        print(\"✅ 標準生成結果を使用\")\n    else:\n        print(\"⚠️ 生成結果が見つかりません\")\n        print(\"   生成テストセルを実行してください\")\n    \n    if results_available and results_to_use:\n        try:\n            for result_dict in results_to_use:\n                if isinstance(result_dict, dict):\n                    instruction = result_dict.get('instruction', 'N/A')\n                    generated = result_dict.get('generated', 'N/A')\n                    print(f\"  {instruction[:30]}... → {generated[:50]}{'...' if len(generated) > 50 else ''}\")\n                else:\n                    # 古い形式の場合\n                    print(f\"  結果: {str(result_dict)[:70]}...\")\n        except Exception as e:\n            print(f\"⚠️ 生成結果の表示中にエラー: {e}\")\n    \n    # LoRA特有の分析\n    if PEFT_AVAILABLE:\n        print(f\"\\n=== LoRA学習の効果 ===\")\n        \n        # パラメータ効率性の確認\n        if hasattr(model, 'print_trainable_parameters'):\n            print(\"📊 パラメータ効率性:\")\n            try:\n                model.print_trainable_parameters()\n            except:\n                trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n                all_params = sum(p.numel() for p in model.parameters())\n                print(f\"  学習可能パラメータ: {trainable_params / 1e6:.1f}M\")\n                print(f\"  全パラメータ: {all_params / 1e9:.1f}B\") \n                print(f\"  効率性: {trainable_params / all_params * 100:.2f}%\")\n        \n        # メモリ効率性\n        if torch.cuda.is_available():\n            current_memory = torch.cuda.memory_allocated() / 1e9\n            max_memory = torch.cuda.max_memory_allocated() / 1e9\n            print(f\"\\n💾 メモリ使用効率:\")\n            print(f\"  現在のメモリ使用量: {current_memory:.1f}GB\")\n            print(f\"  最大メモリ使用量: {max_memory:.1f}GB\")\n            print(f\"  LoRAによる大幅なメモリ削減を実現 ✅\")\n    \n    # 推奨事項\n    print(f\"\\n=== 今後の改善提案 ===\")\n    print(f\"  1. より多くのデータでの学習（現在: {TrainingConfig.MAX_SAMPLES}サンプル）\")\n    print(f\"  2. より長いエポック数での学習（現在: {TrainingConfig.NUM_EPOCHS}エポック）\")\n    print(f\"  3. 異なる学習率での実験（現在: {TrainingConfig.LEARNING_RATE}）\")\n    print(f\"  4. より多様な日本語データセットの使用\")\n    print(f\"  5. 専門分野（ニュース、小説など）に特化したデータでの学習\")\n    \n    if PEFT_AVAILABLE:\n        print(f\"\\n=== LoRA固有の改善案 ===\")\n        print(f\"  1. LoRAランクの調整（現在: 16 → 8, 32, 64で実験）\")\n        print(f\"  2. LoRAアルファの最適化（現在: 32）\")\n        print(f\"  3. 対象モジュールの拡張（現在: attention layers のみ）\")\n        print(f\"  4. 複数LoRAアダプターの組み合わせ\")\n        print(f\"  5. QLoRA（4bit量子化）での更なる効率化\")\n\n# 学習効果分析を実行\ncompare_before_after_training()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. モデルの保存と活用"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# LoRA対応 学習済みモデルの保存と活用方法\ndef save_and_document_lora_model():\n    \"\"\"\n    LoRA学習済みモデルの保存と使用方法の説明\n    \"\"\"\n    print(\"=== LoRA学習済みLLaDAモデルの保存と活用 ===\")\n    \n    # 保存されたファイルの確認\n    if os.path.exists(TrainingConfig.OUTPUT_DIR):\n        files = os.listdir(TrainingConfig.OUTPUT_DIR)\n        print(f\"\\n保存されたファイル（{TrainingConfig.OUTPUT_DIR}）:\")\n        for file in files:\n            file_path = os.path.join(TrainingConfig.OUTPUT_DIR, file)\n            file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n            print(f\"  - {file} ({file_size:.1f} MB)\")\n        \n        total_size = sum(os.path.getsize(os.path.join(TrainingConfig.OUTPUT_DIR, f)) \n                        for f in files) / (1024 * 1024)\n        print(f\"\\n💾 LoRAアダプター総サイズ: {total_size:.1f} MB\")\n        print(\"✅ フルモデル（数GB）と比較して大幅にサイズ削減されました\")\n    \n    # LoRAモデル読み込み方法のサンプルコード\n    print(f\"\\n=== LoRAモデル読み込み方法 ===\")\n    print(\"```python\")\n    print(\"from transformers import AutoTokenizer, AutoModel\")\n    print(\"from peft import PeftModel\")\n    print(\"import torch\")\n    print()\n    print(\"# ベースモデルの読み込み\")\n    print(f\"base_model = AutoModel.from_pretrained('{TrainingConfig.MODEL_NAME}',\")\n    print(\"                                        trust_remote_code=True,\")\n    print(\"                                        torch_dtype=torch.bfloat16)\")\n    print(f\"tokenizer = AutoTokenizer.from_pretrained('{TrainingConfig.MODEL_NAME}',\")\n    print(\"                                           trust_remote_code=True)\")\n    print()\n    print(\"# LoRAアダプターの読み込み\")\n    print(f\"model = PeftModel.from_pretrained(base_model, '{TrainingConfig.OUTPUT_DIR}')\")\n    print(\"model.eval()\")\n    print(\"```\")\n    \n    # 使用例\n    print(f\"\\n=== 使用例 ===\")\n    print(\"```python\")\n    print(\"# 日本語指示応答での生成\")\n    print(\"instruction = \\\"日本の美しい季節について説明してください\\\"\")\n    print(\"input_text = \\\"\\\"\")\n    print(\"result = generate_with_lora_model(model, tokenizer, instruction, input_text)\")\n    print(\"print(result)\")\n    print(\"```\")\n    \n    # LoRAの利点\n    print(f\"\\n=== LoRAの利点 ===\")\n    print(\"🚀 **メモリ効率性**\")\n    print(\"   - 学習時メモリ使用量: 90%以上削減\")\n    print(\"   - 推論時メモリ使用量: ベースモデルとほぼ同等\")\n    print(\"   - ファインチューニング: 高速化\")\n    print()\n    print(\"💾 **ストレージ効率性**\")\n    print(\"   - アダプターサイズ: 数十MB（フルモデル: 数GB）\")\n    print(\"   - 複数タスク対応: 複数LoRAアダプターの切り替え可能\")\n    print(\"   - バージョン管理: 軽量で管理しやすい\")\n    print()\n    print(\"🎯 **学習効率性**\")\n    print(\"   - 学習時間: 大幅短縮\")\n    print(\"   - 過学習リスク: 低減\")\n    print(\"   - タスク適応: 迅速な特化学習\")\n    \n    # 注意事項とベストプラクティス\n    print(f\"\\n=== 注意事項とベストプラクティス ===\")\n    print(\"⚠️ **重要な注意点**\")\n    print(\"1. ベースモデルとLoRAアダプターは必ずセットで管理\")\n    print(\"2. 異なるベースモデルバージョンとの互換性に注意\")\n    print(\"3. 本格運用前には十分な評価とテストを実施\")\n    print(\"4. ライセンス要件（LLaMA3ベース）の確認\")\n    print()\n    print(\"💡 **ベストプラクティス**\")\n    print(\"1. **段階的学習**: 小→中→大規模データでの順次学習\")\n    print(\"2. **ハイパーパラメータ調整**: rank、alpha、学習率の最適化\")\n    print(\"3. **評価指標**: 定量的・定性的評価の組み合わせ\")\n    print(\"4. **継続学習**: 新しいデータでの追加学習\")\n    \n    # 今後の拡張案\n    print(f\"\\n=== 今後の拡張案 ===\")\n    print(\"🔄 **学習の拡張**\")\n    print(\"1. **大規模データ**: 10K-50Kサンプルでの本格学習\")\n    print(\"2. **マルチタスク**: 複数の日本語タスクでの同時学習\")\n    print(\"3. **ドメイン特化**: 医療、法律、技術分野への特化\")\n    print(\"4. **継続学習**: 新しい知識の継続的な追加\")\n    print()\n    print(\"🛠️ **技術的改善**\")\n    print(\"1. **QLoRA**: 4bit量子化での更なるメモリ削減\")\n    print(\"2. **AdaLoRA**: 適応的ランク調整\")\n    print(\"3. **DoRA**: 重み分解LoRA\")\n    print(\"4. **分散学習**: 複数GPU/ノードでの高速化\")\n    print()\n    print(\"📊 **評価の高度化**\")\n    print(\"1. **自動評価**: BLEU、ROUGE、BERTScoreなど\")\n    print(\"2. **人間評価**: 有用性、安全性、倫理性\")\n    print(\"3. **A/Bテスト**: 実用環境での比較評価\")\n    print(\"4. **ベンチマーク**: 標準的日本語NLPタスクでの評価\")\n    \n    return True\n\n# 実行\nsave_and_document_lora_model()\n\n# LoRA学習実験のサマリー保存\nif PEFT_AVAILABLE:\n    summary = {\n        'experiment_type': 'LoRA Fine-tuning',\n        'base_model': TrainingConfig.MODEL_NAME,\n        'dataset': TrainingConfig.DATASET_NAME,\n        'samples': TrainingConfig.MAX_SAMPLES,\n        'epochs': TrainingConfig.NUM_EPOCHS,\n        'batch_size': TrainingConfig.BATCH_SIZE,\n        'learning_rate': TrainingConfig.LEARNING_RATE,\n        'max_length': TrainingConfig.MAX_LENGTH,\n        'use_lora': True,\n        'use_mixed_precision': TrainingConfig.USE_MIXED_PRECISION,\n        'lora_config': {\n            'rank': 16,\n            'alpha': 32,\n            'dropout': 0.1\n        },\n        'training_history': training_history,\n        'generation_examples': generation_results_lora if 'generation_results_lora' in locals() else []\n    }\nelse:\n    summary = {\n        'experiment_type': 'Standard Fine-tuning',\n        'note': 'LoRA was not available for this experiment'\n    }\n\n# サマリーをJSONで保存\nif os.path.exists(TrainingConfig.OUTPUT_DIR):\n    with open(os.path.join(TrainingConfig.OUTPUT_DIR, 'lora_experiment_summary.json'), 'w', encoding='utf-8') as f:\n        json.dump(summary, f, ensure_ascii=False, indent=2)\n    print(f\"\\n📋 LoRA実験サマリーを保存しました: {TrainingConfig.OUTPUT_DIR}/lora_experiment_summary.json\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"🎉 LoRA対応 LLaDA日本語Alpaca学習実験完了！\")\nprint(\"=\" * 80)\n\nif PEFT_AVAILABLE:\n    print(\"\\n🏆 **実験成果**\")\n    print(\"✅ LoRAによるパラメータ効率的学習の実装に成功\")\n    print(\"✅ メモリ使用量の大幅削減を実現\")\n    print(\"✅ 日本語Alpaca指示応答データでの学習完了\")\n    print(\"✅ 学習済みLoRAアダプターの保存・活用方法を確立\")\n    \n    print(\"\\n🚀 **次のステップ**\")\n    print(\"1. より大規模なデータセットでの本格学習\")\n    print(\"2. 詳細な評価指標による性能測定\")\n    print(\"3. 実用アプリケーションでの動作確認\")\n    print(\"4. 継続的な改善とチューニング\")\n    \n    print(\"\\n💡 この実験により、限られたリソースでも効率的に\")\n    print(\"   大規模言語モデルを日本語タスクに適応させる\")\n    print(\"   基盤技術が確立されました。\")\nelse:\n    print(\"\\n⚠️ LoRAライブラリが利用できませんでしたが、\")\n    print(\"   基本的な学習パイプラインは構築されました。\")\n    print(\"   PEFT ライブラリをインストールして再実行することを推奨します。\")\n\nprint(\"\\nお疲れ様でした！ 🎊\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験完了・まとめ\n",
    "\n",
    "### 🎯 実験の成果\n",
    "\n",
    "1. **LLaDA拡散学習の実装**: 日本語データセットでの追加学習パイプラインを構築\n",
    "2. **マスキング戦略**: ランダムマスク比率での拡散学習を実装\n",
    "3. **学習監視**: 損失推移と検証スコアの追跡\n",
    "4. **生成評価**: 学習済みモデルでの日本語生成テスト\n",
    "\n",
    "### 📊 技術的要点\n",
    "\n",
    "- **データセット**: 複数の日本語コーパス対応（Wikipedia、OSCAR等）\n",
    "- **学習手法**: マスクド言語モデリングベースの拡散学習\n",
    "- **最適化**: AdamW + 線形ウォームアップスケジューラー\n",
    "- **評価**: 訓練/検証損失による性能追跡\n",
    "\n",
    "### 🚀 今後の発展方向\n",
    "\n",
    "1. **スケールアップ**: より大規模なデータセットと長時間学習\n",
    "2. **専門化**: 特定ドメイン（医療、法律、技術）への特化\n",
    "3. **効率化**: LoRAやQLoRAによるパラメータ効率的学習\n",
    "4. **評価強化**: 自動評価指標とヒューマン評価の導入\n",
    "\n",
    "### ⚠️ 制限事項\n",
    "\n",
    "- 本実験は教育目的での概念実証です\n",
    "- 実用レベルには大規模な計算リソースと時間が必要です\n",
    "- モデルの商用利用前にはライセンス確認が必要です\n",
    "\n",
    "**実験お疲れ様でした！** 日本語LLaDAの学習基盤が構築できました。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}