{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaDA日本語Alpaca学習 - 最小構成版\n",
    "\n",
    "## 概要\n",
    "LoRAを使用してLLaDAモデルを日本語Alpacaデータセットで効率的に学習します。\n",
    "\n",
    "## 特徴\n",
    "- ✅ LoRA（パラメータ効率的学習）\n",
    "- ✅ 混合精度学習\n",
    "- ✅ 日本語Alpaca指示応答データ\n",
    "- ✅ メモリ効率最適化\n",
    "\n",
    "## 必要環境\n",
    "- GPU: 8GB以上推奨\n",
    "- Python 3.8+\n",
    "- CUDA対応"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 1. 環境設定とライブラリインストール\nimport sys\n\n# Google Colab対応\nif 'google.colab' in sys.modules:\n    # 互換性のあるバージョンを明示的にインストール\n    print(\"📦 Installing compatible versions...\")\n    !pip install transformers==4.49.0 accelerate==0.34.2\n    !pip install datasets==2.18.0 peft==0.13.2\n    \n    # bitsandbyteとtritonの互換バージョンをインストール\n    try:\n        !pip install bitsandbytes==0.43.1 triton==2.1.0\n        print(\"✅ bitsandbytes and triton installed successfully\")\n        USE_QUANTIZATION = True\n    except:\n        print(\"⚠️ bitsandbytes installation failed, proceeding without quantization\")\n        USE_QUANTIZATION = False\nelse:\n    USE_QUANTIZATION = True\n\n# 必要ライブラリインポート\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import AdamW\nfrom torch.cuda.amp import autocast, GradScaler\n\nfrom transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom datasets import load_dataset\nimport numpy as np\nimport random\nfrom tqdm import tqdm\nimport os\n\n# bitsandbyteのインポートを安全に試行\ntry:\n    if USE_QUANTIZATION:\n        import bitsandbytes\n        print(\"✅ bitsandbytes imported successfully\")\nexcept ImportError:\n    print(\"⚠️ bitsandbytes not available, using standard precision\")\n    USE_QUANTIZATION = False\n\n# シード設定\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Device: {device}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"Quantization enabled: {USE_QUANTIZATION}\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 2. 設定クラス - 本格学習対応版\nclass BaseConfig:\n    \"\"\"基本設定\"\"\"\n    # モデル設定\n    MODEL_NAME = 'GSAI-ML/LLaDA-8B-Instruct'\n    MASK_ID = 126336\n    \n    # 保存設定\n    OUTPUT_DIR = './llada_japanese_lora'\n\nclass TestConfig(BaseConfig):\n    \"\"\"テスト用設定（動作確認）\"\"\"\n    # データ設定\n    DATASET_NAME = 'sample'\n    MAX_SAMPLES = 50\n    MAX_LENGTH = 128\n    \n    # 学習設定\n    BATCH_SIZE = 1\n    GRADIENT_ACCUMULATION = 2\n    LEARNING_RATE = 1e-4\n    NUM_EPOCHS = 1\n    WARMUP_RATIO = 0.1\n    \n    # LoRA設定\n    LORA_R = 8\n    LORA_ALPHA = 16\n    LORA_DROPOUT = 0.1\n\nclass MediumConfig(BaseConfig):\n    \"\"\"中規模学習設定（推奨開始点）\"\"\"\n    # データ設定\n    DATASET_NAME = 'fujiki/japanese_alpaca_data'\n    MAX_SAMPLES = 5000      # 5K samples\n    MAX_LENGTH = 512        # 長めの文脈\n    \n    # 学習設定\n    BATCH_SIZE = 1\n    GRADIENT_ACCUMULATION = 8   # 実効バッチサイズ8\n    LEARNING_RATE = 1e-4        # やや高めの学習率\n    NUM_EPOCHS = 3              # 十分な学習\n    WARMUP_RATIO = 0.1\n    \n    # LoRA設定\n    LORA_R = 32             # 表現力向上\n    LORA_ALPHA = 64         # バランス調整\n    LORA_DROPOUT = 0.1\n\nclass LargeConfig(BaseConfig):\n    \"\"\"大規模学習設定（高性能GPU用）\"\"\"\n    # データ設定\n    DATASET_NAME = 'fujiki/japanese_alpaca_data'\n    MAX_SAMPLES = 20000     # 20K samples\n    MAX_LENGTH = 768        # 長文対応\n    \n    # 学習設定\n    BATCH_SIZE = 2          # メモリ許可範囲で増加\n    GRADIENT_ACCUMULATION = 8   # 実効バッチサイズ16\n    LEARNING_RATE = 5e-5        # 安定した学習率\n    NUM_EPOCHS = 5              # 深い学習\n    WARMUP_RATIO = 0.1\n    \n    # LoRA設定\n    LORA_R = 64             # 高い表現力\n    LORA_ALPHA = 128        # 強いLoRA影響\n    LORA_DROPOUT = 0.05     # 低いドロップアウト\n\nclass ProductionConfig(BaseConfig):\n    \"\"\"本番品質設定（最高品質）\"\"\"\n    # データ設定\n    DATASET_NAME = 'fujiki/japanese_alpaca_data'\n    MAX_SAMPLES = 52000     # 全データ活用\n    MAX_LENGTH = 1024       # 最長文脈\n    \n    # 学習設定\n    BATCH_SIZE = 1          # 安全なバッチサイズ\n    GRADIENT_ACCUMULATION = 16  # 大きな実効バッチサイズ\n    LEARNING_RATE = 3e-5        # 慎重な学習率\n    NUM_EPOCHS = 8              # 徹底的な学習\n    WARMUP_RATIO = 0.1\n    \n    # LoRA設定\n    LORA_R = 128            # 最高の表現力\n    LORA_ALPHA = 256        # 最強のLoRA影響\n    LORA_DROPOUT = 0.05\n\n# 設定選択（ここを変更して学習レベルを選択）\nTRAINING_LEVEL = \"medium\"  # \"test\", \"medium\", \"large\", \"production\"\n\nconfig_map = {\n    \"test\": TestConfig(),\n    \"medium\": MediumConfig(),\n    \"large\": LargeConfig(),\n    \"production\": ProductionConfig()\n}\n\nconfig = config_map[TRAINING_LEVEL]\n\n# 設定情報表示\nprint(f\"=== {TRAINING_LEVEL.upper()} 学習設定 ===\")\nprint(f\"データセット: {config.DATASET_NAME}\")\nprint(f\"サンプル数: {config.MAX_SAMPLES:,}\")\nprint(f\"最大長: {config.MAX_LENGTH}\")\nprint(f\"バッチサイズ: {config.BATCH_SIZE}\")\nprint(f\"実効バッチサイズ: {config.BATCH_SIZE * config.GRADIENT_ACCUMULATION}\")\nprint(f\"学習率: {config.LEARNING_RATE}\")\nprint(f\"エポック数: {config.NUM_EPOCHS}\")\nprint(f\"LoRAランク: {config.LORA_R}\")\n\n# メモリ使用量予測\ndef estimate_memory_usage(config):\n    \"\"\"メモリ使用量の概算\"\"\"\n    base_memory = 16  # ベースモデルのメモリ（GB）\n    sequence_factor = config.MAX_LENGTH / 256  # シーケンス長による係数\n    batch_factor = config.BATCH_SIZE\n    lora_factor = config.LORA_R / 16  # LoRAランクによる係数\n    \n    estimated = base_memory * sequence_factor * batch_factor * (1 + lora_factor * 0.1)\n    return estimated\n\nestimated_memory = estimate_memory_usage(config)\nprint(f\"\\n💾 推定メモリ使用量: {estimated_memory:.1f}GB\")\n\nif estimated_memory > 40:\n    print(\"⚠️ 高メモリ使用量！より小さな設定を検討してください\")\nelif estimated_memory > 24:\n    print(\"⚠️ 高性能GPU推奨（A100など）\")\nelse:\n    print(\"✅ 一般的なGPUで実行可能\")\n\nprint(f\"\\n🎯 学習段階の推奨順序:\")\nprint(f\"1. test → 動作確認（数分）\")\nprint(f\"2. medium → 実用的学習（1-2時間）\")\nprint(f\"3. large → 高品質学習（4-8時間）\") \nprint(f\"4. production → 最高品質（12-24時間）\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 3. データセット準備\nclass AlpacaDataset(Dataset):\n    def __init__(self, texts, tokenizer):\n        self.texts = texts\n        self.tokenizer = tokenizer\n        self.vocab_size = len(tokenizer)\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        \n        # トークン化\n        encoding = self.tokenizer(\n            text,\n            max_length=config.MAX_LENGTH,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        input_ids = encoding['input_ids'].squeeze(0)\n        attention_mask = encoding['attention_mask'].squeeze(0)\n        \n        # 安全性チェック: すべてのトークンIDが語彙サイズ内にあることを確認\n        input_ids = torch.clamp(input_ids, 0, self.vocab_size - 1)\n        \n        # ランダムマスキング（安全版）\n        valid_positions = (attention_mask == 1).nonzero(as_tuple=True)[0]\n        if len(valid_positions) > 0:\n            mask_ratio = random.uniform(0.2, 0.7)\n            num_mask = max(1, int(len(valid_positions) * mask_ratio))\n            \n            # インデックス範囲の安全性チェック\n            valid_positions = valid_positions[valid_positions < len(input_ids)]\n            if len(valid_positions) > 0:\n                num_mask = min(num_mask, len(valid_positions))\n                mask_indices = torch.randperm(len(valid_positions))[:num_mask]\n                mask_positions = valid_positions[mask_indices]\n                \n                masked_input_ids = input_ids.clone()\n                # MASK_IDも語彙サイズ内であることを確認\n                safe_mask_id = min(config.MASK_ID, self.vocab_size - 1)\n                masked_input_ids[mask_positions] = safe_mask_id\n                \n                mask_bool = torch.zeros_like(input_ids, dtype=torch.bool)\n                mask_bool[mask_positions] = True\n            else:\n                masked_input_ids = input_ids.clone()\n                mask_bool = torch.zeros_like(input_ids, dtype=torch.bool)\n        else:\n            masked_input_ids = input_ids.clone()\n            mask_bool = torch.zeros_like(input_ids, dtype=torch.bool)\n        \n        return {\n            'input_ids': masked_input_ids,\n            'attention_mask': attention_mask,\n            'labels': input_ids,\n            'mask_positions': mask_bool\n        }\n\ndef load_alpaca_data():\n    \"\"\"Alpacaデータ読み込み\"\"\"\n    if config.DATASET_NAME == 'sample':\n        # サンプルデータ\n        sample_data = [\n            \"日本の首都について説明してください。\\n\\n日本の首都は東京です。\",\n            \"健康的な食事のアドバイスをしてください。\\n\\nバランスの取れた食事が重要です。\",\n            \"プログラミング初心者におすすめの言語を教えてください。\\n\\nPythonがおすすめです。\",\n        ]\n        return sample_data * (config.MAX_SAMPLES // len(sample_data))\n    else:\n        # 実データ読み込み\n        try:\n            dataset = load_dataset(config.DATASET_NAME, split=f'train[:{config.MAX_SAMPLES}]')\n            texts = []\n            for item in dataset:\n                instruction = item.get('instruction', '').strip()\n                input_text = item.get('input', '').strip()\n                output = item.get('output', '').strip()\n                \n                if len(instruction) > 5 and len(output) > 10:\n                    if input_text:\n                        prompt = f\"{instruction}\\n\\n入力: {input_text}\"\n                    else:\n                        prompt = instruction\n                    full_text = f\"{prompt}\\n\\n{output}\"\n                    texts.append(full_text)\n            return texts\n        except Exception as e:\n            print(f\"Data loading error: {e}, using sample data\")\n            config.DATASET_NAME = 'sample'  # フォールバックのために設定変更\n            return load_alpaca_data()\n\n# データ読み込み\ntexts = load_alpaca_data()\nprint(f\"Loaded {len(texts)} samples\")\nprint(f\"Sample: {texts[0][:100]}...\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 4. モデル準備\n# トークナイザー読み込み\ntokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME, trust_remote_code=True)\n\n# MASK_IDの安全性チェック\nvocab_size = len(tokenizer)\nprint(f\"Vocabulary size: {vocab_size}\")\nprint(f\"Original MASK_ID: {config.MASK_ID}\")\n\n# MASK_IDが語彙サイズを超えている場合の修正\nif config.MASK_ID >= vocab_size:\n    # 利用可能なマスクトークンを使用\n    if hasattr(tokenizer, 'mask_token_id') and tokenizer.mask_token_id is not None:\n        config.MASK_ID = tokenizer.mask_token_id\n    elif hasattr(tokenizer, 'unk_token_id') and tokenizer.unk_token_id is not None:\n        config.MASK_ID = tokenizer.unk_token_id\n    else:\n        # 最後の手段として語彙サイズ-1を使用\n        config.MASK_ID = vocab_size - 1\n    print(f\"⚠️ MASK_ID adjusted to: {config.MASK_ID}\")\nelse:\n    print(f\"✅ MASK_ID is valid: {config.MASK_ID}\")\n\n# ベースモデル読み込み（安全なdevice_map設定）\nprint(f\"Loading model: {config.MODEL_NAME}\")\ntry:\n    model = AutoModel.from_pretrained(\n        config.MODEL_NAME,\n        trust_remote_code=True,\n        torch_dtype=torch.bfloat16,\n        device_map={'': 0}  # より安全なdevice_map設定\n    )\n    print(\"✅ Model loaded successfully\")\nexcept Exception as e:\n    print(f\"⚠️ Loading with device_map failed: {e}\")\n    print(\"Trying alternative loading method...\")\n    model = AutoModel.from_pretrained(\n        config.MODEL_NAME,\n        trust_remote_code=True,\n        torch_dtype=torch.bfloat16\n    ).to(device)\n    print(\"✅ Model loaded with alternative method\")\n\n# LoRA設定適用（量子化対応）\nprint(f\"Setting up LoRA (quantization: {USE_QUANTIZATION})...\")\n\nif USE_QUANTIZATION:\n    # 量子化ありでのLoRA設定\n    lora_config = LoraConfig(\n        task_type=TaskType.CAUSAL_LM,\n        r=config.LORA_R,\n        lora_alpha=config.LORA_ALPHA,\n        lora_dropout=config.LORA_DROPOUT,\n        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n        bias=\"none\",\n        inference_mode=False,\n    )\nelse:\n    # 量子化なしでのLoRA設定\n    lora_config = LoraConfig(\n        task_type=TaskType.CAUSAL_LM,\n        r=config.LORA_R,\n        lora_alpha=config.LORA_ALPHA,\n        lora_dropout=config.LORA_DROPOUT,\n        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n        bias=\"none\",\n        inference_mode=False,\n    )\n\ntry:\n    model = get_peft_model(model, lora_config)\n    model.train()\n    print(\"✅ LoRA applied successfully\")\nexcept Exception as e:\n    print(f\"❌ LoRA setup failed: {e}\")\n    print(\"Please restart runtime and try again, or use a different configuration\")\n    raise e\n\n# パラメータ確認\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nall_params = sum(p.numel() for p in model.parameters())\nprint(f\"Trainable params: {trainable_params/1e6:.1f}M ({trainable_params/all_params*100:.2f}%)\")\n\n# データセット作成\ntrain_texts = texts[:int(len(texts)*0.9)]\nval_texts = texts[int(len(texts)*0.9):]\n\ntrain_dataset = AlpacaDataset(train_texts, tokenizer)\nval_dataset = AlpacaDataset(val_texts, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False)\n\nprint(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n\n# メモリ使用量確認\nif torch.cuda.is_available():\n    try:\n        memory_allocated = torch.cuda.memory_allocated() / 1e9\n        memory_reserved = torch.cuda.memory_reserved() / 1e9\n        print(f\"GPU Memory - Allocated: {memory_allocated:.1f}GB, Reserved: {memory_reserved:.1f}GB\")\n    except:\n        print(\"Could not check GPU memory usage\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 5. 学習実行\ndef compute_loss(model, batch):\n    \"\"\"安全な損失計算\"\"\"\n    try:\n        # 全てのテンソルを同じデバイスに移動\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        mask_positions = batch['mask_positions'].to(device)\n        \n        # デバッグ情報（最初のバッチでのみ表示）\n        if not hasattr(compute_loss, 'debug_printed'):\n            print(f\"Debug - Input shape: {input_ids.shape}\")\n            print(f\"Debug - Max token ID: {input_ids.max().item()}\")\n            print(f\"Debug - Min token ID: {input_ids.min().item()}\")\n            print(f\"Debug - Vocab size: {len(tokenizer)}\")\n            compute_loss.debug_printed = True\n        \n        # トークンIDの安全性チェック\n        vocab_size = len(tokenizer)\n        input_ids = torch.clamp(input_ids, 0, vocab_size - 1)\n        labels = torch.clamp(labels, 0, vocab_size - 1)\n        \n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        \n        # マスク位置での損失計算\n        masked_logits = logits[mask_positions]\n        masked_labels = labels[mask_positions]\n        \n        if len(masked_labels) == 0:\n            return torch.tensor(0.0, device=device, requires_grad=True)\n        \n        # 安全な損失計算\n        loss = F.cross_entropy(masked_logits, masked_labels, ignore_index=-100)\n        \n        # NaNチェック\n        if torch.isnan(loss) or torch.isinf(loss):\n            print(f\"Warning: Invalid loss detected: {loss.item()}\")\n            return torch.tensor(0.0, device=device, requires_grad=True)\n        \n        return loss\n        \n    except Exception as e:\n        print(f\"Error in compute_loss: {e}\")\n        return torch.tensor(0.0, device=device, requires_grad=True)\n\n# オプティマイザー設定\noptimizer = AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=0.01)\n\ntotal_steps = len(train_loader) * config.NUM_EPOCHS\nwarmup_steps = int(total_steps * config.WARMUP_RATIO)\nscheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n\n# 混合精度設定\nscaler = GradScaler()\n\n# 学習ループ\nprint(\"Starting training...\")\nmodel.train()\nglobal_step = 0\nbest_val_loss = float('inf')\n\ntry:\n    for epoch in range(config.NUM_EPOCHS):\n        print(f\"\\nEpoch {epoch+1}/{config.NUM_EPOCHS}\")\n        \n        epoch_losses = []\n        optimizer.zero_grad()\n        \n        for batch_idx, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n            try:\n                with autocast():\n                    loss = compute_loss(model, batch)\n                    loss = loss / config.GRADIENT_ACCUMULATION\n                \n                scaler.scale(loss).backward()\n                epoch_losses.append(loss.item() * config.GRADIENT_ACCUMULATION)\n                \n                if (batch_idx + 1) % config.GRADIENT_ACCUMULATION == 0:\n                    scaler.unscale_(optimizer)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                    scaler.step(optimizer)\n                    scaler.update()\n                    scheduler.step()\n                    optimizer.zero_grad()\n                    global_step += 1\n                    \n            except Exception as e:\n                print(f\"Error in training step {batch_idx}: {e}\")\n                optimizer.zero_grad()\n                continue\n        \n        # 検証\n        model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for batch in val_loader:\n                try:\n                    with autocast():\n                        loss = compute_loss(model, batch)\n                    val_losses.append(loss.item())\n                except Exception as e:\n                    print(f\"Error in validation: {e}\")\n                    continue\n        \n        if epoch_losses and val_losses:\n            train_loss = np.mean(epoch_losses)\n            val_loss = np.mean(val_losses)\n            \n            print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n            \n            # ベストモデル保存\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n                model.save_pretrained(config.OUTPUT_DIR)\n                print(f\"Best model saved: {val_loss:.4f}\")\n        \n        model.train()\n\nexcept Exception as e:\n    print(f\"Training error: {e}\")\n    import traceback\n    traceback.print_exc()\n\nprint(f\"\\nTraining completed. Best val loss: {best_val_loss:.4f}\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 6. テストと保存\ndef test_generation(model, tokenizer, prompts):\n    \"\"\"安全な生成テスト\"\"\"\n    model.eval()\n    \n    with torch.no_grad():\n        for prompt in prompts:\n            print(f\"\\nPrompt: {prompt}\")\n            \n            try:\n                # チャットテンプレート適用\n                messages = [{\"role\": \"user\", \"content\": prompt}]\n                formatted = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n                input_ids = tokenizer(formatted)['input_ids']\n                input_ids = torch.tensor(input_ids).to(device).unsqueeze(0)\n                \n                prompt_length = input_ids.shape[1]\n                gen_length = 64\n                \n                # 安全なマスクID取得\n                vocab_size = len(tokenizer)\n                safe_mask_id = min(config.MASK_ID, vocab_size - 1)\n                \n                # マスク生成\n                x = torch.full((1, prompt_length + gen_length), safe_mask_id, dtype=torch.long).to(device)\n                x[:, :prompt_length] = input_ids.clone()\n                \n                # 安全性チェック\n                x = torch.clamp(x, 0, vocab_size - 1)\n                \n                # 予測\n                outputs = model(x)\n                logits = outputs.logits\n                predictions = torch.argmax(logits, dim=-1)\n                \n                # 予測結果も安全性チェック\n                predictions = torch.clamp(predictions, 0, vocab_size - 1)\n                \n                # マスクを予測で置換\n                mask_positions = (x == safe_mask_id)\n                x[mask_positions] = predictions[mask_positions]\n                \n                # デコード\n                result = tokenizer.decode(x[0, prompt_length:], skip_special_tokens=True)\n                print(f\"Result: {result}\")\n                \n            except Exception as e:\n                print(f\"Generation error for prompt '{prompt}': {e}\")\n                print(\"Result: [Generation failed]\")\n\n# テスト実行\ntest_prompts = [\n    \"日本の首都について説明してください。\",\n    \"健康的な食事のアドバイスをしてください。\",\n    \"プログラミング初心者におすすめの言語を教えてください。\"\n]\n\nprint(\"=== Generation Test ===\")\ntest_generation(model, tokenizer, test_prompts)\n\n# 最終保存\ntry:\n    model.save_pretrained(config.OUTPUT_DIR)\n    print(f\"\\nModel saved to: {config.OUTPUT_DIR}\")\nexcept Exception as e:\n    print(f\"Error saving model: {e}\")\n\n# メモリ使用量確認\nif torch.cuda.is_available():\n    try:\n        memory_used = torch.cuda.max_memory_allocated() / 1e9\n        print(f\"Max GPU memory used: {memory_used:.1f}GB\")\n        \n        # メモリクリーンアップ\n        torch.cuda.empty_cache()\n    except Exception as e:\n        print(f\"Error checking memory: {e}\")\n\nprint(\"\\n=== Training Complete ===\")\nprint(\"If CUDA errors occurred, try:\")\nprint(\"1. Restart the runtime\")\nprint(\"2. Set config.DATASET_NAME = 'sample' for testing\")\nprint(\"3. Reduce config.MAX_SAMPLES or config.MAX_LENGTH\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}