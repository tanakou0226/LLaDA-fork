{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaDAæ—¥æœ¬èªAlpacaå­¦ç¿’ - æœ€å°æ§‹æˆç‰ˆ\n",
    "\n",
    "## æ¦‚è¦\n",
    "LoRAã‚’ä½¿ç”¨ã—ã¦LLaDAãƒ¢ãƒ‡ãƒ«ã‚’æ—¥æœ¬èªAlpacaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§åŠ¹ç‡çš„ã«å­¦ç¿’ã—ã¾ã™ã€‚\n",
    "\n",
    "## ç‰¹å¾´\n",
    "- âœ… LoRAï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡çš„å­¦ç¿’ï¼‰\n",
    "- âœ… æ··åˆç²¾åº¦å­¦ç¿’\n",
    "- âœ… æ—¥æœ¬èªAlpacaæŒ‡ç¤ºå¿œç­”ãƒ‡ãƒ¼ã‚¿\n",
    "- âœ… ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æœ€é©åŒ–\n",
    "\n",
    "## å¿…è¦ç’°å¢ƒ\n",
    "- GPU: 8GBä»¥ä¸Šæ¨å¥¨\n",
    "- Python 3.8+\n",
    "- CUDAå¯¾å¿œ"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 1. ç’°å¢ƒè¨­å®šã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\nimport sys\n\n# Google Colabå¯¾å¿œ\nif 'google.colab' in sys.modules:\n    # äº’æ›æ€§ã®ã‚ã‚‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’æ˜ç¤ºçš„ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n    print(\"ğŸ“¦ Installing compatible versions...\")\n    !pip install transformers==4.49.0 accelerate==0.34.2\n    !pip install datasets==2.18.0 peft==0.13.2\n    \n    # bitsandbyteã¨tritonã®äº’æ›ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n    try:\n        !pip install bitsandbytes==0.43.1 triton==2.1.0\n        print(\"âœ… bitsandbytes and triton installed successfully\")\n        USE_QUANTIZATION = True\n    except:\n        print(\"âš ï¸ bitsandbytes installation failed, proceeding without quantization\")\n        USE_QUANTIZATION = False\nelse:\n    USE_QUANTIZATION = True\n\n# å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆ\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import AdamW\nfrom torch.cuda.amp import autocast, GradScaler\n\nfrom transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom datasets import load_dataset\nimport numpy as np\nimport random\nfrom tqdm import tqdm\nimport os\n\n# bitsandbyteã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’å®‰å…¨ã«è©¦è¡Œ\ntry:\n    if USE_QUANTIZATION:\n        import bitsandbytes\n        print(\"âœ… bitsandbytes imported successfully\")\nexcept ImportError:\n    print(\"âš ï¸ bitsandbytes not available, using standard precision\")\n    USE_QUANTIZATION = False\n\n# ã‚·ãƒ¼ãƒ‰è¨­å®š\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Device: {device}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"Quantization enabled: {USE_QUANTIZATION}\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 2. è¨­å®šã‚¯ãƒ©ã‚¹ - æœ¬æ ¼å­¦ç¿’å¯¾å¿œç‰ˆ\nclass BaseConfig:\n    \"\"\"åŸºæœ¬è¨­å®š\"\"\"\n    # ãƒ¢ãƒ‡ãƒ«è¨­å®š\n    MODEL_NAME = 'GSAI-ML/LLaDA-8B-Instruct'\n    MASK_ID = 126336\n    \n    # ä¿å­˜è¨­å®š\n    OUTPUT_DIR = './llada_japanese_lora'\n\nclass TestConfig(BaseConfig):\n    \"\"\"ãƒ†ã‚¹ãƒˆç”¨è¨­å®šï¼ˆå‹•ä½œç¢ºèªï¼‰\"\"\"\n    # ãƒ‡ãƒ¼ã‚¿è¨­å®š\n    DATASET_NAME = 'sample'\n    MAX_SAMPLES = 50\n    MAX_LENGTH = 128\n    \n    # å­¦ç¿’è¨­å®š\n    BATCH_SIZE = 1\n    GRADIENT_ACCUMULATION = 2\n    LEARNING_RATE = 1e-4\n    NUM_EPOCHS = 1\n    WARMUP_RATIO = 0.1\n    \n    # LoRAè¨­å®š\n    LORA_R = 8\n    LORA_ALPHA = 16\n    LORA_DROPOUT = 0.1\n\nclass MediumConfig(BaseConfig):\n    \"\"\"ä¸­è¦æ¨¡å­¦ç¿’è¨­å®šï¼ˆæ¨å¥¨é–‹å§‹ç‚¹ï¼‰\"\"\"\n    # ãƒ‡ãƒ¼ã‚¿è¨­å®š\n    DATASET_NAME = 'fujiki/japanese_alpaca_data'\n    MAX_SAMPLES = 5000      # 5K samples\n    MAX_LENGTH = 512        # é•·ã‚ã®æ–‡è„ˆ\n    \n    # å­¦ç¿’è¨­å®š\n    BATCH_SIZE = 1\n    GRADIENT_ACCUMULATION = 8   # å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º8\n    LEARNING_RATE = 1e-4        # ã‚„ã‚„é«˜ã‚ã®å­¦ç¿’ç‡\n    NUM_EPOCHS = 3              # ååˆ†ãªå­¦ç¿’\n    WARMUP_RATIO = 0.1\n    \n    # LoRAè¨­å®š\n    LORA_R = 32             # è¡¨ç¾åŠ›å‘ä¸Š\n    LORA_ALPHA = 64         # ãƒãƒ©ãƒ³ã‚¹èª¿æ•´\n    LORA_DROPOUT = 0.1\n\nclass LargeConfig(BaseConfig):\n    \"\"\"å¤§è¦æ¨¡å­¦ç¿’è¨­å®šï¼ˆé«˜æ€§èƒ½GPUç”¨ï¼‰\"\"\"\n    # ãƒ‡ãƒ¼ã‚¿è¨­å®š\n    DATASET_NAME = 'fujiki/japanese_alpaca_data'\n    MAX_SAMPLES = 20000     # 20K samples\n    MAX_LENGTH = 768        # é•·æ–‡å¯¾å¿œ\n    \n    # å­¦ç¿’è¨­å®š\n    BATCH_SIZE = 2          # ãƒ¡ãƒ¢ãƒªè¨±å¯ç¯„å›²ã§å¢—åŠ \n    GRADIENT_ACCUMULATION = 8   # å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º16\n    LEARNING_RATE = 5e-5        # å®‰å®šã—ãŸå­¦ç¿’ç‡\n    NUM_EPOCHS = 5              # æ·±ã„å­¦ç¿’\n    WARMUP_RATIO = 0.1\n    \n    # LoRAè¨­å®š\n    LORA_R = 64             # é«˜ã„è¡¨ç¾åŠ›\n    LORA_ALPHA = 128        # å¼·ã„LoRAå½±éŸ¿\n    LORA_DROPOUT = 0.05     # ä½ã„ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ\n\nclass ProductionConfig(BaseConfig):\n    \"\"\"æœ¬ç•ªå“è³ªè¨­å®šï¼ˆæœ€é«˜å“è³ªï¼‰\"\"\"\n    # ãƒ‡ãƒ¼ã‚¿è¨­å®š\n    DATASET_NAME = 'fujiki/japanese_alpaca_data'\n    MAX_SAMPLES = 52000     # å…¨ãƒ‡ãƒ¼ã‚¿æ´»ç”¨\n    MAX_LENGTH = 1024       # æœ€é•·æ–‡è„ˆ\n    \n    # å­¦ç¿’è¨­å®š\n    BATCH_SIZE = 1          # å®‰å…¨ãªãƒãƒƒãƒã‚µã‚¤ã‚º\n    GRADIENT_ACCUMULATION = 16  # å¤§ããªå®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º\n    LEARNING_RATE = 3e-5        # æ…é‡ãªå­¦ç¿’ç‡\n    NUM_EPOCHS = 8              # å¾¹åº•çš„ãªå­¦ç¿’\n    WARMUP_RATIO = 0.1\n    \n    # LoRAè¨­å®š\n    LORA_R = 128            # æœ€é«˜ã®è¡¨ç¾åŠ›\n    LORA_ALPHA = 256        # æœ€å¼·ã®LoRAå½±éŸ¿\n    LORA_DROPOUT = 0.05\n\n# è¨­å®šé¸æŠï¼ˆã“ã“ã‚’å¤‰æ›´ã—ã¦å­¦ç¿’ãƒ¬ãƒ™ãƒ«ã‚’é¸æŠï¼‰\nTRAINING_LEVEL = \"medium\"  # \"test\", \"medium\", \"large\", \"production\"\n\nconfig_map = {\n    \"test\": TestConfig(),\n    \"medium\": MediumConfig(),\n    \"large\": LargeConfig(),\n    \"production\": ProductionConfig()\n}\n\nconfig = config_map[TRAINING_LEVEL]\n\n# è¨­å®šæƒ…å ±è¡¨ç¤º\nprint(f\"=== {TRAINING_LEVEL.upper()} å­¦ç¿’è¨­å®š ===\")\nprint(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {config.DATASET_NAME}\")\nprint(f\"ã‚µãƒ³ãƒ—ãƒ«æ•°: {config.MAX_SAMPLES:,}\")\nprint(f\"æœ€å¤§é•·: {config.MAX_LENGTH}\")\nprint(f\"ãƒãƒƒãƒã‚µã‚¤ã‚º: {config.BATCH_SIZE}\")\nprint(f\"å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º: {config.BATCH_SIZE * config.GRADIENT_ACCUMULATION}\")\nprint(f\"å­¦ç¿’ç‡: {config.LEARNING_RATE}\")\nprint(f\"ã‚¨ãƒãƒƒã‚¯æ•°: {config.NUM_EPOCHS}\")\nprint(f\"LoRAãƒ©ãƒ³ã‚¯: {config.LORA_R}\")\n\n# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡äºˆæ¸¬\ndef estimate_memory_usage(config):\n    \"\"\"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¦‚ç®—\"\"\"\n    base_memory = 16  # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ãƒ¢ãƒªï¼ˆGBï¼‰\n    sequence_factor = config.MAX_LENGTH / 256  # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã«ã‚ˆã‚‹ä¿‚æ•°\n    batch_factor = config.BATCH_SIZE\n    lora_factor = config.LORA_R / 16  # LoRAãƒ©ãƒ³ã‚¯ã«ã‚ˆã‚‹ä¿‚æ•°\n    \n    estimated = base_memory * sequence_factor * batch_factor * (1 + lora_factor * 0.1)\n    return estimated\n\nestimated_memory = estimate_memory_usage(config)\nprint(f\"\\nğŸ’¾ æ¨å®šãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {estimated_memory:.1f}GB\")\n\nif estimated_memory > 40:\n    print(\"âš ï¸ é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ã‚ˆã‚Šå°ã•ãªè¨­å®šã‚’æ¤œè¨ã—ã¦ãã ã•ã„\")\nelif estimated_memory > 24:\n    print(\"âš ï¸ é«˜æ€§èƒ½GPUæ¨å¥¨ï¼ˆA100ãªã©ï¼‰\")\nelse:\n    print(\"âœ… ä¸€èˆ¬çš„ãªGPUã§å®Ÿè¡Œå¯èƒ½\")\n\nprint(f\"\\nğŸ¯ å­¦ç¿’æ®µéšã®æ¨å¥¨é †åº:\")\nprint(f\"1. test â†’ å‹•ä½œç¢ºèªï¼ˆæ•°åˆ†ï¼‰\")\nprint(f\"2. medium â†’ å®Ÿç”¨çš„å­¦ç¿’ï¼ˆ1-2æ™‚é–“ï¼‰\")\nprint(f\"3. large â†’ é«˜å“è³ªå­¦ç¿’ï¼ˆ4-8æ™‚é–“ï¼‰\") \nprint(f\"4. production â†’ æœ€é«˜å“è³ªï¼ˆ12-24æ™‚é–“ï¼‰\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™\nclass AlpacaDataset(Dataset):\n    def __init__(self, texts, tokenizer):\n        self.texts = texts\n        self.tokenizer = tokenizer\n        self.vocab_size = len(tokenizer)\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        \n        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n        encoding = self.tokenizer(\n            text,\n            max_length=config.MAX_LENGTH,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        input_ids = encoding['input_ids'].squeeze(0)\n        attention_mask = encoding['attention_mask'].squeeze(0)\n        \n        # å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯: ã™ã¹ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³IDãŒèªå½™ã‚µã‚¤ã‚ºå†…ã«ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª\n        input_ids = torch.clamp(input_ids, 0, self.vocab_size - 1)\n        \n        # ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¹ã‚­ãƒ³ã‚°ï¼ˆå®‰å…¨ç‰ˆï¼‰\n        valid_positions = (attention_mask == 1).nonzero(as_tuple=True)[0]\n        if len(valid_positions) > 0:\n            mask_ratio = random.uniform(0.2, 0.7)\n            num_mask = max(1, int(len(valid_positions) * mask_ratio))\n            \n            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²ã®å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯\n            valid_positions = valid_positions[valid_positions < len(input_ids)]\n            if len(valid_positions) > 0:\n                num_mask = min(num_mask, len(valid_positions))\n                mask_indices = torch.randperm(len(valid_positions))[:num_mask]\n                mask_positions = valid_positions[mask_indices]\n                \n                masked_input_ids = input_ids.clone()\n                # MASK_IDã‚‚èªå½™ã‚µã‚¤ã‚ºå†…ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª\n                safe_mask_id = min(config.MASK_ID, self.vocab_size - 1)\n                masked_input_ids[mask_positions] = safe_mask_id\n                \n                mask_bool = torch.zeros_like(input_ids, dtype=torch.bool)\n                mask_bool[mask_positions] = True\n            else:\n                masked_input_ids = input_ids.clone()\n                mask_bool = torch.zeros_like(input_ids, dtype=torch.bool)\n        else:\n            masked_input_ids = input_ids.clone()\n            mask_bool = torch.zeros_like(input_ids, dtype=torch.bool)\n        \n        return {\n            'input_ids': masked_input_ids,\n            'attention_mask': attention_mask,\n            'labels': input_ids,\n            'mask_positions': mask_bool\n        }\n\ndef load_alpaca_data():\n    \"\"\"Alpacaãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\"\"\"\n    if config.DATASET_NAME == 'sample':\n        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿\n        sample_data = [\n            \"æ—¥æœ¬ã®é¦–éƒ½ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\\n\\næ—¥æœ¬ã®é¦–éƒ½ã¯æ±äº¬ã§ã™ã€‚\",\n            \"å¥åº·çš„ãªé£Ÿäº‹ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ã—ã¦ãã ã•ã„ã€‚\\n\\nãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸé£Ÿäº‹ãŒé‡è¦ã§ã™ã€‚\",\n            \"ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°åˆå¿ƒè€…ã«ãŠã™ã™ã‚ã®è¨€èªã‚’æ•™ãˆã¦ãã ã•ã„ã€‚\\n\\nPythonãŒãŠã™ã™ã‚ã§ã™ã€‚\",\n        ]\n        return sample_data * (config.MAX_SAMPLES // len(sample_data))\n    else:\n        # å®Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n        try:\n            dataset = load_dataset(config.DATASET_NAME, split=f'train[:{config.MAX_SAMPLES}]')\n            texts = []\n            for item in dataset:\n                instruction = item.get('instruction', '').strip()\n                input_text = item.get('input', '').strip()\n                output = item.get('output', '').strip()\n                \n                if len(instruction) > 5 and len(output) > 10:\n                    if input_text:\n                        prompt = f\"{instruction}\\n\\nå…¥åŠ›: {input_text}\"\n                    else:\n                        prompt = instruction\n                    full_text = f\"{prompt}\\n\\n{output}\"\n                    texts.append(full_text)\n            return texts\n        except Exception as e:\n            print(f\"Data loading error: {e}, using sample data\")\n            config.DATASET_NAME = 'sample'  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®ãŸã‚ã«è¨­å®šå¤‰æ›´\n            return load_alpaca_data()\n\n# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\ntexts = load_alpaca_data()\nprint(f\"Loaded {len(texts)} samples\")\nprint(f\"Sample: {texts[0][:100]}...\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 4. ãƒ¢ãƒ‡ãƒ«æº–å‚™\n# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿\ntokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME, trust_remote_code=True)\n\n# MASK_IDã®å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯\nvocab_size = len(tokenizer)\nprint(f\"Vocabulary size: {vocab_size}\")\nprint(f\"Original MASK_ID: {config.MASK_ID}\")\n\n# MASK_IDãŒèªå½™ã‚µã‚¤ã‚ºã‚’è¶…ãˆã¦ã„ã‚‹å ´åˆã®ä¿®æ­£\nif config.MASK_ID >= vocab_size:\n    # åˆ©ç”¨å¯èƒ½ãªãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨\n    if hasattr(tokenizer, 'mask_token_id') and tokenizer.mask_token_id is not None:\n        config.MASK_ID = tokenizer.mask_token_id\n    elif hasattr(tokenizer, 'unk_token_id') and tokenizer.unk_token_id is not None:\n        config.MASK_ID = tokenizer.unk_token_id\n    else:\n        # æœ€å¾Œã®æ‰‹æ®µã¨ã—ã¦èªå½™ã‚µã‚¤ã‚º-1ã‚’ä½¿ç”¨\n        config.MASK_ID = vocab_size - 1\n    print(f\"âš ï¸ MASK_ID adjusted to: {config.MASK_ID}\")\nelse:\n    print(f\"âœ… MASK_ID is valid: {config.MASK_ID}\")\n\n# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆå®‰å…¨ãªdevice_mapè¨­å®šï¼‰\nprint(f\"Loading model: {config.MODEL_NAME}\")\ntry:\n    model = AutoModel.from_pretrained(\n        config.MODEL_NAME,\n        trust_remote_code=True,\n        torch_dtype=torch.bfloat16,\n        device_map={'': 0}  # ã‚ˆã‚Šå®‰å…¨ãªdevice_mapè¨­å®š\n    )\n    print(\"âœ… Model loaded successfully\")\nexcept Exception as e:\n    print(f\"âš ï¸ Loading with device_map failed: {e}\")\n    print(\"Trying alternative loading method...\")\n    model = AutoModel.from_pretrained(\n        config.MODEL_NAME,\n        trust_remote_code=True,\n        torch_dtype=torch.bfloat16\n    ).to(device)\n    print(\"âœ… Model loaded with alternative method\")\n\n# LoRAè¨­å®šé©ç”¨ï¼ˆé‡å­åŒ–å¯¾å¿œï¼‰\nprint(f\"Setting up LoRA (quantization: {USE_QUANTIZATION})...\")\n\nif USE_QUANTIZATION:\n    # é‡å­åŒ–ã‚ã‚Šã§ã®LoRAè¨­å®š\n    lora_config = LoraConfig(\n        task_type=TaskType.CAUSAL_LM,\n        r=config.LORA_R,\n        lora_alpha=config.LORA_ALPHA,\n        lora_dropout=config.LORA_DROPOUT,\n        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n        bias=\"none\",\n        inference_mode=False,\n    )\nelse:\n    # é‡å­åŒ–ãªã—ã§ã®LoRAè¨­å®š\n    lora_config = LoraConfig(\n        task_type=TaskType.CAUSAL_LM,\n        r=config.LORA_R,\n        lora_alpha=config.LORA_ALPHA,\n        lora_dropout=config.LORA_DROPOUT,\n        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n        bias=\"none\",\n        inference_mode=False,\n    )\n\ntry:\n    model = get_peft_model(model, lora_config)\n    model.train()\n    print(\"âœ… LoRA applied successfully\")\nexcept Exception as e:\n    print(f\"âŒ LoRA setup failed: {e}\")\n    print(\"Please restart runtime and try again, or use a different configuration\")\n    raise e\n\n# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¢ºèª\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nall_params = sum(p.numel() for p in model.parameters())\nprint(f\"Trainable params: {trainable_params/1e6:.1f}M ({trainable_params/all_params*100:.2f}%)\")\n\n# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ\ntrain_texts = texts[:int(len(texts)*0.9)]\nval_texts = texts[int(len(texts)*0.9):]\n\ntrain_dataset = AlpacaDataset(train_texts, tokenizer)\nval_dataset = AlpacaDataset(val_texts, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False)\n\nprint(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n\n# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª\nif torch.cuda.is_available():\n    try:\n        memory_allocated = torch.cuda.memory_allocated() / 1e9\n        memory_reserved = torch.cuda.memory_reserved() / 1e9\n        print(f\"GPU Memory - Allocated: {memory_allocated:.1f}GB, Reserved: {memory_reserved:.1f}GB\")\n    except:\n        print(\"Could not check GPU memory usage\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 5. å­¦ç¿’å®Ÿè¡Œ\ndef compute_loss(model, batch):\n    \"\"\"å®‰å…¨ãªæå¤±è¨ˆç®—\"\"\"\n    try:\n        # å…¨ã¦ã®ãƒ†ãƒ³ã‚½ãƒ«ã‚’åŒã˜ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        mask_positions = batch['mask_positions'].to(device)\n        \n        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆæœ€åˆã®ãƒãƒƒãƒã§ã®ã¿è¡¨ç¤ºï¼‰\n        if not hasattr(compute_loss, 'debug_printed'):\n            print(f\"Debug - Input shape: {input_ids.shape}\")\n            print(f\"Debug - Max token ID: {input_ids.max().item()}\")\n            print(f\"Debug - Min token ID: {input_ids.min().item()}\")\n            print(f\"Debug - Vocab size: {len(tokenizer)}\")\n            compute_loss.debug_printed = True\n        \n        # ãƒˆãƒ¼ã‚¯ãƒ³IDã®å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯\n        vocab_size = len(tokenizer)\n        input_ids = torch.clamp(input_ids, 0, vocab_size - 1)\n        labels = torch.clamp(labels, 0, vocab_size - 1)\n        \n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        \n        # ãƒã‚¹ã‚¯ä½ç½®ã§ã®æå¤±è¨ˆç®—\n        masked_logits = logits[mask_positions]\n        masked_labels = labels[mask_positions]\n        \n        if len(masked_labels) == 0:\n            return torch.tensor(0.0, device=device, requires_grad=True)\n        \n        # å®‰å…¨ãªæå¤±è¨ˆç®—\n        loss = F.cross_entropy(masked_logits, masked_labels, ignore_index=-100)\n        \n        # NaNãƒã‚§ãƒƒã‚¯\n        if torch.isnan(loss) or torch.isinf(loss):\n            print(f\"Warning: Invalid loss detected: {loss.item()}\")\n            return torch.tensor(0.0, device=device, requires_grad=True)\n        \n        return loss\n        \n    except Exception as e:\n        print(f\"Error in compute_loss: {e}\")\n        return torch.tensor(0.0, device=device, requires_grad=True)\n\n# ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼è¨­å®š\noptimizer = AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=0.01)\n\ntotal_steps = len(train_loader) * config.NUM_EPOCHS\nwarmup_steps = int(total_steps * config.WARMUP_RATIO)\nscheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n\n# æ··åˆç²¾åº¦è¨­å®š\nscaler = GradScaler()\n\n# å­¦ç¿’ãƒ«ãƒ¼ãƒ—\nprint(\"Starting training...\")\nmodel.train()\nglobal_step = 0\nbest_val_loss = float('inf')\n\ntry:\n    for epoch in range(config.NUM_EPOCHS):\n        print(f\"\\nEpoch {epoch+1}/{config.NUM_EPOCHS}\")\n        \n        epoch_losses = []\n        optimizer.zero_grad()\n        \n        for batch_idx, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n            try:\n                with autocast():\n                    loss = compute_loss(model, batch)\n                    loss = loss / config.GRADIENT_ACCUMULATION\n                \n                scaler.scale(loss).backward()\n                epoch_losses.append(loss.item() * config.GRADIENT_ACCUMULATION)\n                \n                if (batch_idx + 1) % config.GRADIENT_ACCUMULATION == 0:\n                    scaler.unscale_(optimizer)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                    scaler.step(optimizer)\n                    scaler.update()\n                    scheduler.step()\n                    optimizer.zero_grad()\n                    global_step += 1\n                    \n            except Exception as e:\n                print(f\"Error in training step {batch_idx}: {e}\")\n                optimizer.zero_grad()\n                continue\n        \n        # æ¤œè¨¼\n        model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for batch in val_loader:\n                try:\n                    with autocast():\n                        loss = compute_loss(model, batch)\n                    val_losses.append(loss.item())\n                except Exception as e:\n                    print(f\"Error in validation: {e}\")\n                    continue\n        \n        if epoch_losses and val_losses:\n            train_loss = np.mean(epoch_losses)\n            val_loss = np.mean(val_losses)\n            \n            print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n            \n            # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n                model.save_pretrained(config.OUTPUT_DIR)\n                print(f\"Best model saved: {val_loss:.4f}\")\n        \n        model.train()\n\nexcept Exception as e:\n    print(f\"Training error: {e}\")\n    import traceback\n    traceback.print_exc()\n\nprint(f\"\\nTraining completed. Best val loss: {best_val_loss:.4f}\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 6. ãƒ†ã‚¹ãƒˆã¨ä¿å­˜\ndef test_generation(model, tokenizer, prompts):\n    \"\"\"å®‰å…¨ãªç”Ÿæˆãƒ†ã‚¹ãƒˆ\"\"\"\n    model.eval()\n    \n    with torch.no_grad():\n        for prompt in prompts:\n            print(f\"\\nPrompt: {prompt}\")\n            \n            try:\n                # ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨\n                messages = [{\"role\": \"user\", \"content\": prompt}]\n                formatted = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n                input_ids = tokenizer(formatted)['input_ids']\n                input_ids = torch.tensor(input_ids).to(device).unsqueeze(0)\n                \n                prompt_length = input_ids.shape[1]\n                gen_length = 64\n                \n                # å®‰å…¨ãªãƒã‚¹ã‚¯IDå–å¾—\n                vocab_size = len(tokenizer)\n                safe_mask_id = min(config.MASK_ID, vocab_size - 1)\n                \n                # ãƒã‚¹ã‚¯ç”Ÿæˆ\n                x = torch.full((1, prompt_length + gen_length), safe_mask_id, dtype=torch.long).to(device)\n                x[:, :prompt_length] = input_ids.clone()\n                \n                # å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯\n                x = torch.clamp(x, 0, vocab_size - 1)\n                \n                # äºˆæ¸¬\n                outputs = model(x)\n                logits = outputs.logits\n                predictions = torch.argmax(logits, dim=-1)\n                \n                # äºˆæ¸¬çµæœã‚‚å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯\n                predictions = torch.clamp(predictions, 0, vocab_size - 1)\n                \n                # ãƒã‚¹ã‚¯ã‚’äºˆæ¸¬ã§ç½®æ›\n                mask_positions = (x == safe_mask_id)\n                x[mask_positions] = predictions[mask_positions]\n                \n                # ãƒ‡ã‚³ãƒ¼ãƒ‰\n                result = tokenizer.decode(x[0, prompt_length:], skip_special_tokens=True)\n                print(f\"Result: {result}\")\n                \n            except Exception as e:\n                print(f\"Generation error for prompt '{prompt}': {e}\")\n                print(\"Result: [Generation failed]\")\n\n# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ\ntest_prompts = [\n    \"æ—¥æœ¬ã®é¦–éƒ½ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\",\n    \"å¥åº·çš„ãªé£Ÿäº‹ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ã—ã¦ãã ã•ã„ã€‚\",\n    \"ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°åˆå¿ƒè€…ã«ãŠã™ã™ã‚ã®è¨€èªã‚’æ•™ãˆã¦ãã ã•ã„ã€‚\"\n]\n\nprint(\"=== Generation Test ===\")\ntest_generation(model, tokenizer, test_prompts)\n\n# æœ€çµ‚ä¿å­˜\ntry:\n    model.save_pretrained(config.OUTPUT_DIR)\n    print(f\"\\nModel saved to: {config.OUTPUT_DIR}\")\nexcept Exception as e:\n    print(f\"Error saving model: {e}\")\n\n# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª\nif torch.cuda.is_available():\n    try:\n        memory_used = torch.cuda.max_memory_allocated() / 1e9\n        print(f\"Max GPU memory used: {memory_used:.1f}GB\")\n        \n        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n        torch.cuda.empty_cache()\n    except Exception as e:\n        print(f\"Error checking memory: {e}\")\n\nprint(\"\\n=== Training Complete ===\")\nprint(\"If CUDA errors occurred, try:\")\nprint(\"1. Restart the runtime\")\nprint(\"2. Set config.DATASET_NAME = 'sample' for testing\")\nprint(\"3. Reduce config.MAX_SAMPLES or config.MAX_LENGTH\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}