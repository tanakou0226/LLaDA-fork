# CLAUDE.md

このファイルは、このリポジトリのコードを扱う際にClaude Code (claude.ai/code) にガイダンスを提供します。

## プロジェクト概要

LLaDA (Large Language Diffusion with mAsking) は、LLaMA3 8Bの性能に匹敵する8Bパラメータの拡散言語モデルです。自己回帰モデルとは異なり、LLaDAはマスクされた拡散モデリングを使用し、ノイズ除去ステップを通じてトークンを段階的に明らかにします。

### 主要なアーキテクチャ概念

- **Transformer Encoder**: 標準的なTransformerアーキテクチャを使用しますが、因果マスキングなし（双方向注意）
- **マスクトークン**: トークンID 126336を特別な[MASK]トークンとして使用
- **拡散プロセス**: テキスト生成は左から右への生成ではなく、反復的なデマスキングによって行われる
- **2つのモデル**: LLaDA-8B-Base（事前学習済み）とLLaDA-8B-Instruct（微調整済み）

## 主要コンポーネント

### 生成関数
- `generate.py`: 設定可能なサンプリングパラメータを持つメイン生成関数
- `chat.py`: マルチターン会話のためのインタラクティブチャットインターフェース  
- `app.py`: リアルタイム拡散可視化機能付きGradio Webデモ
- `get_log_likelihood.py`: 評価のための条件付き尤度推定

### 主要な生成パラメータ
- `steps`: ノイズ除去ステップ数（通常32-128）
- `gen_length`: 生成するテキストの長さ
- `block_length`: 半自己回帰生成用（≤ gen_length）
- `temperature`: サンプリング温度（決定論的な場合は0.0）
- `cfg_scale`: 分類器フリーガイダンススケール
- `remasking`: 再マスキング戦略（'low_confidence'または'random'）

### サンプリング戦略
1. **固定長**: 事前に決められた長さのテキストを生成
2. **半自己回帰**: 効率のためブロック単位で生成
3. **再マスキング**: 信頼度ベースまたはランダムなトークン選択

## 一般的な開発タスク

### 推論の実行
```bash
# LLaDA-8B-Instructとのインタラクティブチャット
python chat.py

# Gradio Webデモ（拡散可視化機能付き）
pip install gradio
python app.py

# カスタム生成スクリプト
python generate.py
```

### 依存関係のインストール
```bash
# コア依存関係（eval_llada.shから）
pip install transformers==4.49.0 lm_eval==0.4.8 accelerate==0.34.2
pip install antlr4-python3-runtime==4.11 math_verify sympy hf_xet
```

### 評価
```bash
# 必要な環境変数を設定
export HF_ALLOW_CODE_EVAL=1
export HF_DATASETS_TRUST_REMOTE_CODE=true

# 評価の実行（完全なコマンドはeval_llada.shを参照）
accelerate launch eval_llada.py --tasks <task_name> --model llada_dist --model_args model_path='GSAI-ML/LLaDA-8B-Base'
```

## モデルアーキテクチャの詳細

### 拡散 vs 自己回帰
- 自己回帰モデルは次のトークンを順次予測
- LLaDAはマスクされたすべてのトークンを同時に予測し、高信頼度の予測を選択的に明らかにする
- 学習目的は負の対数尤度の上界として機能

### 学習プロセス
- **事前学習**: 様々な比率（0-1）でのランダムマスキング、2.3Tトークンで学習
- **SFT**: 事前学習と似ているが、プロンプトトークンを保持（ユーザー入力のマスキングなし）
- **損失**: 逆マスキング確率で重み付けされたクロスエントロピー

### サンプリングプロセス
1. すべてのレスポンストークンをマスクした状態で開始
2. モデルがすべてのトークンを同時に予測
3. 最も信頼度の高い予測を選択してマスクを解除
4. 残りのトークンを再マスクして繰り返し
5. すべてのトークンが明らかになるまで継続

## 重要な定数

- `MASK_ID = 126336`: [MASK]トークンのトークンID
- `EOS_ID = 126081`: 終了トークンのトークンID（チャット履歴処理で使用）
- デフォルト生成パラメータ: `steps=128, gen_length=128, block_length=32`
- モデル精度: 効率のため`torch.bfloat16`を推奨

## 既知の問題と制限

1. **評価バグ**: lm-evaluation-harnessはInstructモデルに問題がある（EVAL.mdを参照）
2. **サンプリング効率**: 固定コンテキスト長とKV-cacheの欠如により自己回帰より遅い
3. **アイデンティティ応答**: 学習データのため「Who are you」と聞かれると「Bailing」と応答する場合がある

## 高度な機能

### Gradio Webデモ (`app.py`)
- **リアルタイム拡散可視化**: 各ノイズ除去ステップで明らかになるトークンを色分け表示
- **制約ベース生成**: 特定位置に特定単語を配置する機能（"position:word"形式）
- **マルチターン会話**: チャット履歴を維持した対話
- **詳細設定**: 生成長、ステップ数、温度、CFGスケール、ブロック長、再マスキング戦略
- **色分けシステム**:
  - 灰色: マスクされたトークン
  - 赤色: 低信頼度で新規生成されたトークン
  - オレンジ色: 中信頼度で新規生成されたトークン
  - 緑色: 高信頼度で新規生成されたトークン
  - 青色: 以前のステップで生成されたトークン

### 3つのサンプリング戦略（GUIDELINES.mdから）
1. **固定長サンプリング**: 事前定義された長さでテキスト生成
2. **半自己回帰（Origin）**: 可変長生成、未論文記載
3. **半自己回帰（Padding）**: パディング付きブロック生成

### モデル別推奨設定
- **LLaDA-Base**: すべてのサンプリング戦略で低信頼度再マスキングを使用
- **LLaDA-Instruct**: 
  - 短文生成: 固定長 + ランダム再マスキング
  - 長文生成（>512トークン）: 半自己回帰（Padding）+ 低信頼度再マスキング

## ファイル構造メモ

- ルートディレクトリにコア生成ロジック
- `visualization/`: サンプリングプロセス可視化用ツール
- `imgs/`: 比較チャートと出力例を含む
- `GUIDELINES.md`: 学習と推論の詳細な技術ガイドライン（3つのサンプリング戦略詳細）
- `EVAL.md`: 評価方法論と既知の問題