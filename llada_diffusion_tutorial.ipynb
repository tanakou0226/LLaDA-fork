{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaDA: 拡散言語モデル学習ノートブック\n",
    "\n",
    "このノートブックでは、LLaDA（Large Language Diffusion with mAsking）を使って拡散言語モデルの特徴と動作原理を学習します。\n",
    "\n",
    "## 📚 目次\n",
    "1. [拡散言語モデルとは？](#1-拡散言語モデルとは)\n",
    "2. [LLaDAの特徴](#2-lladaの特徴)\n",
    "3. [環境設定（Google Colab）](#3-環境設定google-colab)\n",
    "4. [基本的な生成実験](#4-基本的な生成実験)\n",
    "5. [拡散プロセスの可視化](#5-拡散プロセスの可視化)\n",
    "6. [パラメータの影響を調べる](#6-パラメータの影響を調べる)\n",
    "7. [自己回帰モデルとの比較](#7-自己回帰モデルとの比較)\n",
    "8. [まとめと発展的な実験](#8-まとめと発展的な実験)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 拡散言語モデルとは？\n",
    "\n",
    "### 従来の自己回帰言語モデル\n",
    "- **GPT**や**LLaMA**のような自己回帰モデルは、テキストを**左から右**へ順番に生成します\n",
    "- 各ステップで次のトークンを予測し、それを元に次の予測を行います\n",
    "\n",
    "```\n",
    "自己回帰: \"The\" → \"The cat\" → \"The cat sits\" → \"The cat sits on\"\n",
    "```\n",
    "\n",
    "### 拡散言語モデル\n",
    "- **拡散モデル**は画像生成（DALL-E、Stable Diffusionなど）で広く使われている技術です\n",
    "- **LLaDA**は拡散の概念をテキスト生成に応用した革新的なモデルです\n",
    "\n",
    "#### 拡散プロセスの特徴：\n",
    "1. **全体から部分へ**: 最初に全体の構造を決め、徐々に詳細を埋めていく\n",
    "2. **並列処理**: すべての位置を同時に考慮できる\n",
    "3. **反復改善**: 複数のステップを通じて品質を向上させる\n",
    "\n",
    "```\n",
    "拡散: \"[MASK] [MASK] [MASK] [MASK]\" → \"The [MASK] [MASK] on\" → \"The cat [MASK] on\" → \"The cat sits on\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LLaDAの特徴\n",
    "\n",
    "### 🔑 主要な概念\n",
    "\n",
    "#### マスクトークン [MASK]\n",
    "- トークンID: `126336`\n",
    "- 未確定の位置を表す特別なトークン\n",
    "- 生成過程で段階的に実際のトークンに置き換えられる\n",
    "\n",
    "#### 信頼度ベースの選択\n",
    "- モデルは各位置でのトークン予測に**信頼度スコア**を付ける\n",
    "- 高信頼度のトークンから順番に確定していく\n",
    "- 低信頼度のトークンは再マスクされ、次のステップで再予測\n",
    "\n",
    "#### 双方向注意\n",
    "- 自己回帰モデルと違い、**因果マスキングがない**\n",
    "- 各トークンが文章全体の文脈を参照できる\n",
    "- より一貫性のある文章生成が可能\n",
    "\n",
    "### 🏗️ アーキテクチャ\n",
    "- **ベースモデル**: Transformer Encoder（8Bパラメータ）\n",
    "- **学習データ**: 2.3Tトークン\n",
    "- **2つのバリアント**:\n",
    "  - `LLaDA-8B-Base`: 事前学習のみ\n",
    "  - `LLaDA-8B-Instruct`: 指示に従うよう微調整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 環境設定（Google Colab）\n",
    "\n",
    "Google Colabで実行する場合は、以下のセルを実行してください："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colabでの実行用\n",
    "import sys\n",
    "\n",
    "# Google Colabかどうかをチェック\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Google Colab環境を検出しました\")\n",
    "    \n",
    "    # 必要なライブラリをインストール\n",
    "    !pip install transformers==4.49.0 accelerate==0.34.2 torch numpy matplotlib ipywidgets\n",
    "    \n",
    "    # GPUが利用可能かチェック\n",
    "    import torch\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"ローカル環境で実行中です\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリをインポート\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from typing import List, Tuple, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 日本語フォント設定（Colab用）\n",
    "if IN_COLAB:\n",
    "    !apt-get -qq install fonts-noto-cjk\n",
    "    import matplotlib.font_manager as fm\n",
    "    plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "print(\"ライブラリのインポートが完了しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 基本的な生成実験\n",
    "\n",
    "まず、LLaDAの基本的な使い方を学びましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLaDAモデルとトークナイザーの読み込み\n",
    "print(\"LLaDAモデルを読み込み中...（数分かかります）\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"使用デバイス: {device}\")\n",
    "\n",
    "# モデルとトークナイザーの読み込み\n",
    "model_name = 'GSAI-ML/LLaDA-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True, \n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device).eval()\n",
    "\n",
    "print(\"モデルの読み込みが完了しました！\")\n",
    "print(f\"モデルサイズ: {sum(p.numel() for p in model.parameters()) / 1e9:.1f}B パラメータ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaDAの核となる関数を実装\n",
    "\n",
    "ここでは、LLaDAの拡散プロセスを理解するために、重要な関数を詳しく見ていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重要な定数\n",
    "MASK_ID = 126336  # [MASK]トークンのID\n",
    "\n",
    "def add_gumbel_noise(logits, temperature):\n",
    "    \"\"\"\n",
    "    Gumbelノイズを追加してサンプリングを行う\n",
    "    temperature=0: 決定論的（最も確率の高いトークンを選択）\n",
    "    temperature>0: ランダム性を導入（高い温度ほどランダム）\n",
    "    \"\"\"\n",
    "    if temperature == 0:\n",
    "        return logits\n",
    "    \n",
    "    # 高精度演算のためfloat64を使用\n",
    "    logits = logits.to(torch.float64)\n",
    "    noise = torch.rand_like(logits, dtype=torch.float64)\n",
    "    gumbel_noise = (-torch.log(noise)) ** temperature\n",
    "    return logits.exp() / gumbel_noise\n",
    "\n",
    "def get_num_transfer_tokens(mask_index, steps):\n",
    "    \"\"\"\n",
    "    各ステップで何個のトークンをマスク解除するかを計算\n",
    "    線形スケジュールに従って均等に分散\n",
    "    \"\"\"\n",
    "    mask_num = mask_index.sum(dim=1, keepdim=True)  # マスクされたトークンの総数\n",
    "    \n",
    "    base = mask_num // steps  # 基本的な数\n",
    "    remainder = mask_num % steps  # 余り\n",
    "    \n",
    "    # 各ステップでのトークン数を計算\n",
    "    num_transfer_tokens = torch.zeros(\n",
    "        mask_num.size(0), steps, \n",
    "        device=mask_index.device, \n",
    "        dtype=torch.int64\n",
    "    ) + base\n",
    "    \n",
    "    # 余りを最初のステップに分散\n",
    "    for i in range(mask_num.size(0)):\n",
    "        num_transfer_tokens[i, :remainder[i]] += 1\n",
    "    \n",
    "    return num_transfer_tokens\n",
    "\n",
    "print(\"コア関数の定義が完了しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 簡単な生成実験"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "@torch.no_grad()\ndef generate_llada(model, prompt, steps=32, gen_length=64, block_length=32, \n                   temperature=0.0, cfg_scale=0.0, remasking='low_confidence'):\n    \"\"\"\n    完全なLLaDA生成関数（Google Colab単体動作用）\n    元のgenerate.pyの完全実装\n    \"\"\"\n    # プロンプトをトークン化\n    if isinstance(prompt, str):\n        # Instructモデル用のチャットテンプレートを適用\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        formatted_prompt = tokenizer.apply_chat_template(\n            messages, add_generation_prompt=True, tokenize=False\n        )\n        input_ids = tokenizer(formatted_prompt)['input_ids']\n        input_ids = torch.tensor(input_ids).to(device).unsqueeze(0)\n    else:\n        input_ids = prompt\n    \n    prompt_length = input_ids.shape[1]\n    \n    # 応答部分を[MASK]で初期化\n    x = torch.full(\n        (1, prompt_length + gen_length), \n        MASK_ID, \n        dtype=torch.long\n    ).to(device)\n    x[:, :prompt_length] = input_ids.clone()\n    \n    # プロンプト部分のインデックス（CFG用）\n    prompt_index = (x != MASK_ID)\n    \n    # ブロック処理の設定（修正版：より柔軟な設定）\n    # gen_lengthがblock_lengthで割り切れない場合も対応\n    block_length = min(block_length, gen_length)\n    num_blocks = (gen_length + block_length - 1) // block_length  # 切り上げ除算\n    \n    # stepsがnum_blocksで割り切れない場合の対応\n    if steps % num_blocks != 0:\n        # 最も近い割り切れるsteps数に調整\n        adjusted_steps = ((steps + num_blocks - 1) // num_blocks) * num_blocks\n        print(f\"ステップ数を {steps} から {adjusted_steps} に調整しました（{num_blocks}ブロックに対応）\")\n        steps = adjusted_steps\n    \n    steps_per_block = steps // num_blocks\n    \n    # ステップごとの生成記録\n    generation_history = []\n    \n    print(f\"生成開始: {num_blocks}ブロック x {steps_per_block}ステップ = 計{steps}ステップ\")\n    print(f\"ブロック設定: gen_length={gen_length}, block_length={block_length}\")\n    \n    # 各ブロックを処理\n    for block_idx in range(num_blocks):\n        block_start = prompt_length + block_idx * block_length\n        block_end = min(prompt_length + (block_idx + 1) * block_length, x.shape[1])\n        \n        print(f\"\\\\nブロック {block_idx + 1}/{num_blocks} (位置 {block_start}-{block_end-1}):\")\n        \n        # 現在のブロックのマスクインデックス\n        block_mask_index = (x[:, block_start:block_end] == MASK_ID)\n        \n        # スキップ条件: ブロックにマスクがない場合\n        if not block_mask_index.any():\n            print(f\"  ブロック {block_idx + 1}: マスクなし、スキップ\")\n            continue\n            \n        # このブロックでのトークン転送数を計算\n        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps_per_block)\n        \n        # ブロック内の各ステップ\n        for step in range(steps_per_block):\n            # 現在のマスク位置を取得\n            mask_index = (x == MASK_ID)\n            \n            if not mask_index.any():\n                print(f\"  ステップ {step + 1}: 全トークン確定済み\")\n                break\n            \n            # 分類器フリーガイダンス (CFG)\n            if cfg_scale > 0.0:\n                # 無条件生成用：プロンプト部分もマスク\n                un_x = x.clone()\n                un_x[prompt_index] = MASK_ID\n                x_ = torch.cat([x, un_x], dim=0)\n                logits = model(x_).logits\n                logits, un_logits = torch.chunk(logits, 2, dim=0)\n                # CFGスケールを適用\n                logits = un_logits + (cfg_scale + 1) * (logits - un_logits)\n            else:\n                logits = model(x).logits\n            \n            # Gumbelノイズでサンプリング\n            logits_with_noise = add_gumbel_noise(logits, temperature)\n            x0 = torch.argmax(logits_with_noise, dim=-1)\n            \n            # 再マスキング戦略に基づく信頼度計算\n            if remasking == 'low_confidence':\n                p = F.softmax(logits, dim=-1)\n                x0_p = torch.squeeze(\n                    torch.gather(p, dim=-1, index=torch.unsqueeze(x0, -1)), -1\n                )\n            elif remasking == 'random':\n                x0_p = torch.rand((x0.shape[0], x0.shape[1]), device=x0.device)\n            else:\n                raise NotImplementedError(f\"Remasking strategy '{remasking}' not implemented\")\n            \n            # 現在のブロック以外の位置は除外\n            x0_p[:, block_end:] = -float('inf')\n            \n            # マスク位置のみの予測と信頼度\n            x0 = torch.where(mask_index, x0, x)\n            confidence = torch.where(mask_index, x0_p, -float('inf'))\n            \n            # 高信頼度のトークンを選択してマスク解除\n            transfer_index = torch.zeros_like(x0, dtype=torch.bool, device=device)\n            for j in range(confidence.shape[0]):\n                num_tokens_to_transfer = min(\n                    num_transfer_tokens[j, step].item(),\n                    mask_index[j].sum().item()  # 実際のマスク数を超えないよう制限\n                )\n                if num_tokens_to_transfer > 0:\n                    _, select_index = torch.topk(confidence[j], k=num_tokens_to_transfer)\n                    transfer_index[j, select_index] = True\n            \n            # 選択されたトークンを確定\n            x[transfer_index] = x0[transfer_index]\n            \n            # 進捗を記録\n            current_text = tokenizer.decode(\n                x[0, prompt_length:], skip_special_tokens=False\n            )\n            avg_conf = confidence[0, transfer_index[0]].mean().item() if transfer_index[0].any() else 0\n            \n            generation_history.append({\n                'block': block_idx,\n                'step': step,\n                'global_step': block_idx * steps_per_block + step,\n                'text': current_text,\n                'num_revealed': transfer_index[0].sum().item(),\n                'avg_confidence': avg_conf\n            })\n            \n            print(f\"  ステップ {step+1:2d}: {transfer_index[0].sum().item():2d}個確定 \"\n                  f\"(信頼度: {avg_conf:.3f})\")\n    \n    # 最終結果\n    final_text = tokenizer.decode(\n        x[0, prompt_length:], skip_special_tokens=True\n    )\n    \n    return final_text, generation_history\n\n# 簡略版も保持（学習用）\n@torch.no_grad()\ndef simple_generate(model, prompt, steps=32, gen_length=64, temperature=0.0):\n    \"\"\"\n    簡略化されたLLaDA生成関数（学習用）\n    \"\"\"\n    return generate_llada(\n        model, prompt, \n        steps=steps, gen_length=gen_length, block_length=gen_length,\n        temperature=temperature, cfg_scale=0.0, remasking='low_confidence'\n    )\n\n# テスト実行\ntest_prompt = \"What is the capital of Japan?\"\nprint(f\"\\\\n質問: {test_prompt}\")\nprint(\"=\" * 50)\n\nresult, history = simple_generate(\n    model, test_prompt, \n    steps=16, gen_length=32, temperature=0.0\n)\n\nprint(\"=\" * 50)\nprint(f\"最終回答: {result}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 拡散プロセスの可視化\n",
    "\n",
    "LLaDAの拡散プロセスがどのように動作するかを可視化してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def visualize_generation_process(history, title=\"LLaDA Generation Process\"):\n    \"\"\"\n    生成プロセスを段階的に表示\n    \"\"\"\n    print(f\"\\n{title}\")\n    print(\"=\" * 60)\n    \n    for entry in history:\n        step = entry['step']\n        text = entry['text']\n        num_revealed = entry['num_revealed']\n        avg_conf = entry['avg_confidence']\n        \n        # [MASK]を視覚的に表現\n        visual_text = text.replace('<|reserved_special_token_250|>', '[MASK]')\n        \n        print(f\"Step {step+1:2d} (+{num_revealed:2d} tokens, conf={avg_conf:.3f}):\")\n        print(f\"  {visual_text}\")\n        print()\n\ndef detect_token_changes(prev_tokens, curr_tokens):\n    \"\"\"\n    前のステップと現在のステップでのトークン変化を検出\n    \"\"\"\n    changes = []\n    \n    # トークン化して比較\n    if prev_tokens is None:\n        # 初回の場合、すべてが新規\n        for i, token in enumerate(curr_tokens):\n            if token != '[MASK]':\n                changes.append((i, 'new', token))\n    else:\n        # 前のステップと比較\n        for i, (prev_token, curr_token) in enumerate(zip(prev_tokens, curr_tokens)):\n            if prev_token != curr_token:\n                if prev_token == '[MASK]' and curr_token != '[MASK]':\n                    changes.append((i, 'revealed', curr_token))\n                elif prev_token != '[MASK]' and curr_token == '[MASK]':\n                    changes.append((i, 'masked', prev_token))\n                elif prev_token != '[MASK]' and curr_token != '[MASK]':\n                    changes.append((i, 'changed', curr_token))\n    \n    return changes\n\ndef get_confidence_color(confidence):\n    \"\"\"\n    信頼度に基づいて落ち着いた色を決定\n    \"\"\"\n    if confidence >= 0.8:\n        return '\\033[38;5;28m'  # 深い緑（高信頼度）\n    elif confidence >= 0.6:\n        return '\\033[38;5;94m'  # オリーブ（中信頼度）\n    elif confidence >= 0.4:\n        return '\\033[38;5;130m'  # ブラウン（低信頼度）\n    else:\n        return '\\033[38;5;240m'  # グレー（非常に低信頼度）\n\ndef visualize_generation_with_highlights(history, title=\"LLaDA Generation Process (Enhanced)\", show_details=True):\n    \"\"\"\n    新しく生成された単語をハイライト表示する拡張可視化関数（落ち着いた配色版）\n    \"\"\"\n    print(f\"\\n{title}\")\n    print(\"=\" * 70)\n    \n    # 色の説明\n    if show_details:\n        print(\"信頼度色分け:\")\n        print(\"  \\033[38;5;28m■\\033[0m 高信頼度 (≥0.8)  \\033[38;5;94m■\\033[0m 中信頼度 (≥0.6)  \\033[38;5;130m■\\033[0m 低信頼度 (≥0.4)  \\033[38;5;240m■\\033[0m 極低 (<0.4)\")\n        print(\"  強調: \\033[1m太字\\033[0m = 新規生成  \\033[2m薄字\\033[0m = 再マスク  \\033[4m下線\\033[0m = 変更\")\n        print()\n    \n    prev_tokens = None\n    \n    for i, entry in enumerate(history):\n        global_step = entry.get('global_step', entry['step'])\n        block = entry.get('block', 0)\n        step = entry['step']\n        text = entry['text']\n        num_revealed = entry['num_revealed']\n        avg_conf = entry['avg_confidence']\n        \n        # [MASK]を標準化\n        clean_text = text.replace('<|reserved_special_token_250|>', '[MASK]')\n        \n        # トークンに分割（簡易版）\n        curr_tokens = clean_text.split()\n        \n        # 変化を検出\n        changes = detect_token_changes(prev_tokens, curr_tokens)\n        \n        # ステップ情報を表示\n        block_info = f\"[Block {block+1}]\" if 'block' in entry else \"\"\n        print(f\"Step {global_step+1:2d} {block_info} (+{num_revealed:2d} tokens, conf={avg_conf:.3f}):\")\n        \n        # ハイライト付きテキストを構築\n        highlighted_text = []\n        change_map = {pos: (change_type, token) for pos, change_type, token in changes}\n        \n        for pos, token in enumerate(curr_tokens):\n            if pos in change_map:\n                change_type, _ = change_map[pos]\n                color = get_confidence_color(avg_conf)\n                \n                if change_type == 'revealed':\n                    # 新規生成: 太字 + 色\n                    highlighted_text.append(f\"{color}\\033[1m{token}\\033[0m\")\n                elif change_type == 'masked':\n                    # 再マスク: 薄字\n                    highlighted_text.append(f\"\\033[2m[MASK]\\033[0m\")\n                elif change_type == 'changed':\n                    # 変更: 下線 + 色\n                    highlighted_text.append(f\"{color}\\033[4m{token}\\033[0m\")\n                else:\n                    highlighted_text.append(f\"{color}{token}\\033[0m\")\n            else:\n                # 変化なし\n                if token == '[MASK]':\n                    highlighted_text.append(f\"\\033[38;5;245m{token}\\033[0m\")  # 薄いグレー\n                else:\n                    highlighted_text.append(token)\n        \n        # 表示\n        print(f\"  {' '.join(highlighted_text)}\")\n        \n        # 変化の詳細を表示\n        if show_details and changes:\n            change_summary = []\n            for pos, change_type, token in changes:\n                if change_type == 'revealed':\n                    change_summary.append(f\"pos{pos}:新規({token})\")\n                elif change_type == 'masked':\n                    change_summary.append(f\"pos{pos}:マスク\")\n                elif change_type == 'changed':\n                    change_summary.append(f\"pos{pos}:変更({token})\")\n            \n            if change_summary:\n                print(f\"    変化: {', '.join(change_summary)}\")\n        \n        print()\n        prev_tokens = curr_tokens\n\ndef visualize_step_by_step_animation(history, title=\"LLaDA Step-by-Step Animation\", delay=1.0):\n    \"\"\"\n    段階的なアニメーション風表示（Jupyter用）\n    \"\"\"\n    import time\n    try:\n        from IPython.display import display, clear_output\n        use_jupyter = True\n    except ImportError:\n        use_jupyter = False\n    \n    print(f\"{title}\")\n    print(\"=\" * 50)\n    \n    for i, entry in enumerate(history):\n        if use_jupyter:\n            clear_output(wait=True)\n        \n        # 現在のステップまでを表示\n        print(f\"{title} - Step {i+1}/{len(history)}\")\n        print(\"=\" * 50)\n        \n        visualize_generation_with_highlights(history[:i+1], \n                                           title=f\"Progress: {i+1}/{len(history)} steps\", \n                                           show_details=False)\n        \n        if i < len(history) - 1:  # 最後でなければ待機\n            time.sleep(delay)\n\ndef visualize_block_progression(history):\n    \"\"\"\n    ブロック別の生成進捗を可視化（修正版）\n    \"\"\"\n    if not history:\n        print(\"履歴データがありません\")\n        return\n        \n    print(\"\\nブロック別生成進捗:\")\n    print(\"=\" * 50)\n    \n    current_block = -1\n    for entry in history:\n        block = entry.get('block', 0)\n        if block != current_block:\n            current_block = block\n            print(f\"\\nBlock {current_block + 1}:\")\n            \n        step = entry['step']\n        text = entry['text']\n        num_revealed = entry['num_revealed']\n        avg_conf = entry['avg_confidence']\n        \n        # [MASK]を視覚的に表現\n        visual_text = text.replace('<|reserved_special_token_250|>', '[MASK]')\n        \n        print(f\"  Step {step+1:2d}: +{num_revealed:2d} tokens (conf={avg_conf:.3f})\")\n        print(f\"    状態: {visual_text[:80]}{'...' if len(visual_text) > 80 else ''}\")\n\ndef compare_generation_steps(history_list, titles=None, highlight_differences=True):\n    \"\"\"\n    複数の生成履歴を並べて比較表示\n    \"\"\"\n    if titles is None:\n        titles = [f\"Generation {i+1}\" for i in range(len(history_list))]\n    \n    print(\"生成プロセス比較\")\n    print(\"=\" * 80)\n    \n    max_steps = max(len(history) for history in history_list)\n    \n    for step in range(max_steps):\n        print(f\"\\nStep {step+1}:\")\n        print(\"-\" * 60)\n        \n        for i, (history, title) in enumerate(zip(history_list, titles)):\n            if step < len(history):\n                entry = history[step]\n                text = entry['text'].replace('<|reserved_special_token_250|>', '[MASK]')\n                conf = entry['avg_confidence']\n                revealed = entry['num_revealed']\n                \n                print(f\"  {title}: (+{revealed:2d}, conf={conf:.3f})\")\n                print(f\"    {text}\")\n            else:\n                print(f\"  {title}: [完了]\")\n        print()\n\n# 前回の結果を拡張可視化\nprint(\"拡張可視化機能のデモ:\")\nif 'history' in locals():\n    visualize_generation_with_highlights(history, \"日本の首都に関する質問の生成プロセス（拡張版）\")\nelse:\n    print(\"履歴データが見つかりません。先に生成実験を実行してください。\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### より複雑な質問での実験"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# より複雑な数学の問題を試してみる\nmath_prompt = \"Solve this step by step: If a train travels 120 km in 1.5 hours, what is its average speed in km/h?\"\n\nprint(f\"数学問題: {math_prompt}\")\nprint(\"=\" * 70)\n\nmath_result, math_history = simple_generate(\n    model, math_prompt,\n    steps=24, gen_length=80, temperature=0.0\n)\n\nprint(\"=\" * 70)\nprint(f\"回答: {math_result}\")\n\n# 新機能：拡張可視化でプロセスを詳細に表示\nprint(\"\\n\" + \"=\" * 40)\nprint(\"新機能デモ: ハイライト付き可視化\")\nprint(\"=\" * 40)\n\nvisualize_generation_with_highlights(math_history[-8:], \"数学問題の詳細生成プロセス（最後の8ステップ）\")\n\n# アニメーション風表示のデモ\nprint(\"\\n\" + \"=\" * 40)\nprint(\"新機能デモ: 段階的アニメーション表示\")\nprint(\"=\" * 40)\nprint(\"注意: Jupyter環境では画面がクリアされてアニメーションのように表示されます\")\n\n# 最後の5ステップをアニメーション風に表示\nvisualize_step_by_step_animation(math_history[-5:], \n                                title=\"数学問題生成の最終段階\", \n                                delay=0.5)\n\n# 従来の表示と比較\nprint(\"\\n\" + \"=\" * 40)\nprint(\"従来表示 vs 拡張表示の比較\")\nprint(\"=\" * 40)\n\nprint(\"従来の表示:\")\nvisualize_generation_process(math_history[-3:], \"従来の可視化\")\n\nprint(\"\\n拡張表示（ハイライト付き）:\")\nvisualize_generation_with_highlights(math_history[-3:], \"拡張可視化\", show_details=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. パラメータの影響を調べる\n",
    "\n",
    "LLaDAの異なるパラメータが生成結果にどのような影響を与えるかを実験してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 ステップ数の影響"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def compare_different_steps(prompt, step_counts=[8, 16, 32]):\n    \"\"\"\n    異なるステップ数での生成結果を比較（修正版）\n    \"\"\"\n    print(f\"プロンプト: {prompt}\")\n    print(\"=\" * 60)\n    \n    results = {}\n    \n    for steps in step_counts:\n        print(f\"\\\\n📊 {steps}ステップでの生成:\")\n        start_time = time.time()\n        \n        # ブロック設定を調整：ステップ数に応じて適切なブロックサイズを選択\n        if steps <= 16:\n            block_length = 48  # 1ブロックで処理\n        else:\n            block_length = 24  # 2ブロックで処理\n        \n        result, history = generate_llada(\n            model, prompt,\n            steps=steps, gen_length=48, block_length=block_length,\n            temperature=0.0, cfg_scale=0.0, remasking='low_confidence'\n        )\n        \n        end_time = time.time()\n        \n        results[steps] = {\n            'text': result,\n            'time': end_time - start_time,\n            'history': history\n        }\n        \n        print(f\"結果: {result}\")\n        print(f\"時間: {end_time - start_time:.2f}秒\")\n    \n    return results\n\ndef compare_advanced_settings(prompt):\n    \"\"\"\n    高度な設定での比較実験（修正版）\n    \"\"\"\n    print(f\"\\\\n🔬 高度な設定比較\")\n    print(f\"プロンプト: {prompt}\")\n    print(\"=\" * 60)\n    \n    settings = [\n        {\n            \"name\": \"標準設定\",\n            \"steps\": 16, \"block_length\": 64, \"temperature\": 0.0, \n            \"cfg_scale\": 0.0, \"remasking\": \"low_confidence\"\n        },\n        {\n            \"name\": \"高品質設定\",\n            \"steps\": 32, \"block_length\": 32, \"temperature\": 0.0, \n            \"cfg_scale\": 0.0, \"remasking\": \"low_confidence\"\n        },\n        {\n            \"name\": \"創造的設定\",\n            \"steps\": 24, \"block_length\": 32, \"temperature\": 0.3, \n            \"cfg_scale\": 0.0, \"remasking\": \"low_confidence\"\n        },\n        {\n            \"name\": \"CFG強化設定\",\n            \"steps\": 20, \"block_length\": 32, \"temperature\": 0.0, \n            \"cfg_scale\": 1.0, \"remasking\": \"low_confidence\"\n        },\n        {\n            \"name\": \"ランダム再マスク\",\n            \"steps\": 16, \"block_length\": 64, \"temperature\": 0.0, \n            \"cfg_scale\": 0.0, \"remasking\": \"random\"\n        }\n    ]\n    \n    results = []\n    \n    for setting in settings:\n        print(f\"\\\\n🧪 {setting['name']}:\")\n        print(f\"  設定: steps={setting['steps']}, block={setting['block_length']}, \"\n              f\"temp={setting['temperature']}, cfg={setting['cfg_scale']}, \"\n              f\"remasking={setting['remasking']}\")\n        \n        start_time = time.time()\n        result, history = generate_llada(\n            model, prompt,\n            steps=setting['steps'],\n            gen_length=64,\n            block_length=setting['block_length'],\n            temperature=setting['temperature'],\n            cfg_scale=setting['cfg_scale'],\n            remasking=setting['remasking']\n        )\n        generation_time = time.time() - start_time\n        \n        avg_confidence = np.mean([h['avg_confidence'] for h in history if h['avg_confidence'] > 0])\n        \n        results.append({\n            'name': setting['name'],\n            'result': result,\n            'time': generation_time,\n            'avg_confidence': avg_confidence,\n            'setting': setting\n        })\n        \n        print(f\"  結果: {result}\")\n        print(f\"  時間: {generation_time:.2f}秒, 平均信頼度: {avg_confidence:.3f}\")\n    \n    return results\n\n# 創作的なプロンプトで実験\ncreative_prompt = \"Write a haiku about artificial intelligence.\"\nstep_results = compare_different_steps(creative_prompt)\n\n# 高度な設定での比較\nadvanced_results = compare_advanced_settings(\"Tell me a short story about a time-traveling scientist.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 温度（Temperature）の影響"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def compare_temperatures_with_highlights(prompt, temperatures=[0.0, 0.3, 0.7]):\n    \"\"\"\n    異なる温度設定での生成結果を拡張可視化で比較\n    \"\"\"\n    print(f\"温度比較実験（拡張可視化）\")\n    print(f\"プロンプト: {prompt}\")\n    print(\"=\" * 60)\n    \n    all_histories = []\n    all_titles = []\n    \n    for temp in temperatures:\n        print(f\"\\n温度 {temp}での生成:\")\n        \n        result, history = simple_generate(\n            model, prompt,\n            steps=16, gen_length=40, temperature=temp\n        )\n        \n        print(f\"結果: {result}\")\n        \n        # 生成プロセスをハイライト表示\n        print(f\"\\n温度{temp}の生成プロセス:\")\n        visualize_generation_with_highlights(history[-5:], \n                                           title=f\"温度{temp}での生成（最後の5ステップ）\", \n                                           show_details=False)\n        \n        all_histories.append(history[-3:])  # 最後の3ステップを保存\n        all_titles.append(f\"温度{temp}\")\n        \n        if temp == 0.0:\n            print(\"  → 決定論的（毎回同じ結果）\")\n        elif temp < 0.5:\n            print(\"  → 低ランダム性（安定した結果）\")\n        else:\n            print(\"  → 高ランダム性（創造的だが不安定）\")\n    \n    # 複数温度の比較表示\n    print(f\"\\n温度別生成プロセス比較:\")\n    compare_generation_steps(all_histories, all_titles)\n\ndef demonstrate_new_visualization_features():\n    \"\"\"\n    新しい可視化機能の包括的なデモンストレーション\n    \"\"\"\n    print(\"LLaDA拡張可視化機能デモンストレーション\")\n    print(\"=\" * 60)\n    \n    # デモ用の短い生成実験\n    demo_prompt = \"What makes a good leader?\"\n    \n    print(f\"デモプロンプト: {demo_prompt}\")\n    print(\"生成設定: 16ステップ, 48トークン, 温度0.2\")\n    print(\"-\" * 50)\n    \n    demo_result, demo_history = generate_llada(\n        model, demo_prompt,\n        steps=16, gen_length=48, block_length=24,\n        temperature=0.2, cfg_scale=0.0, remasking='low_confidence'\n    )\n    \n    print(f\"\\n生成結果: {demo_result}\")\n    \n    # 機能1: ハイライト付き可視化\n    print(f\"\\n\" + \"=\" * 30)\n    print(\"機能1: ハイライト付き可視化\")\n    print(\"=\" * 30)\n    visualize_generation_with_highlights(demo_history, \n                                       \"リーダーシップに関する生成プロセス\", \n                                       show_details=True)\n    \n    # 機能2: アニメーション風表示\n    print(f\"\\n\" + \"=\" * 30)\n    print(\"機能2: アニメーション風表示（最後の6ステップ）\")\n    print(\"=\" * 30)\n    visualize_step_by_step_animation(demo_history[-6:], \n                                   title=\"生成プロセスのアニメーション\", \n                                   delay=0.3)\n    \n    # 機能3: 信頼度とブロック分析\n    print(f\"\\n\" + \"=\" * 30)\n    print(\"機能3: ブロック別詳細分析\")\n    print(\"=\" * 30)\n    visualize_block_progression(demo_history)\n    \n    return demo_result, demo_history\n\n# 物語の始まりを拡張可視化で生成\nstory_prompt = \"Once upon a time, in a small village\"\ncompare_temperatures_with_highlights(story_prompt)\n\n# 包括的なデモンストレーション\nprint(\"\\n\" + \"=\" * 50)\nprint(\"新機能の包括的デモンストレーション\")\nprint(\"=\" * 50)\n\ndemo_result, demo_history = demonstrate_new_visualization_features()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 新機能: 拡張可視化システム\n\nこのセクションでは、LLaDAの生成プロセスをより深く理解するための拡張可視化機能を紹介します。\n\n### 追加された主要機能\n\n#### 1. **ハイライト付き可視化** (`visualize_generation_with_highlights()`)\n- **新しく生成された単語**を太字と色付きで強調表示\n- **信頼度レベル**に応じた落ち着いた色分け：\n  - 深い緑: 高信頼度 (≥0.8)\n  - オリーブ: 中信頼度 (≥0.6)  \n  - ブラウン: 低信頼度 (≥0.4)\n  - グレー: 極低信頼度 (<0.4)\n- **変化の詳細**：位置別の変更履歴を表示\n\n#### 2. **段階的アニメーション表示** (`visualize_step_by_step_animation()`)\n- Jupyter環境で**画面をクリア**して動的表示\n- 各ステップの進行を**時系列で可視化**\n- 調整可能な表示間隔（`delay`パラメータ）\n\n#### 3. **比較表示機能** (`compare_generation_steps()`)\n- **複数の生成結果**を並べて比較\n- 異なる設定（温度、CFGなど）の影響を分析\n- ステップ別の差異を明確化\n\n#### 4. **トークン差分検出** (`detect_token_changes()`)\n- 前のステップとの**自動比較**\n- 変化タイプの分類：\n  - `revealed`: [MASK] → 実際の単語（**太字**表示）\n  - `masked`: 実際の単語 → [MASK]（薄字表示）\n  - `changed`: 単語 → 別の単語（**下線**表示）\n\n### 使用方法\n\n```python\n# 基本的なハイライト表示\nvisualize_generation_with_highlights(history, \"生成プロセス\")\n\n# アニメーション風表示（Jupyter推奨）\nvisualize_step_by_step_animation(history, delay=1.0)\n\n# 複数結果の比較\ncompare_generation_steps([history1, history2], [\"設定A\", \"設定B\"])\n```\n\n### 活用のヒント\n\n1. **パラメータ調整時**: 異なる設定での生成プロセスを比較\n2. **デバッグ時**: 予期しない生成結果の原因を特定\n3. **学習時**: 拡散プロセスの理解を深める\n4. **プレゼン時**: 拡散モデルの動作を視覚的に説明\n\n### 実践例\n\n以下のセルでは、これらの新機能を実際に体験できます：",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def analyze_confidence_over_steps(history):\n    \"\"\"\n    ステップごとの信頼度の変化をグラフ化\n    \"\"\"\n    global_steps = [entry['global_step'] + 1 for entry in history]\n    confidences = [entry['avg_confidence'] for entry in history]\n    tokens_revealed = [entry['num_revealed'] for entry in history]\n    blocks = [entry['block'] for entry in history]\n    \n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n    \n    # 信頼度の変化（ブロック別に色分け）\n    unique_blocks = sorted(set(blocks))\n    colors = plt.cm.Set1(np.linspace(0, 1, len(unique_blocks)))\n    \n    for i, block in enumerate(unique_blocks):\n        block_mask = [b == block for b in blocks]\n        block_steps = [s for s, m in zip(global_steps, block_mask) if m]\n        block_confs = [c for c, m in zip(confidences, block_mask) if m]\n        \n        ax1.plot(block_steps, block_confs, 'o-', color=colors[i], \n                linewidth=2, markersize=6, label=f'ブロック {block+1}')\n    \n    ax1.set_xlabel('Global Generation Step')\n    ax1.set_ylabel('Average Confidence')\n    ax1.set_title('Confidence Score Over Generation Steps (by Block)')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # 各ステップで明かされたトークン数\n    bars = ax2.bar(global_steps, tokens_revealed, alpha=0.7)\n    \n    # ブロックごとに色分け\n    for i, (bar, block) in enumerate(zip(bars, blocks)):\n        bar.set_color(colors[block])\n    \n    ax2.set_xlabel('Global Generation Step')\n    ax2.set_ylabel('Tokens Revealed')\n    ax2.set_title('Number of Tokens Revealed per Step')\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 統計情報\n    print(f\"平均信頼度: {np.mean(confidences):.3f}\")\n    print(f\"信頼度の標準偏差: {np.std(confidences):.3f}\")\n    print(f\"総トークン数: {sum(tokens_revealed)}\")\n    print(f\"ブロック数: {len(unique_blocks)}\")\n\ndef visualize_block_progression(history):\n    \"\"\"\n    ブロック別の生成進捗を可視化\n    \"\"\"\n    if not history:\n        print(\"履歴データがありません\")\n        return\n        \n    print(\"\\\\n🔍 ブロック別生成進捗:\")\n    print(\"=\" * 50)\n    \n    current_block = -1\n    for entry in history:\n        if entry['block'] != current_block:\n            current_block = entry['block']\n            print(f\"\\\\n📦 ブロック {current_block + 1}:\")\n            \n        step = entry['step']\n        text = entry['text']\n        num_revealed = entry['num_revealed']\n        avg_conf = entry['avg_confidence']\n        \n        # [MASK]を視覚的に表現\n        visual_text = text.replace('<|reserved_special_token_250|>', '[MASK]')\n        \n        print(f\"  Step {step+1:2d}: +{num_revealed:2d} tokens (conf={avg_conf:.3f})\")\n        print(f\"    状態: {visual_text[:80]}{'...' if len(visual_text) > 80 else ''}\")\n\n# 前回の数学問題の結果を分析\nif 'math_history' in locals():\n    print(\"数学問題の生成プロセス分析:\")\n    analyze_confidence_over_steps(math_history)\n    visualize_block_progression(math_history[-10:])  # 最後の10ステップ\nelse:\n    print(\"分析用のデータを生成中...\")\n    _, analysis_history = generate_llada(\n        model, \"Explain how photosynthesis works in plants.\",\n        steps=24, gen_length=80, block_length=20,\n        temperature=0.0, cfg_scale=0.0, remasking='low_confidence'\n    )\n    analyze_confidence_over_steps(analysis_history)\n    visualize_block_progression(analysis_history[-8:])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 自己回帰モデルとの比較\n",
    "\n",
    "拡散モデルと自己回帰モデルの違いを理解するために、LLaDAの特徴を整理してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_generation_strategies():\n",
    "    \"\"\"\n",
    "    生成戦略の違いを視覚的に説明\n",
    "    \"\"\"\n",
    "    print(\"📊 生成戦略の比較\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 自己回帰モデル（GPT風）のシミュレーション\n",
    "    print(\"🔄 自己回帰モデル (GPT/LLaMA):\")\n",
    "    autoregressive_steps = [\n",
    "        \"The\",\n",
    "        \"The capital\",\n",
    "        \"The capital of\",\n",
    "        \"The capital of Japan\",\n",
    "        \"The capital of Japan is\",\n",
    "        \"The capital of Japan is Tokyo\"\n",
    "    ]\n",
    "    \n",
    "    for i, step in enumerate(autoregressive_steps):\n",
    "        print(f\"  Step {i+1}: {step}\")\n",
    "    \n",
    "    print(\"\\n  特徴:\")\n",
    "    print(\"  ✓ 左から右へ順次生成\")\n",
    "    print(\"  ✓ 各トークンは過去のトークンのみ参照\")\n",
    "    print(\"  ✓ 高速（KV-cacheが使える）\")\n",
    "    print(\"  ✗ 後続の文脈を考慮できない\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # 拡散モデル（LLaDA）\n",
    "    print(\"🌀 拡散モデル (LLaDA):\")\n",
    "    diffusion_steps = [\n",
    "        \"[MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\",\n",
    "        \"The [MASK] [MASK] Japan [MASK] [MASK]\",\n",
    "        \"The capital [MASK] Japan [MASK] Tokyo\",\n",
    "        \"The capital of Japan is Tokyo\"\n",
    "    ]\n",
    "    \n",
    "    for i, step in enumerate(diffusion_steps):\n",
    "        print(f\"  Step {i+1}: {step}\")\n",
    "    \n",
    "    print(\"\\n  特徴:\")\n",
    "    print(\"  ✓ 全体構造から詳細へ\")\n",
    "    print(\"  ✓ 双方向の文脈を考慮\")\n",
    "    print(\"  ✓ 反復的な品質改善\")\n",
    "    print(\"  ✗ 比較的低速（KV-cache使用不可）\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"\\n💡 主な違い:\")\n",
    "    print(\"1. 生成方向: 自己回帰は一方向、拡散は双方向\")\n",
    "    print(\"2. 品質向上: 拡散は複数ステップで反復改善\")\n",
    "    print(\"3. 計算効率: 自己回帰が高速、拡散は高品質\")\n",
    "    print(\"4. 制御性: 拡散はより細かい制御が可能\")\n",
    "\n",
    "compare_generation_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実際の性能比較実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_analysis(prompts, steps_list=[8, 16, 32]):\n",
    "    \"\"\"\n",
    "    異なるプロンプトとステップ数での性能分析\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"\\n🔍 分析中: {prompt[:50]}...\")\n",
    "        \n",
    "        for steps in steps_list:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            result, history = simple_generate(\n",
    "                model, prompt,\n",
    "                steps=steps, gen_length=40, temperature=0.0\n",
    "            )\n",
    "            \n",
    "            generation_time = time.time() - start_time\n",
    "            avg_confidence = np.mean([h['avg_confidence'] for h in history])\n",
    "            \n",
    "            results.append({\n",
    "                'prompt': prompt,\n",
    "                'steps': steps,\n",
    "                'time': generation_time,\n",
    "                'avg_confidence': avg_confidence,\n",
    "                'result': result\n",
    "            })\n",
    "            \n",
    "            print(f\"  {steps:2d}ステップ: {generation_time:.2f}秒, 信頼度{avg_confidence:.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 異なるタイプの質問で性能テスト\n",
    "test_prompts = [\n",
    "    \"What is 15 + 27?\",  # 簡単な計算\n",
    "    \"Explain the water cycle.\",  # 説明文\n",
    "    \"Write a short poem about spring.\",  # 創作\n",
    "]\n",
    "\n",
    "perf_results = performance_analysis(test_prompts, [8, 16, 24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果の可視化\n",
    "def visualize_performance_results(results):\n",
    "    # データの整理\n",
    "    steps_data = {}\n",
    "    for result in results:\n",
    "        steps = result['steps']\n",
    "        if steps not in steps_data:\n",
    "            steps_data[steps] = {'times': [], 'confidences': []}\n",
    "        steps_data[steps]['times'].append(result['time'])\n",
    "        steps_data[steps]['confidences'].append(result['avg_confidence'])\n",
    "    \n",
    "    # グラフ作成\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    steps_list = sorted(steps_data.keys())\n",
    "    avg_times = [np.mean(steps_data[s]['times']) for s in steps_list]\n",
    "    avg_confidences = [np.mean(steps_data[s]['confidences']) for s in steps_list]\n",
    "    \n",
    "    # 生成時間\n",
    "    ax1.bar(steps_list, avg_times, color='skyblue', alpha=0.7)\n",
    "    ax1.set_xlabel('Number of Steps')\n",
    "    ax1.set_ylabel('Average Generation Time (seconds)')\n",
    "    ax1.set_title('Generation Time vs Steps')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 平均信頼度\n",
    "    ax2.bar(steps_list, avg_confidences, color='lightcoral', alpha=0.7)\n",
    "    ax2.set_xlabel('Number of Steps')\n",
    "    ax2.set_ylabel('Average Confidence Score')\n",
    "    ax2.set_title('Confidence vs Steps')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n📈 性能分析結果:\")\n",
    "    for steps in steps_list:\n",
    "        avg_time = np.mean(steps_data[steps]['times'])\n",
    "        avg_conf = np.mean(steps_data[steps]['confidences'])\n",
    "        print(f\"  {steps:2d}ステップ: 平均{avg_time:.2f}秒, 信頼度{avg_conf:.3f}\")\n",
    "\n",
    "visualize_performance_results(perf_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. まとめと発展的な実験\n",
    "\n",
    "### 学んだこと\n",
    "\n",
    "このノートブックを通じて、以下のような拡散言語モデルの特徴を学びました：\n",
    "\n",
    "1. **拡散プロセス**: [MASK]トークンから始まり、段階的に実際のトークンを明かしていく\n",
    "2. **信頼度ベース選択**: モデルの予測信頼度に基づいて、どのトークンを確定するかを決定\n",
    "3. **双方向文脈**: 従来の自己回帰モデルと異なり、全体の文脈を考慮できる\n",
    "4. **パラメータ制御**: ステップ数、温度などで生成の品質と多様性を調整可能\n",
    "\n",
    "### 発展的な実験\n",
    "\n",
    "以下のような実験をさらに試してみることができます："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_experiments():\n",
    "    \"\"\"\n",
    "    発展的な実験のアイデアを提示\n",
    "    \"\"\"\n",
    "    experiments = [\n",
    "        {\n",
    "            \"title\": \"🎯 制約付き生成\",\n",
    "            \"description\": \"特定の単語を特定の位置に配置した生成\",\n",
    "            \"example\": \"位置3に'beautiful'、位置7に'sunset'を指定\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"🔄 再マスキング戦略\",\n",
    "            \"description\": \"'low_confidence'と'random'の違いを比較\",\n",
    "            \"example\": \"同じプロンプトで戦略を変えて結果を比較\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"📊 長文生成\",\n",
    "            \"description\": \"長いテキストでの拡散プロセスの観察\",\n",
    "            \"example\": \"200トークン以上の物語や記事の生成\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"🌡️ 温度スケジューリング\",\n",
    "            \"description\": \"ステップごとに温度を変化させる\",\n",
    "            \"example\": \"初期は高温度、後期は低温度で安定化\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"🎨 創作性評価\",\n",
    "            \"description\": \"詩や物語の創作での拡散の効果\",\n",
    "            \"example\": \"同じテーマで複数回生成して多様性を測定\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"🚀 発展的な実験アイデア\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, exp in enumerate(experiments, 1):\n",
    "        print(f\"\\n{i}. {exp['title']}\")\n",
    "        print(f\"   概要: {exp['description']}\")\n",
    "        print(f\"   例: {exp['example']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"💡 実験のヒント:\")\n",
    "    print(\"- 異なるドメイン（数学、科学、文学）での性能を比較\")\n",
    "    print(\"- 生成品質の定量的評価指標を考案\")\n",
    "    print(\"- 他の拡散モデルとの比較研究\")\n",
    "    print(\"- リアルタイム可視化ツールの開発\")\n",
    "\n",
    "advanced_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 簡単な発展実験: 創作性の比較"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def creativity_experiment():\n    \"\"\"\n    創作的なタスクでの拡散モデルの特徴を調べる\n    \"\"\"\n    creative_prompt = \"Write a creative story about a robot discovering emotions.\"\n    \n    print(\"🎨 創作性実験\")\n    print(\"プロンプト:\", creative_prompt)\n    print(\"=\" * 60)\n    \n    # 異なる設定で生成\n    settings = [\n        {\n            \"steps\": 16, \"block_length\": 40, \"temperature\": 0.0, \n            \"cfg_scale\": 0.0, \"remasking\": \"low_confidence\",\n            \"name\": \"決定論的・高速\"\n        },\n        {\n            \"steps\": 32, \"block_length\": 20, \"temperature\": 0.0, \n            \"cfg_scale\": 0.0, \"remasking\": \"low_confidence\",\n            \"name\": \"決定論的・高品質\"\n        },\n        {\n            \"steps\": 24, \"block_length\": 30, \"temperature\": 0.4, \n            \"cfg_scale\": 0.0, \"remasking\": \"low_confidence\",\n            \"name\": \"創造的・中品質\"\n        },\n        {\n            \"steps\": 20, \"block_length\": 25, \"temperature\": 0.2, \n            \"cfg_scale\": 1.5, \"remasking\": \"low_confidence\",\n            \"name\": \"CFG強化・創造的\"\n        },\n        {\n            \"steps\": 18, \"block_length\": 35, \"temperature\": 0.0, \n            \"cfg_scale\": 0.0, \"remasking\": \"random\",\n            \"name\": \"ランダム再マスク\"\n        }\n    ]\n    \n    results = []\n    \n    for setting in settings:\n        print(f\"\\\\n📝 {setting['name']}:\")\n        print(f\"  設定: steps={setting['steps']}, block={setting['block_length']}, \"\n              f\"temp={setting['temperature']}, cfg={setting['cfg_scale']}, \"\n              f\"remasking={setting['remasking']}\")\n        \n        start_time = time.time()\n        result, history = generate_llada(\n            model, creative_prompt,\n            steps=setting['steps'],\n            gen_length=80,\n            block_length=setting['block_length'],\n            temperature=setting['temperature'],\n            cfg_scale=setting['cfg_scale'],\n            remasking=setting['remasking']\n        )\n        generation_time = time.time() - start_time\n        \n        avg_confidence = np.mean([h['avg_confidence'] for h in history])\n        \n        results.append({\n            'setting': setting['name'],\n            'text': result,\n            'time': generation_time,\n            'avg_confidence': avg_confidence,\n            'parameters': setting\n        })\n        \n        print(f\"  結果: {result}\")\n        print(f\"  時間: {generation_time:.2f}秒, 平均信頼度: {avg_confidence:.3f}\")\n    \n    print(\"\\\\n\" + \"=\" * 60)\n    print(\"🔍 分析:\")\n    print(\"- 温度0.0では一貫した結果が得られる\")\n    print(\"- ステップ数を増やすことで詳細度が向上\")\n    print(\"- 適度な温度は創造性を高める可能性\")\n    print(\"- CFGは条件付き生成の品質を向上\")\n    print(\"- ランダム再マスクは予期しない創造性を提供\")\n    \n    return results\n\ndef compare_remasking_strategies():\n    \"\"\"\n    再マスキング戦略の比較実験\n    \"\"\"\n    prompt = \"Describe the process of making coffee step by step.\"\n    \n    print(\"\\\\n🔄 再マスキング戦略比較\")\n    print(\"プロンプト:\", prompt)\n    print(\"=\" * 60)\n    \n    strategies = ['low_confidence', 'random']\n    results = {}\n    \n    for strategy in strategies:\n        print(f\"\\\\n🎯 戦略: {strategy}\")\n        \n        # 同じ設定で複数回実行して安定性を確認\n        runs = []\n        for run in range(3):\n            result, history = generate_llada(\n                model, prompt,\n                steps=20, gen_length=60, block_length=20,\n                temperature=0.0, cfg_scale=0.0, remasking=strategy\n            )\n            \n            avg_confidence = np.mean([h['avg_confidence'] for h in history])\n            runs.append({\n                'run': run + 1,\n                'result': result,\n                'avg_confidence': avg_confidence\n            })\n            \n            print(f\"  実行 {run + 1}: 信頼度 {avg_confidence:.3f}\")\n            print(f\"    結果: {result[:60]}{'...' if len(result) > 60 else ''}\")\n        \n        results[strategy] = runs\n    \n    print(\"\\\\n\" + \"=\" * 60)\n    print(\"📊 戦略比較結果:\")\n    for strategy, runs in results.items():\n        avg_conf = np.mean([r['avg_confidence'] for r in runs])\n        std_conf = np.std([r['avg_confidence'] for r in runs])\n        print(f\"{strategy}: 平均信頼度 {avg_conf:.3f} ± {std_conf:.3f}\")\n    \n    return results\n\n# 創作実験を実行\ncreativity_results = creativity_experiment()\n\n# 再マスキング戦略比較\nremasking_results = compare_remasking_strategies()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最終チャレンジ: オリジナル実験"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# あなた自身の実験を試してみましょう！\n\nprint(\"🎯 チャレンジ: あなた自身の実験\")\nprint(\"=\" * 40)\nprint(\"完全なLLaDAの機能を使って自由に実験してみてください:\")\nprint()\nprint(\"📝 プロンプトの例:\")\nprint('- \"Explain quantum physics in simple terms.\"')\nprint('- \"Write a dialogue between two AIs.\"')\nprint('- \"Create a recipe for happiness.\"')\nprint('- \"Describe a day in the life of a photon.\"')\nprint()\nprint(\"⚙️ 利用可能なパラメータ:\")\nprint(\"- steps: 8~64 (推奨: 16, 24, 32)\")\nprint(\"- gen_length: 32~128 (推奨: 48, 64, 80)\")\nprint(\"- block_length: 8~gen_length (推奨: 16, 24, 32)\")\nprint(\"- temperature: 0.0~1.0 (推奨: 0.0, 0.3, 0.5)\")\nprint(\"- cfg_scale: 0.0~3.0 (推奨: 0.0, 1.0, 2.0)\")\nprint(\"- remasking: 'low_confidence' または 'random'\")\nprint()\nprint(\"🚀 新機能:\")\nprint(\"✓ セミ自己回帰生成（ブロック処理）\")\nprint(\"✓ 分類器フリーガイダンス（CFG）\")\nprint(\"✓ 再マスキング戦略選択\")\nprint(\"✓ 詳細な生成プロセス追跡\")\nprint()\nprint(\"例:\")\nprint('your_result, your_history = generate_llada(')\nprint('    model, \"あなたのプロンプト\",')\nprint('    steps=24, gen_length=64, block_length=16,')\nprint('    temperature=0.2, cfg_scale=1.0, remasking=\"low_confidence\"')\nprint(')')\nprint('visualize_block_progression(your_history)')\nprint('analyze_confidence_over_steps(your_history)')\nprint()\nprint(\"💡 実験アイデア:\")\nprint(\"1. CFGスケール（0.0, 1.0, 2.0）の影響を比較\")\nprint(\"2. ブロックサイズが生成品質に与える影響\")\nprint(\"3. 創作的なタスクでの温度とCFGの組み合わせ\")\nprint(\"4. 科学的説明vs創作的文章での最適設定\")\nprint(\"5. 再マスキング戦略の使い分け\")\n\n# ここにあなたの実験コードを書いてください！\n# 例:\n# my_prompt = \"Write a short poem about the beauty of mathematics.\"\n# my_result, my_history = generate_llada(\n#     model, my_prompt,\n#     steps=20, gen_length=60, block_length=20,\n#     temperature=0.3, cfg_scale=1.2, remasking=\"low_confidence\"\n# )\n# print(f\"結果: {my_result}\")\n# visualize_block_progression(my_history[-5:])  # 最後の5ステップを表示"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 学習のまとめ\n",
    "\n",
    "### LLaDA拡散言語モデルで学んだ重要なポイント\n",
    "\n",
    "1. **拡散プロセスの理解**\n",
    "   - 全てのトークンを[MASK]で初期化\n",
    "   - 信頼度に基づく段階的な明かし\n",
    "   - 双方向の文脈考慮\n",
    "\n",
    "2. **自己回帰との違い**\n",
    "   - 生成方向: 一方向 vs 全方向\n",
    "   - 品質改善: 一回 vs 反復的\n",
    "   - 制御性: 限定的 vs 柔軟\n",
    "\n",
    "3. **パラメータの影響**\n",
    "   - ステップ数: 品質と計算時間のトレードオフ\n",
    "   - 温度: 決定論的 vs 創造的\n",
    "   - 生成長: 応答の詳細度\n",
    "\n",
    "4. **実用的な洞察**\n",
    "   - 数学的な問題: 低温度、多ステップが効果的\n",
    "   - 創作的なタスク: 中程度の温度で多様性向上\n",
    "   - 短い応答: 少ないステップでも十分\n",
    "\n",
    "### 次のステップ\n",
    "\n",
    "- 他の拡散言語モデルとの比較\n",
    "- 専門分野（医療、法律、技術）での性能評価\n",
    "- リアルタイム応用の可能性探索\n",
    "- ファインチューニング手法の研究\n",
    "\n",
    "**おめでとうございます！** 拡散言語モデルの基礎を理解し、実際にLLaDAを使った実験を完了しました。🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}