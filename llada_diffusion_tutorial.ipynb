{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaDA: æ‹¡æ•£è¨€èªãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€LLaDAï¼ˆLarge Language Diffusion with mAskingï¼‰ã‚’ä½¿ã£ã¦æ‹¡æ•£è¨€èªãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´ã¨å‹•ä½œåŸç†ã‚’å­¦ç¿’ã—ã¾ã™ã€‚\n",
    "\n",
    "## ğŸ“š ç›®æ¬¡\n",
    "1. [æ‹¡æ•£è¨€èªãƒ¢ãƒ‡ãƒ«ã¨ã¯ï¼Ÿ](#1-æ‹¡æ•£è¨€èªãƒ¢ãƒ‡ãƒ«ã¨ã¯)\n",
    "2. [LLaDAã®ç‰¹å¾´](#2-lladaã®ç‰¹å¾´)\n",
    "3. [ç’°å¢ƒè¨­å®šï¼ˆGoogle Colabï¼‰](#3-ç’°å¢ƒè¨­å®šgoogle-colab)\n",
    "4. [åŸºæœ¬çš„ãªç”Ÿæˆå®Ÿé¨“](#4-åŸºæœ¬çš„ãªç”Ÿæˆå®Ÿé¨“)\n",
    "5. [æ‹¡æ•£ãƒ—ãƒ­ã‚»ã‚¹ã®å¯è¦–åŒ–](#5-æ‹¡æ•£ãƒ—ãƒ­ã‚»ã‚¹ã®å¯è¦–åŒ–)\n",
    "6. [ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿ã‚’èª¿ã¹ã‚‹](#6-ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿ã‚’èª¿ã¹ã‚‹)\n",
    "7. [è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒ](#7-è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒ)\n",
    "8. [ã¾ã¨ã‚ã¨ç™ºå±•çš„ãªå®Ÿé¨“](#8-ã¾ã¨ã‚ã¨ç™ºå±•çš„ãªå®Ÿé¨“)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. æ‹¡æ•£è¨€èªãƒ¢ãƒ‡ãƒ«ã¨ã¯ï¼Ÿ\n",
    "\n",
    "### å¾“æ¥ã®è‡ªå·±å›å¸°è¨€èªãƒ¢ãƒ‡ãƒ«\n",
    "- **GPT**ã‚„**LLaMA**ã®ã‚ˆã†ãªè‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’**å·¦ã‹ã‚‰å³**ã¸é †ç•ªã«ç”Ÿæˆã—ã¾ã™\n",
    "- å„ã‚¹ãƒ†ãƒƒãƒ—ã§æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ã—ã€ãã‚Œã‚’å…ƒã«æ¬¡ã®äºˆæ¸¬ã‚’è¡Œã„ã¾ã™\n",
    "\n",
    "```\n",
    "è‡ªå·±å›å¸°: \"The\" â†’ \"The cat\" â†’ \"The cat sits\" â†’ \"The cat sits on\"\n",
    "```\n",
    "\n",
    "### æ‹¡æ•£è¨€èªãƒ¢ãƒ‡ãƒ«\n",
    "- **æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«**ã¯ç”»åƒç”Ÿæˆï¼ˆDALL-Eã€Stable Diffusionãªã©ï¼‰ã§åºƒãä½¿ã‚ã‚Œã¦ã„ã‚‹æŠ€è¡“ã§ã™\n",
    "- **LLaDA**ã¯æ‹¡æ•£ã®æ¦‚å¿µã‚’ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã«å¿œç”¨ã—ãŸé©æ–°çš„ãªãƒ¢ãƒ‡ãƒ«ã§ã™\n",
    "\n",
    "#### æ‹¡æ•£ãƒ—ãƒ­ã‚»ã‚¹ã®ç‰¹å¾´ï¼š\n",
    "1. **å…¨ä½“ã‹ã‚‰éƒ¨åˆ†ã¸**: æœ€åˆã«å…¨ä½“ã®æ§‹é€ ã‚’æ±ºã‚ã€å¾ã€…ã«è©³ç´°ã‚’åŸ‹ã‚ã¦ã„ã\n",
    "2. **ä¸¦åˆ—å‡¦ç†**: ã™ã¹ã¦ã®ä½ç½®ã‚’åŒæ™‚ã«è€ƒæ…®ã§ãã‚‹\n",
    "3. **åå¾©æ”¹å–„**: è¤‡æ•°ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’é€šã˜ã¦å“è³ªã‚’å‘ä¸Šã•ã›ã‚‹\n",
    "\n",
    "```\n",
    "æ‹¡æ•£: \"[MASK] [MASK] [MASK] [MASK]\" â†’ \"The [MASK] [MASK] on\" â†’ \"The cat [MASK] on\" â†’ \"The cat sits on\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LLaDAã®ç‰¹å¾´\n",
    "\n",
    "### ğŸ”‘ ä¸»è¦ãªæ¦‚å¿µ\n",
    "\n",
    "#### ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ [MASK]\n",
    "- ãƒˆãƒ¼ã‚¯ãƒ³ID: `126336`\n",
    "- æœªç¢ºå®šã®ä½ç½®ã‚’è¡¨ã™ç‰¹åˆ¥ãªãƒˆãƒ¼ã‚¯ãƒ³\n",
    "- ç”Ÿæˆéç¨‹ã§æ®µéšçš„ã«å®Ÿéš›ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«ç½®ãæ›ãˆã‚‰ã‚Œã‚‹\n",
    "\n",
    "#### ä¿¡é ¼åº¦ãƒ™ãƒ¼ã‚¹ã®é¸æŠ\n",
    "- ãƒ¢ãƒ‡ãƒ«ã¯å„ä½ç½®ã§ã®ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã«**ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢**ã‚’ä»˜ã‘ã‚‹\n",
    "- é«˜ä¿¡é ¼åº¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰é †ç•ªã«ç¢ºå®šã—ã¦ã„ã\n",
    "- ä½ä¿¡é ¼åº¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¯å†ãƒã‚¹ã‚¯ã•ã‚Œã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã§å†äºˆæ¸¬\n",
    "\n",
    "#### åŒæ–¹å‘æ³¨æ„\n",
    "- è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã¨é•ã„ã€**å› æœãƒã‚¹ã‚­ãƒ³ã‚°ãŒãªã„**\n",
    "- å„ãƒˆãƒ¼ã‚¯ãƒ³ãŒæ–‡ç« å…¨ä½“ã®æ–‡è„ˆã‚’å‚ç…§ã§ãã‚‹\n",
    "- ã‚ˆã‚Šä¸€è²«æ€§ã®ã‚ã‚‹æ–‡ç« ç”ŸæˆãŒå¯èƒ½\n",
    "\n",
    "### ğŸ—ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£\n",
    "- **ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«**: Transformer Encoderï¼ˆ8Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰\n",
    "- **å­¦ç¿’ãƒ‡ãƒ¼ã‚¿**: 2.3Tãƒˆãƒ¼ã‚¯ãƒ³\n",
    "- **2ã¤ã®ãƒãƒªã‚¢ãƒ³ãƒˆ**:\n",
    "  - `LLaDA-8B-Base`: äº‹å‰å­¦ç¿’ã®ã¿\n",
    "  - `LLaDA-8B-Instruct`: æŒ‡ç¤ºã«å¾“ã†ã‚ˆã†å¾®èª¿æ•´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ç’°å¢ƒè¨­å®šï¼ˆGoogle Colabï¼‰\n",
    "\n",
    "Google Colabã§å®Ÿè¡Œã™ã‚‹å ´åˆã¯ã€ä»¥ä¸‹ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colabã§ã®å®Ÿè¡Œç”¨\n",
    "import sys\n",
    "\n",
    "# Google Colabã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Google Colabç’°å¢ƒã‚’æ¤œå‡ºã—ã¾ã—ãŸ\")\n",
    "    \n",
    "    # å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    !pip install transformers==4.49.0 accelerate==0.34.2 torch numpy matplotlib ipywidgets\n",
    "    \n",
    "    # GPUãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯\n",
    "    import torch\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å®Ÿè¡Œä¸­ã§ã™\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from typing import List, Tuple, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆColabç”¨ï¼‰\n",
    "if IN_COLAB:\n",
    "    !apt-get -qq install fonts-noto-cjk\n",
    "    import matplotlib.font_manager as fm\n",
    "    plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "print(\"ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. åŸºæœ¬çš„ãªç”Ÿæˆå®Ÿé¨“\n",
    "\n",
    "ã¾ãšã€LLaDAã®åŸºæœ¬çš„ãªä½¿ã„æ–¹ã‚’å­¦ã³ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLaDAãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿\n",
    "print(\"LLaDAãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...ï¼ˆæ•°åˆ†ã‹ã‹ã‚Šã¾ã™ï¼‰\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿\n",
    "model_name = 'GSAI-ML/LLaDA-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True, \n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device).eval()\n",
    "\n",
    "print(\"ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "print(f\"ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {sum(p.numel() for p in model.parameters()) / 1e9:.1f}B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaDAã®æ ¸ã¨ãªã‚‹é–¢æ•°ã‚’å®Ÿè£…\n",
    "\n",
    "ã“ã“ã§ã¯ã€LLaDAã®æ‹¡æ•£ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ã€é‡è¦ãªé–¢æ•°ã‚’è©³ã—ãè¦‹ã¦ã„ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é‡è¦ãªå®šæ•°\n",
    "MASK_ID = 126336  # [MASK]ãƒˆãƒ¼ã‚¯ãƒ³ã®ID\n",
    "\n",
    "def add_gumbel_noise(logits, temperature):\n",
    "    \"\"\"\n",
    "    Gumbelãƒã‚¤ã‚ºã‚’è¿½åŠ ã—ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’è¡Œã†\n",
    "    temperature=0: æ±ºå®šè«–çš„ï¼ˆæœ€ã‚‚ç¢ºç‡ã®é«˜ã„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸æŠï¼‰\n",
    "    temperature>0: ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’å°å…¥ï¼ˆé«˜ã„æ¸©åº¦ã»ã©ãƒ©ãƒ³ãƒ€ãƒ ï¼‰\n",
    "    \"\"\"\n",
    "    if temperature == 0:\n",
    "        return logits\n",
    "    \n",
    "    # é«˜ç²¾åº¦æ¼”ç®—ã®ãŸã‚float64ã‚’ä½¿ç”¨\n",
    "    logits = logits.to(torch.float64)\n",
    "    noise = torch.rand_like(logits, dtype=torch.float64)\n",
    "    gumbel_noise = (-torch.log(noise)) ** temperature\n",
    "    return logits.exp() / gumbel_noise\n",
    "\n",
    "def get_num_transfer_tokens(mask_index, steps):\n",
    "    \"\"\"\n",
    "    å„ã‚¹ãƒ†ãƒƒãƒ—ã§ä½•å€‹ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒã‚¹ã‚¯è§£é™¤ã™ã‚‹ã‹ã‚’è¨ˆç®—\n",
    "    ç·šå½¢ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã«å¾“ã£ã¦å‡ç­‰ã«åˆ†æ•£\n",
    "    \"\"\"\n",
    "    mask_num = mask_index.sum(dim=1, keepdim=True)  # ãƒã‚¹ã‚¯ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®ç·æ•°\n",
    "    \n",
    "    base = mask_num // steps  # åŸºæœ¬çš„ãªæ•°\n",
    "    remainder = mask_num % steps  # ä½™ã‚Š\n",
    "    \n",
    "    # å„ã‚¹ãƒ†ãƒƒãƒ—ã§ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—\n",
    "    num_transfer_tokens = torch.zeros(\n",
    "        mask_num.size(0), steps, \n",
    "        device=mask_index.device, \n",
    "        dtype=torch.int64\n",
    "    ) + base\n",
    "    \n",
    "    # ä½™ã‚Šã‚’æœ€åˆã®ã‚¹ãƒ†ãƒƒãƒ—ã«åˆ†æ•£\n",
    "    for i in range(mask_num.size(0)):\n",
    "        num_transfer_tokens[i, :remainder[i]] += 1\n",
    "    \n",
    "    return num_transfer_tokens\n",
    "\n",
    "print(\"ã‚³ã‚¢é–¢æ•°ã®å®šç¾©ãŒå®Œäº†ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç°¡å˜ãªç”Ÿæˆå®Ÿé¨“"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "@torch.no_grad()\ndef generate_llada(model, prompt, steps=32, gen_length=64, block_length=32, \n                   temperature=0.0, cfg_scale=0.0, remasking='low_confidence'):\n    \"\"\"\n    å®Œå…¨ãªLLaDAç”Ÿæˆé–¢æ•°ï¼ˆGoogle Colabå˜ä½“å‹•ä½œç”¨ï¼‰\n    å…ƒã®generate.pyã®å®Œå…¨å®Ÿè£…\n    \"\"\"\n    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n    if isinstance(prompt, str):\n        # Instructãƒ¢ãƒ‡ãƒ«ç”¨ã®ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é©ç”¨\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        formatted_prompt = tokenizer.apply_chat_template(\n            messages, add_generation_prompt=True, tokenize=False\n        )\n        input_ids = tokenizer(formatted_prompt)['input_ids']\n        input_ids = torch.tensor(input_ids).to(device).unsqueeze(0)\n    else:\n        input_ids = prompt\n    \n    prompt_length = input_ids.shape[1]\n    \n    # å¿œç­”éƒ¨åˆ†ã‚’[MASK]ã§åˆæœŸåŒ–\n    x = torch.full(\n        (1, prompt_length + gen_length), \n        MASK_ID, \n        dtype=torch.long\n    ).to(device)\n    x[:, :prompt_length] = input_ids.clone()\n    \n    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆCFGç”¨ï¼‰\n    prompt_index = (x != MASK_ID)\n    \n    # ãƒ–ãƒ­ãƒƒã‚¯å‡¦ç†ã®è¨­å®šï¼ˆä¿®æ­£ç‰ˆï¼šã‚ˆã‚ŠæŸ”è»Ÿãªè¨­å®šï¼‰\n    # gen_lengthãŒblock_lengthã§å‰²ã‚Šåˆ‡ã‚Œãªã„å ´åˆã‚‚å¯¾å¿œ\n    block_length = min(block_length, gen_length)\n    num_blocks = (gen_length + block_length - 1) // block_length  # åˆ‡ã‚Šä¸Šã’é™¤ç®—\n    \n    # stepsãŒnum_blocksã§å‰²ã‚Šåˆ‡ã‚Œãªã„å ´åˆã®å¯¾å¿œ\n    if steps % num_blocks != 0:\n        # æœ€ã‚‚è¿‘ã„å‰²ã‚Šåˆ‡ã‚Œã‚‹stepsæ•°ã«èª¿æ•´\n        adjusted_steps = ((steps + num_blocks - 1) // num_blocks) * num_blocks\n        print(f\"ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’ {steps} ã‹ã‚‰ {adjusted_steps} ã«èª¿æ•´ã—ã¾ã—ãŸï¼ˆ{num_blocks}ãƒ–ãƒ­ãƒƒã‚¯ã«å¯¾å¿œï¼‰\")\n        steps = adjusted_steps\n    \n    steps_per_block = steps // num_blocks\n    \n    # ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®ç”Ÿæˆè¨˜éŒ²\n    generation_history = []\n    \n    print(f\"ç”Ÿæˆé–‹å§‹: {num_blocks}ãƒ–ãƒ­ãƒƒã‚¯ x {steps_per_block}ã‚¹ãƒ†ãƒƒãƒ— = è¨ˆ{steps}ã‚¹ãƒ†ãƒƒãƒ—\")\n    print(f\"ãƒ–ãƒ­ãƒƒã‚¯è¨­å®š: gen_length={gen_length}, block_length={block_length}\")\n    \n    # å„ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‡¦ç†\n    for block_idx in range(num_blocks):\n        block_start = prompt_length + block_idx * block_length\n        block_end = min(prompt_length + (block_idx + 1) * block_length, x.shape[1])\n        \n        print(f\"\\\\nãƒ–ãƒ­ãƒƒã‚¯ {block_idx + 1}/{num_blocks} (ä½ç½® {block_start}-{block_end-1}):\")\n        \n        # ç¾åœ¨ã®ãƒ–ãƒ­ãƒƒã‚¯ã®ãƒã‚¹ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n        block_mask_index = (x[:, block_start:block_end] == MASK_ID)\n        \n        # ã‚¹ã‚­ãƒƒãƒ—æ¡ä»¶: ãƒ–ãƒ­ãƒƒã‚¯ã«ãƒã‚¹ã‚¯ãŒãªã„å ´åˆ\n        if not block_mask_index.any():\n            print(f\"  ãƒ–ãƒ­ãƒƒã‚¯ {block_idx + 1}: ãƒã‚¹ã‚¯ãªã—ã€ã‚¹ã‚­ãƒƒãƒ—\")\n            continue\n            \n        # ã“ã®ãƒ–ãƒ­ãƒƒã‚¯ã§ã®ãƒˆãƒ¼ã‚¯ãƒ³è»¢é€æ•°ã‚’è¨ˆç®—\n        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps_per_block)\n        \n        # ãƒ–ãƒ­ãƒƒã‚¯å†…ã®å„ã‚¹ãƒ†ãƒƒãƒ—\n        for step in range(steps_per_block):\n            # ç¾åœ¨ã®ãƒã‚¹ã‚¯ä½ç½®ã‚’å–å¾—\n            mask_index = (x == MASK_ID)\n            \n            if not mask_index.any():\n                print(f\"  ã‚¹ãƒ†ãƒƒãƒ— {step + 1}: å…¨ãƒˆãƒ¼ã‚¯ãƒ³ç¢ºå®šæ¸ˆã¿\")\n                break\n            \n            # åˆ†é¡å™¨ãƒ•ãƒªãƒ¼ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ (CFG)\n            if cfg_scale > 0.0:\n                # ç„¡æ¡ä»¶ç”Ÿæˆç”¨ï¼šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚‚ãƒã‚¹ã‚¯\n                un_x = x.clone()\n                un_x[prompt_index] = MASK_ID\n                x_ = torch.cat([x, un_x], dim=0)\n                logits = model(x_).logits\n                logits, un_logits = torch.chunk(logits, 2, dim=0)\n                # CFGã‚¹ã‚±ãƒ¼ãƒ«ã‚’é©ç”¨\n                logits = un_logits + (cfg_scale + 1) * (logits - un_logits)\n            else:\n                logits = model(x).logits\n            \n            # Gumbelãƒã‚¤ã‚ºã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n            logits_with_noise = add_gumbel_noise(logits, temperature)\n            x0 = torch.argmax(logits_with_noise, dim=-1)\n            \n            # å†ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥ã«åŸºã¥ãä¿¡é ¼åº¦è¨ˆç®—\n            if remasking == 'low_confidence':\n                p = F.softmax(logits, dim=-1)\n                x0_p = torch.squeeze(\n                    torch.gather(p, dim=-1, index=torch.unsqueeze(x0, -1)), -1\n                )\n            elif remasking == 'random':\n                x0_p = torch.rand((x0.shape[0], x0.shape[1]), device=x0.device)\n            else:\n                raise NotImplementedError(f\"Remasking strategy '{remasking}' not implemented\")\n            \n            # ç¾åœ¨ã®ãƒ–ãƒ­ãƒƒã‚¯ä»¥å¤–ã®ä½ç½®ã¯é™¤å¤–\n            x0_p[:, block_end:] = -float('inf')\n            \n            # ãƒã‚¹ã‚¯ä½ç½®ã®ã¿ã®äºˆæ¸¬ã¨ä¿¡é ¼åº¦\n            x0 = torch.where(mask_index, x0, x)\n            confidence = torch.where(mask_index, x0_p, -float('inf'))\n            \n            # é«˜ä¿¡é ¼åº¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸æŠã—ã¦ãƒã‚¹ã‚¯è§£é™¤\n            transfer_index = torch.zeros_like(x0, dtype=torch.bool, device=device)\n            for j in range(confidence.shape[0]):\n                num_tokens_to_transfer = min(\n                    num_transfer_tokens[j, step].item(),\n                    mask_index[j].sum().item()  # å®Ÿéš›ã®ãƒã‚¹ã‚¯æ•°ã‚’è¶…ãˆãªã„ã‚ˆã†åˆ¶é™\n                )\n                if num_tokens_to_transfer > 0:\n                    _, select_index = torch.topk(confidence[j], k=num_tokens_to_transfer)\n                    transfer_index[j, select_index] = True\n            \n            # é¸æŠã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç¢ºå®š\n            x[transfer_index] = x0[transfer_index]\n            \n            # é€²æ—ã‚’è¨˜éŒ²\n            current_text = tokenizer.decode(\n                x[0, prompt_length:], skip_special_tokens=False\n            )\n            avg_conf = confidence[0, transfer_index[0]].mean().item() if transfer_index[0].any() else 0\n            \n            generation_history.append({\n                'block': block_idx,\n                'step': step,\n                'global_step': block_idx * steps_per_block + step,\n                'text': current_text,\n                'num_revealed': transfer_index[0].sum().item(),\n                'avg_confidence': avg_conf\n            })\n            \n            print(f\"  ã‚¹ãƒ†ãƒƒãƒ— {step+1:2d}: {transfer_index[0].sum().item():2d}å€‹ç¢ºå®š \"\n                  f\"(ä¿¡é ¼åº¦: {avg_conf:.3f})\")\n    \n    # æœ€çµ‚çµæœ\n    final_text = tokenizer.decode(\n        x[0, prompt_length:], skip_special_tokens=True\n    )\n    \n    return final_text, generation_history\n\n# ç°¡ç•¥ç‰ˆã‚‚ä¿æŒï¼ˆå­¦ç¿’ç”¨ï¼‰\n@torch.no_grad()\ndef simple_generate(model, prompt, steps=32, gen_length=64, temperature=0.0):\n    \"\"\"\n    ç°¡ç•¥åŒ–ã•ã‚ŒãŸLLaDAç”Ÿæˆé–¢æ•°ï¼ˆå­¦ç¿’ç”¨ï¼‰\n    \"\"\"\n    return generate_llada(\n        model, prompt, \n        steps=steps, gen_length=gen_length, block_length=gen_length,\n        temperature=temperature, cfg_scale=0.0, remasking='low_confidence'\n    )\n\n# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ\ntest_prompt = \"What is the capital of Japan?\"\nprint(f\"\\\\nè³ªå•: {test_prompt}\")\nprint(\"=\" * 50)\n\nresult, history = simple_generate(\n    model, test_prompt, \n    steps=16, gen_length=32, temperature=0.0\n)\n\nprint(\"=\" * 50)\nprint(f\"æœ€çµ‚å›ç­”: {result}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æ‹¡æ•£ãƒ—ãƒ­ã‚»ã‚¹ã®å¯è¦–åŒ–\n",
    "\n",
    "LLaDAã®æ‹¡æ•£ãƒ—ãƒ­ã‚»ã‚¹ãŒã©ã®ã‚ˆã†ã«å‹•ä½œã™ã‚‹ã‹ã‚’å¯è¦–åŒ–ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def visualize_generation_process(history, title=\"LLaDA Generation Process\"):\n    \"\"\"\n    ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã‚’æ®µéšçš„ã«è¡¨ç¤º\n    \"\"\"\n    print(f\"\\n{title}\")\n    print(\"=\" * 60)\n    \n    for entry in history:\n        step = entry['step']\n        text = entry['text']\n        num_revealed = entry['num_revealed']\n        avg_conf = entry['avg_confidence']\n        \n        # [MASK]ã‚’è¦–è¦šçš„ã«è¡¨ç¾\n        visual_text = text.replace('<|reserved_special_token_250|>', '[MASK]')\n        \n        print(f\"Step {step+1:2d} (+{num_revealed:2d} tokens, conf={avg_conf:.3f}):\")\n        print(f\"  {visual_text}\")\n        print()\n\ndef detect_token_changes(prev_tokens, curr_tokens):\n    \"\"\"\n    å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã¨ç¾åœ¨ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã®ãƒˆãƒ¼ã‚¯ãƒ³å¤‰åŒ–ã‚’æ¤œå‡º\n    \"\"\"\n    changes = []\n    \n    # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¦æ¯”è¼ƒ\n    if prev_tokens is None:\n        # åˆå›ã®å ´åˆã€ã™ã¹ã¦ãŒæ–°è¦\n        for i, token in enumerate(curr_tokens):\n            if token != '[MASK]':\n                changes.append((i, 'new', token))\n    else:\n        # å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã¨æ¯”è¼ƒ\n        for i, (prev_token, curr_token) in enumerate(zip(prev_tokens, curr_tokens)):\n            if prev_token != curr_token:\n                if prev_token == '[MASK]' and curr_token != '[MASK]':\n                    changes.append((i, 'revealed', curr_token))\n                elif prev_token != '[MASK]' and curr_token == '[MASK]':\n                    changes.append((i, 'masked', prev_token))\n                elif prev_token != '[MASK]' and curr_token != '[MASK]':\n                    changes.append((i, 'changed', curr_token))\n    \n    return changes\n\ndef get_confidence_color(confidence):\n    \"\"\"\n    ä¿¡é ¼åº¦ã«åŸºã¥ã„ã¦è½ã¡ç€ã„ãŸè‰²ã‚’æ±ºå®š\n    \"\"\"\n    if confidence >= 0.8:\n        return '\\033[38;5;28m'  # æ·±ã„ç·‘ï¼ˆé«˜ä¿¡é ¼åº¦ï¼‰\n    elif confidence >= 0.6:\n        return '\\033[38;5;94m'  # ã‚ªãƒªãƒ¼ãƒ–ï¼ˆä¸­ä¿¡é ¼åº¦ï¼‰\n    elif confidence >= 0.4:\n        return '\\033[38;5;130m'  # ãƒ–ãƒ©ã‚¦ãƒ³ï¼ˆä½ä¿¡é ¼åº¦ï¼‰\n    else:\n        return '\\033[38;5;240m'  # ã‚°ãƒ¬ãƒ¼ï¼ˆéå¸¸ã«ä½ä¿¡é ¼åº¦ï¼‰\n\ndef visualize_generation_with_highlights(history, title=\"LLaDA Generation Process (Enhanced)\", show_details=True):\n    \"\"\"\n    æ–°ã—ãç”Ÿæˆã•ã‚ŒãŸå˜èªã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤ºã™ã‚‹æ‹¡å¼µå¯è¦–åŒ–é–¢æ•°ï¼ˆè½ã¡ç€ã„ãŸé…è‰²ç‰ˆï¼‰\n    \"\"\"\n    print(f\"\\n{title}\")\n    print(\"=\" * 70)\n    \n    # è‰²ã®èª¬æ˜\n    if show_details:\n        print(\"ä¿¡é ¼åº¦è‰²åˆ†ã‘:\")\n        print(\"  \\033[38;5;28mâ– \\033[0m é«˜ä¿¡é ¼åº¦ (â‰¥0.8)  \\033[38;5;94mâ– \\033[0m ä¸­ä¿¡é ¼åº¦ (â‰¥0.6)  \\033[38;5;130mâ– \\033[0m ä½ä¿¡é ¼åº¦ (â‰¥0.4)  \\033[38;5;240mâ– \\033[0m æ¥µä½ (<0.4)\")\n        print(\"  å¼·èª¿: \\033[1må¤ªå­—\\033[0m = æ–°è¦ç”Ÿæˆ  \\033[2mè–„å­—\\033[0m = å†ãƒã‚¹ã‚¯  \\033[4mä¸‹ç·š\\033[0m = å¤‰æ›´\")\n        print()\n    \n    prev_tokens = None\n    \n    for i, entry in enumerate(history):\n        global_step = entry.get('global_step', entry['step'])\n        block = entry.get('block', 0)\n        step = entry['step']\n        text = entry['text']\n        num_revealed = entry['num_revealed']\n        avg_conf = entry['avg_confidence']\n        \n        # [MASK]ã‚’æ¨™æº–åŒ–\n        clean_text = text.replace('<|reserved_special_token_250|>', '[MASK]')\n        \n        # ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ†å‰²ï¼ˆç°¡æ˜“ç‰ˆï¼‰\n        curr_tokens = clean_text.split()\n        \n        # å¤‰åŒ–ã‚’æ¤œå‡º\n        changes = detect_token_changes(prev_tokens, curr_tokens)\n        \n        # ã‚¹ãƒ†ãƒƒãƒ—æƒ…å ±ã‚’è¡¨ç¤º\n        block_info = f\"[Block {block+1}]\" if 'block' in entry else \"\"\n        print(f\"Step {global_step+1:2d} {block_info} (+{num_revealed:2d} tokens, conf={avg_conf:.3f}):\")\n        \n        # ãƒã‚¤ãƒ©ã‚¤ãƒˆä»˜ããƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰\n        highlighted_text = []\n        change_map = {pos: (change_type, token) for pos, change_type, token in changes}\n        \n        for pos, token in enumerate(curr_tokens):\n            if pos in change_map:\n                change_type, _ = change_map[pos]\n                color = get_confidence_color(avg_conf)\n                \n                if change_type == 'revealed':\n                    # æ–°è¦ç”Ÿæˆ: å¤ªå­— + è‰²\n                    highlighted_text.append(f\"{color}\\033[1m{token}\\033[0m\")\n                elif change_type == 'masked':\n                    # å†ãƒã‚¹ã‚¯: è–„å­—\n                    highlighted_text.append(f\"\\033[2m[MASK]\\033[0m\")\n                elif change_type == 'changed':\n                    # å¤‰æ›´: ä¸‹ç·š + è‰²\n                    highlighted_text.append(f\"{color}\\033[4m{token}\\033[0m\")\n                else:\n                    highlighted_text.append(f\"{color}{token}\\033[0m\")\n            else:\n                # å¤‰åŒ–ãªã—\n                if token == '[MASK]':\n                    highlighted_text.append(f\"\\033[38;5;245m{token}\\033[0m\")  # è–„ã„ã‚°ãƒ¬ãƒ¼\n                else:\n                    highlighted_text.append(token)\n        \n        # è¡¨ç¤º\n        print(f\"  {' '.join(highlighted_text)}\")\n        \n        # å¤‰åŒ–ã®è©³ç´°ã‚’è¡¨ç¤º\n        if show_details and changes:\n            change_summary = []\n            for pos, change_type, token in changes:\n                if change_type == 'revealed':\n                    change_summary.append(f\"pos{pos}:æ–°è¦({token})\")\n                elif change_type == 'masked':\n                    change_summary.append(f\"pos{pos}:ãƒã‚¹ã‚¯\")\n                elif change_type == 'changed':\n                    change_summary.append(f\"pos{pos}:å¤‰æ›´({token})\")\n            \n            if change_summary:\n                print(f\"    å¤‰åŒ–: {', '.join(change_summary)}\")\n        \n        print()\n        prev_tokens = curr_tokens\n\ndef visualize_step_by_step_animation(history, title=\"LLaDA Step-by-Step Animation\", delay=1.0):\n    \"\"\"\n    æ®µéšçš„ãªã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³é¢¨è¡¨ç¤ºï¼ˆJupyterç”¨ï¼‰\n    \"\"\"\n    import time\n    try:\n        from IPython.display import display, clear_output\n        use_jupyter = True\n    except ImportError:\n        use_jupyter = False\n    \n    print(f\"{title}\")\n    print(\"=\" * 50)\n    \n    for i, entry in enumerate(history):\n        if use_jupyter:\n            clear_output(wait=True)\n        \n        # ç¾åœ¨ã®ã‚¹ãƒ†ãƒƒãƒ—ã¾ã§ã‚’è¡¨ç¤º\n        print(f\"{title} - Step {i+1}/{len(history)}\")\n        print(\"=\" * 50)\n        \n        visualize_generation_with_highlights(history[:i+1], \n                                           title=f\"Progress: {i+1}/{len(history)} steps\", \n                                           show_details=False)\n        \n        if i < len(history) - 1:  # æœ€å¾Œã§ãªã‘ã‚Œã°å¾…æ©Ÿ\n            time.sleep(delay)\n\ndef visualize_block_progression(history):\n    \"\"\"\n    ãƒ–ãƒ­ãƒƒã‚¯åˆ¥ã®ç”Ÿæˆé€²æ—ã‚’å¯è¦–åŒ–ï¼ˆä¿®æ­£ç‰ˆï¼‰\n    \"\"\"\n    if not history:\n        print(\"å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“\")\n        return\n        \n    print(\"\\nãƒ–ãƒ­ãƒƒã‚¯åˆ¥ç”Ÿæˆé€²æ—:\")\n    print(\"=\" * 50)\n    \n    current_block = -1\n    for entry in history:\n        block = entry.get('block', 0)\n        if block != current_block:\n            current_block = block\n            print(f\"\\nBlock {current_block + 1}:\")\n            \n        step = entry['step']\n        text = entry['text']\n        num_revealed = entry['num_revealed']\n        avg_conf = entry['avg_confidence']\n        \n        # [MASK]ã‚’è¦–è¦šçš„ã«è¡¨ç¾\n        visual_text = text.replace('<|reserved_special_token_250|>', '[MASK]')\n        \n        print(f\"  Step {step+1:2d}: +{num_revealed:2d} tokens (conf={avg_conf:.3f})\")\n        print(f\"    çŠ¶æ…‹: {visual_text[:80]}{'...' if len(visual_text) > 80 else ''}\")\n\ndef compare_generation_steps(history_list, titles=None, highlight_differences=True):\n    \"\"\"\n    è¤‡æ•°ã®ç”Ÿæˆå±¥æ­´ã‚’ä¸¦ã¹ã¦æ¯”è¼ƒè¡¨ç¤º\n    \"\"\"\n    if titles is None:\n        titles = [f\"Generation {i+1}\" for i in range(len(history_list))]\n    \n    print(\"ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹æ¯”è¼ƒ\")\n    print(\"=\" * 80)\n    \n    max_steps = max(len(history) for history in history_list)\n    \n    for step in range(max_steps):\n        print(f\"\\nStep {step+1}:\")\n        print(\"-\" * 60)\n        \n        for i, (history, title) in enumerate(zip(history_list, titles)):\n            if step < len(history):\n                entry = history[step]\n                text = entry['text'].replace('<|reserved_special_token_250|>', '[MASK]')\n                conf = entry['avg_confidence']\n                revealed = entry['num_revealed']\n                \n                print(f\"  {title}: (+{revealed:2d}, conf={conf:.3f})\")\n                print(f\"    {text}\")\n            else:\n                print(f\"  {title}: [å®Œäº†]\")\n        print()\n\n# å‰å›ã®çµæœã‚’æ‹¡å¼µå¯è¦–åŒ–\nprint(\"æ‹¡å¼µå¯è¦–åŒ–æ©Ÿèƒ½ã®ãƒ‡ãƒ¢:\")\nif 'history' in locals():\n    visualize_generation_with_highlights(history, \"æ—¥æœ¬ã®é¦–éƒ½ã«é–¢ã™ã‚‹è³ªå•ã®ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ï¼ˆæ‹¡å¼µç‰ˆï¼‰\")\nelse:\n    print(\"å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã«ç”Ÿæˆå®Ÿé¨“ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ã‚ˆã‚Šè¤‡é›‘ãªè³ªå•ã§ã®å®Ÿé¨“"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# ã‚ˆã‚Šè¤‡é›‘ãªæ•°å­¦ã®å•é¡Œã‚’è©¦ã—ã¦ã¿ã‚‹\nmath_prompt = \"Solve this step by step: If a train travels 120 km in 1.5 hours, what is its average speed in km/h?\"\n\nprint(f\"æ•°å­¦å•é¡Œ: {math_prompt}\")\nprint(\"=\" * 70)\n\nmath_result, math_history = simple_generate(\n    model, math_prompt,\n    steps=24, gen_length=80, temperature=0.0\n)\n\nprint(\"=\" * 70)\nprint(f\"å›ç­”: {math_result}\")\n\n# æ–°æ©Ÿèƒ½ï¼šæ‹¡å¼µå¯è¦–åŒ–ã§ãƒ—ãƒ­ã‚»ã‚¹ã‚’è©³ç´°ã«è¡¨ç¤º\nprint(\"\\n\" + \"=\" * 40)\nprint(\"æ–°æ©Ÿèƒ½ãƒ‡ãƒ¢: ãƒã‚¤ãƒ©ã‚¤ãƒˆä»˜ãå¯è¦–åŒ–\")\nprint(\"=\" * 40)\n\nvisualize_generation_with_highlights(math_history[-8:], \"æ•°å­¦å•é¡Œã®è©³ç´°ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ï¼ˆæœ€å¾Œã®8ã‚¹ãƒ†ãƒƒãƒ—ï¼‰\")\n\n# ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³é¢¨è¡¨ç¤ºã®ãƒ‡ãƒ¢\nprint(\"\\n\" + \"=\" * 40)\nprint(\"æ–°æ©Ÿèƒ½ãƒ‡ãƒ¢: æ®µéšçš„ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³è¡¨ç¤º\")\nprint(\"=\" * 40)\nprint(\"æ³¨æ„: Jupyterç’°å¢ƒã§ã¯ç”»é¢ãŒã‚¯ãƒªã‚¢ã•ã‚Œã¦ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚ˆã†ã«è¡¨ç¤ºã•ã‚Œã¾ã™\")\n\n# æœ€å¾Œã®5ã‚¹ãƒ†ãƒƒãƒ—ã‚’ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³é¢¨ã«è¡¨ç¤º\nvisualize_step_by_step_animation(math_history[-5:], \n                                title=\"æ•°å­¦å•é¡Œç”Ÿæˆã®æœ€çµ‚æ®µéš\", \n                                delay=0.5)\n\n# å¾“æ¥ã®è¡¨ç¤ºã¨æ¯”è¼ƒ\nprint(\"\\n\" + \"=\" * 40)\nprint(\"å¾“æ¥è¡¨ç¤º vs æ‹¡å¼µè¡¨ç¤ºã®æ¯”è¼ƒ\")\nprint(\"=\" * 40)\n\nprint(\"å¾“æ¥ã®è¡¨ç¤º:\")\nvisualize_generation_process(math_history[-3:], \"å¾“æ¥ã®å¯è¦–åŒ–\")\n\nprint(\"\\næ‹¡å¼µè¡¨ç¤ºï¼ˆãƒã‚¤ãƒ©ã‚¤ãƒˆä»˜ãï¼‰:\")\nvisualize_generation_with_highlights(math_history[-3:], \"æ‹¡å¼µå¯è¦–åŒ–\", show_details=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿ã‚’èª¿ã¹ã‚‹\n",
    "\n",
    "LLaDAã®ç•°ãªã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒç”Ÿæˆçµæœã«ã©ã®ã‚ˆã†ãªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã‹ã‚’å®Ÿé¨“ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 ã‚¹ãƒ†ãƒƒãƒ—æ•°ã®å½±éŸ¿"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def compare_different_steps(prompt, step_counts=[8, 16, 32]):\n    \"\"\"\n    ç•°ãªã‚‹ã‚¹ãƒ†ãƒƒãƒ—æ•°ã§ã®ç”Ÿæˆçµæœã‚’æ¯”è¼ƒï¼ˆä¿®æ­£ç‰ˆï¼‰\n    \"\"\"\n    print(f\"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}\")\n    print(\"=\" * 60)\n    \n    results = {}\n    \n    for steps in step_counts:\n        print(f\"\\\\nğŸ“Š {steps}ã‚¹ãƒ†ãƒƒãƒ—ã§ã®ç”Ÿæˆ:\")\n        start_time = time.time()\n        \n        # ãƒ–ãƒ­ãƒƒã‚¯è¨­å®šã‚’èª¿æ•´ï¼šã‚¹ãƒ†ãƒƒãƒ—æ•°ã«å¿œã˜ã¦é©åˆ‡ãªãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºã‚’é¸æŠ\n        if steps <= 16:\n            block_length = 48  # 1ãƒ–ãƒ­ãƒƒã‚¯ã§å‡¦ç†\n        else:\n            block_length = 24  # 2ãƒ–ãƒ­ãƒƒã‚¯ã§å‡¦ç†\n        \n        result, history = generate_llada(\n            model, prompt,\n            steps=steps, gen_length=48, block_length=block_length,\n            temperature=0.0, cfg_scale=0.0, remasking='low_confidence'\n        )\n        \n        end_time = time.time()\n        \n        results[steps] = {\n            'text': result,\n            'time': end_time - start_time,\n            'history': history\n        }\n        \n        print(f\"çµæœ: {result}\")\n        print(f\"æ™‚é–“: {end_time - start_time:.2f}ç§’\")\n    \n    return results\n\ndef compare_advanced_settings(prompt):\n    \"\"\"\n    é«˜åº¦ãªè¨­å®šã§ã®æ¯”è¼ƒå®Ÿé¨“ï¼ˆä¿®æ­£ç‰ˆï¼‰\n    \"\"\"\n    print(f\"\\\\nğŸ”¬ é«˜åº¦ãªè¨­å®šæ¯”è¼ƒ\")\n    print(f\"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}\")\n    print(\"=\" * 60)\n    \n    settings = [\n        {\n            \"name\": \"æ¨™æº–è¨­å®š\",\n            \"steps\": 16, \"block_length\": 64, \"temperature\": 0.0, \n            \"cfg_scale\": 0.0, \"remasking\": \"low_confidence\"\n        },\n        {\n            \"name\": \"é«˜å“è³ªè¨­å®š\",\n            \"steps\": 32, \"block_length\": 32, \"temperature\": 0.0, \n            \"cfg_scale\": 0.0, \"remasking\": \"low_confidence\"\n        },\n        {\n            \"name\": \"å‰µé€ çš„è¨­å®š\",\n            \"steps\": 24, \"block_length\": 32, \"temperature\": 0.3, \n            \"cfg_scale\": 0.0, \"remasking\": \"low_confidence\"\n        },\n        {\n            \"name\": \"CFGå¼·åŒ–è¨­å®š\",\n            \"steps\": 20, \"block_length\": 32, \"temperature\": 0.0, \n            \"cfg_scale\": 1.0, \"remasking\": \"low_confidence\"\n        },\n        {\n            \"name\": \"ãƒ©ãƒ³ãƒ€ãƒ å†ãƒã‚¹ã‚¯\",\n            \"steps\": 16, \"block_length\": 64, \"temperature\": 0.0, \n            \"cfg_scale\": 0.0, \"remasking\": \"random\"\n        }\n    ]\n    \n    results = []\n    \n    for setting in settings:\n        print(f\"\\\\nğŸ§ª {setting['name']}:\")\n        print(f\"  è¨­å®š: steps={setting['steps']}, block={setting['block_length']}, \"\n              f\"temp={setting['temperature']}, cfg={setting['cfg_scale']}, \"\n              f\"remasking={setting['remasking']}\")\n        \n        start_time = time.time()\n        result, history = generate_llada(\n            model, prompt,\n            steps=setting['steps'],\n            gen_length=64,\n            block_length=setting['block_length'],\n            temperature=setting['temperature'],\n            cfg_scale=setting['cfg_scale'],\n            remasking=setting['remasking']\n        )\n        generation_time = time.time() - start_time\n        \n        avg_confidence = np.mean([h['avg_confidence'] for h in history if h['avg_confidence'] > 0])\n        \n        results.append({\n            'name': setting['name'],\n            'result': result,\n            'time': generation_time,\n            'avg_confidence': avg_confidence,\n            'setting': setting\n        })\n        \n        print(f\"  çµæœ: {result}\")\n        print(f\"  æ™‚é–“: {generation_time:.2f}ç§’, å¹³å‡ä¿¡é ¼åº¦: {avg_confidence:.3f}\")\n    \n    return results\n\n# å‰µä½œçš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§å®Ÿé¨“\ncreative_prompt = \"Write a haiku about artificial intelligence.\"\nstep_results = compare_different_steps(creative_prompt)\n\n# é«˜åº¦ãªè¨­å®šã§ã®æ¯”è¼ƒ\nadvanced_results = compare_advanced_settings(\"Tell me a short story about a time-traveling scientist.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 æ¸©åº¦ï¼ˆTemperatureï¼‰ã®å½±éŸ¿"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def compare_temperatures_with_highlights(prompt, temperatures=[0.0, 0.3, 0.7]):\n    \"\"\"\n    ç•°ãªã‚‹æ¸©åº¦è¨­å®šã§ã®ç”Ÿæˆçµæœã‚’æ‹¡å¼µå¯è¦–åŒ–ã§æ¯”è¼ƒ\n    \"\"\"\n    print(f\"æ¸©åº¦æ¯”è¼ƒå®Ÿé¨“ï¼ˆæ‹¡å¼µå¯è¦–åŒ–ï¼‰\")\n    print(f\"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}\")\n    print(\"=\" * 60)\n    \n    all_histories = []\n    all_titles = []\n    \n    for temp in temperatures:\n        print(f\"\\næ¸©åº¦ {temp}ã§ã®ç”Ÿæˆ:\")\n        \n        result, history = simple_generate(\n            model, prompt,\n            steps=16, gen_length=40, temperature=temp\n        )\n        \n        print(f\"çµæœ: {result}\")\n        \n        # ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤º\n        print(f\"\\næ¸©åº¦{temp}ã®ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹:\")\n        visualize_generation_with_highlights(history[-5:], \n                                           title=f\"æ¸©åº¦{temp}ã§ã®ç”Ÿæˆï¼ˆæœ€å¾Œã®5ã‚¹ãƒ†ãƒƒãƒ—ï¼‰\", \n                                           show_details=False)\n        \n        all_histories.append(history[-3:])  # æœ€å¾Œã®3ã‚¹ãƒ†ãƒƒãƒ—ã‚’ä¿å­˜\n        all_titles.append(f\"æ¸©åº¦{temp}\")\n        \n        if temp == 0.0:\n            print(\"  â†’ æ±ºå®šè«–çš„ï¼ˆæ¯å›åŒã˜çµæœï¼‰\")\n        elif temp < 0.5:\n            print(\"  â†’ ä½ãƒ©ãƒ³ãƒ€ãƒ æ€§ï¼ˆå®‰å®šã—ãŸçµæœï¼‰\")\n        else:\n            print(\"  â†’ é«˜ãƒ©ãƒ³ãƒ€ãƒ æ€§ï¼ˆå‰µé€ çš„ã ãŒä¸å®‰å®šï¼‰\")\n    \n    # è¤‡æ•°æ¸©åº¦ã®æ¯”è¼ƒè¡¨ç¤º\n    print(f\"\\næ¸©åº¦åˆ¥ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹æ¯”è¼ƒ:\")\n    compare_generation_steps(all_histories, all_titles)\n\ndef demonstrate_new_visualization_features():\n    \"\"\"\n    æ–°ã—ã„å¯è¦–åŒ–æ©Ÿèƒ½ã®åŒ…æ‹¬çš„ãªãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n    \"\"\"\n    print(\"LLaDAæ‹¡å¼µå¯è¦–åŒ–æ©Ÿèƒ½ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\")\n    print(\"=\" * 60)\n    \n    # ãƒ‡ãƒ¢ç”¨ã®çŸ­ã„ç”Ÿæˆå®Ÿé¨“\n    demo_prompt = \"What makes a good leader?\"\n    \n    print(f\"ãƒ‡ãƒ¢ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {demo_prompt}\")\n    print(\"ç”Ÿæˆè¨­å®š: 16ã‚¹ãƒ†ãƒƒãƒ—, 48ãƒˆãƒ¼ã‚¯ãƒ³, æ¸©åº¦0.2\")\n    print(\"-\" * 50)\n    \n    demo_result, demo_history = generate_llada(\n        model, demo_prompt,\n        steps=16, gen_length=48, block_length=24,\n        temperature=0.2, cfg_scale=0.0, remasking='low_confidence'\n    )\n    \n    print(f\"\\nç”Ÿæˆçµæœ: {demo_result}\")\n    \n    # æ©Ÿèƒ½1: ãƒã‚¤ãƒ©ã‚¤ãƒˆä»˜ãå¯è¦–åŒ–\n    print(f\"\\n\" + \"=\" * 30)\n    print(\"æ©Ÿèƒ½1: ãƒã‚¤ãƒ©ã‚¤ãƒˆä»˜ãå¯è¦–åŒ–\")\n    print(\"=\" * 30)\n    visualize_generation_with_highlights(demo_history, \n                                       \"ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—ã«é–¢ã™ã‚‹ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹\", \n                                       show_details=True)\n    \n    # æ©Ÿèƒ½2: ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³é¢¨è¡¨ç¤º\n    print(f\"\\n\" + \"=\" * 30)\n    print(\"æ©Ÿèƒ½2: ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³é¢¨è¡¨ç¤ºï¼ˆæœ€å¾Œã®6ã‚¹ãƒ†ãƒƒãƒ—ï¼‰\")\n    print(\"=\" * 30)\n    visualize_step_by_step_animation(demo_history[-6:], \n                                   title=\"ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã®ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³\", \n                                   delay=0.3)\n    \n    # æ©Ÿèƒ½3: ä¿¡é ¼åº¦ã¨ãƒ–ãƒ­ãƒƒã‚¯åˆ†æ\n    print(f\"\\n\" + \"=\" * 30)\n    print(\"æ©Ÿèƒ½3: ãƒ–ãƒ­ãƒƒã‚¯åˆ¥è©³ç´°åˆ†æ\")\n    print(\"=\" * 30)\n    visualize_block_progression(demo_history)\n    \n    return demo_result, demo_history\n\n# ç‰©èªã®å§‹ã¾ã‚Šã‚’æ‹¡å¼µå¯è¦–åŒ–ã§ç”Ÿæˆ\nstory_prompt = \"Once upon a time, in a small village\"\ncompare_temperatures_with_highlights(story_prompt)\n\n# åŒ…æ‹¬çš„ãªãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\nprint(\"\\n\" + \"=\" * 50)\nprint(\"æ–°æ©Ÿèƒ½ã®åŒ…æ‹¬çš„ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\")\nprint(\"=\" * 50)\n\ndemo_result, demo_history = demonstrate_new_visualization_features()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## æ–°æ©Ÿèƒ½: æ‹¡å¼µå¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ \n\nã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€LLaDAã®ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã‚’ã‚ˆã‚Šæ·±ãç†è§£ã™ã‚‹ãŸã‚ã®æ‹¡å¼µå¯è¦–åŒ–æ©Ÿèƒ½ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚\n\n### è¿½åŠ ã•ã‚ŒãŸä¸»è¦æ©Ÿèƒ½\n\n#### 1. **ãƒã‚¤ãƒ©ã‚¤ãƒˆä»˜ãå¯è¦–åŒ–** (`visualize_generation_with_highlights()`)\n- **æ–°ã—ãç”Ÿæˆã•ã‚ŒãŸå˜èª**ã‚’å¤ªå­—ã¨è‰²ä»˜ãã§å¼·èª¿è¡¨ç¤º\n- **ä¿¡é ¼åº¦ãƒ¬ãƒ™ãƒ«**ã«å¿œã˜ãŸè½ã¡ç€ã„ãŸè‰²åˆ†ã‘ï¼š\n  - æ·±ã„ç·‘: é«˜ä¿¡é ¼åº¦ (â‰¥0.8)\n  - ã‚ªãƒªãƒ¼ãƒ–: ä¸­ä¿¡é ¼åº¦ (â‰¥0.6)  \n  - ãƒ–ãƒ©ã‚¦ãƒ³: ä½ä¿¡é ¼åº¦ (â‰¥0.4)\n  - ã‚°ãƒ¬ãƒ¼: æ¥µä½ä¿¡é ¼åº¦ (<0.4)\n- **å¤‰åŒ–ã®è©³ç´°**ï¼šä½ç½®åˆ¥ã®å¤‰æ›´å±¥æ­´ã‚’è¡¨ç¤º\n\n#### 2. **æ®µéšçš„ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³è¡¨ç¤º** (`visualize_step_by_step_animation()`)\n- Jupyterç’°å¢ƒã§**ç”»é¢ã‚’ã‚¯ãƒªã‚¢**ã—ã¦å‹•çš„è¡¨ç¤º\n- å„ã‚¹ãƒ†ãƒƒãƒ—ã®é€²è¡Œã‚’**æ™‚ç³»åˆ—ã§å¯è¦–åŒ–**\n- èª¿æ•´å¯èƒ½ãªè¡¨ç¤ºé–“éš”ï¼ˆ`delay`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰\n\n#### 3. **æ¯”è¼ƒè¡¨ç¤ºæ©Ÿèƒ½** (`compare_generation_steps()`)\n- **è¤‡æ•°ã®ç”Ÿæˆçµæœ**ã‚’ä¸¦ã¹ã¦æ¯”è¼ƒ\n- ç•°ãªã‚‹è¨­å®šï¼ˆæ¸©åº¦ã€CFGãªã©ï¼‰ã®å½±éŸ¿ã‚’åˆ†æ\n- ã‚¹ãƒ†ãƒƒãƒ—åˆ¥ã®å·®ç•°ã‚’æ˜ç¢ºåŒ–\n\n#### 4. **ãƒˆãƒ¼ã‚¯ãƒ³å·®åˆ†æ¤œå‡º** (`detect_token_changes()`)\n- å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã¨ã®**è‡ªå‹•æ¯”è¼ƒ**\n- å¤‰åŒ–ã‚¿ã‚¤ãƒ—ã®åˆ†é¡ï¼š\n  - `revealed`: [MASK] â†’ å®Ÿéš›ã®å˜èªï¼ˆ**å¤ªå­—**è¡¨ç¤ºï¼‰\n  - `masked`: å®Ÿéš›ã®å˜èª â†’ [MASK]ï¼ˆè–„å­—è¡¨ç¤ºï¼‰\n  - `changed`: å˜èª â†’ åˆ¥ã®å˜èªï¼ˆ**ä¸‹ç·š**è¡¨ç¤ºï¼‰\n\n### ä½¿ç”¨æ–¹æ³•\n\n```python\n# åŸºæœ¬çš„ãªãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤º\nvisualize_generation_with_highlights(history, \"ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹\")\n\n# ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³é¢¨è¡¨ç¤ºï¼ˆJupyteræ¨å¥¨ï¼‰\nvisualize_step_by_step_animation(history, delay=1.0)\n\n# è¤‡æ•°çµæœã®æ¯”è¼ƒ\ncompare_generation_steps([history1, history2], [\"è¨­å®šA\", \"è¨­å®šB\"])\n```\n\n### æ´»ç”¨ã®ãƒ’ãƒ³ãƒˆ\n\n1. **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´æ™‚**: ç•°ãªã‚‹è¨­å®šã§ã®ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã‚’æ¯”è¼ƒ\n2. **ãƒ‡ãƒãƒƒã‚°æ™‚**: äºˆæœŸã—ãªã„ç”Ÿæˆçµæœã®åŸå› ã‚’ç‰¹å®š\n3. **å­¦ç¿’æ™‚**: æ‹¡æ•£ãƒ—ãƒ­ã‚»ã‚¹ã®ç†è§£ã‚’æ·±ã‚ã‚‹\n4. **ãƒ—ãƒ¬ã‚¼ãƒ³æ™‚**: æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®å‹•ä½œã‚’è¦–è¦šçš„ã«èª¬æ˜\n\n### å®Ÿè·µä¾‹\n\nä»¥ä¸‹ã®ã‚»ãƒ«ã§ã¯ã€ã“ã‚Œã‚‰ã®æ–°æ©Ÿèƒ½ã‚’å®Ÿéš›ã«ä½“é¨“ã§ãã¾ã™ï¼š",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def analyze_confidence_over_steps(history):\n    \"\"\"\n    ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®ä¿¡é ¼åº¦ã®å¤‰åŒ–ã‚’ã‚°ãƒ©ãƒ•åŒ–\n    \"\"\"\n    global_steps = [entry['global_step'] + 1 for entry in history]\n    confidences = [entry['avg_confidence'] for entry in history]\n    tokens_revealed = [entry['num_revealed'] for entry in history]\n    blocks = [entry['block'] for entry in history]\n    \n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n    \n    # ä¿¡é ¼åº¦ã®å¤‰åŒ–ï¼ˆãƒ–ãƒ­ãƒƒã‚¯åˆ¥ã«è‰²åˆ†ã‘ï¼‰\n    unique_blocks = sorted(set(blocks))\n    colors = plt.cm.Set1(np.linspace(0, 1, len(unique_blocks)))\n    \n    for i, block in enumerate(unique_blocks):\n        block_mask = [b == block for b in blocks]\n        block_steps = [s for s, m in zip(global_steps, block_mask) if m]\n        block_confs = [c for c, m in zip(confidences, block_mask) if m]\n        \n        ax1.plot(block_steps, block_confs, 'o-', color=colors[i], \n                linewidth=2, markersize=6, label=f'ãƒ–ãƒ­ãƒƒã‚¯ {block+1}')\n    \n    ax1.set_xlabel('Global Generation Step')\n    ax1.set_ylabel('Average Confidence')\n    ax1.set_title('Confidence Score Over Generation Steps (by Block)')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # å„ã‚¹ãƒ†ãƒƒãƒ—ã§æ˜ã‹ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³æ•°\n    bars = ax2.bar(global_steps, tokens_revealed, alpha=0.7)\n    \n    # ãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã«è‰²åˆ†ã‘\n    for i, (bar, block) in enumerate(zip(bars, blocks)):\n        bar.set_color(colors[block])\n    \n    ax2.set_xlabel('Global Generation Step')\n    ax2.set_ylabel('Tokens Revealed')\n    ax2.set_title('Number of Tokens Revealed per Step')\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # çµ±è¨ˆæƒ…å ±\n    print(f\"å¹³å‡ä¿¡é ¼åº¦: {np.mean(confidences):.3f}\")\n    print(f\"ä¿¡é ¼åº¦ã®æ¨™æº–åå·®: {np.std(confidences):.3f}\")\n    print(f\"ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {sum(tokens_revealed)}\")\n    print(f\"ãƒ–ãƒ­ãƒƒã‚¯æ•°: {len(unique_blocks)}\")\n\ndef visualize_block_progression(history):\n    \"\"\"\n    ãƒ–ãƒ­ãƒƒã‚¯åˆ¥ã®ç”Ÿæˆé€²æ—ã‚’å¯è¦–åŒ–\n    \"\"\"\n    if not history:\n        print(\"å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“\")\n        return\n        \n    print(\"\\\\nğŸ” ãƒ–ãƒ­ãƒƒã‚¯åˆ¥ç”Ÿæˆé€²æ—:\")\n    print(\"=\" * 50)\n    \n    current_block = -1\n    for entry in history:\n        if entry['block'] != current_block:\n            current_block = entry['block']\n            print(f\"\\\\nğŸ“¦ ãƒ–ãƒ­ãƒƒã‚¯ {current_block + 1}:\")\n            \n        step = entry['step']\n        text = entry['text']\n        num_revealed = entry['num_revealed']\n        avg_conf = entry['avg_confidence']\n        \n        # [MASK]ã‚’è¦–è¦šçš„ã«è¡¨ç¾\n        visual_text = text.replace('<|reserved_special_token_250|>', '[MASK]')\n        \n        print(f\"  Step {step+1:2d}: +{num_revealed:2d} tokens (conf={avg_conf:.3f})\")\n        print(f\"    çŠ¶æ…‹: {visual_text[:80]}{'...' if len(visual_text) > 80 else ''}\")\n\n# å‰å›ã®æ•°å­¦å•é¡Œã®çµæœã‚’åˆ†æ\nif 'math_history' in locals():\n    print(\"æ•°å­¦å•é¡Œã®ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹åˆ†æ:\")\n    analyze_confidence_over_steps(math_history)\n    visualize_block_progression(math_history[-10:])  # æœ€å¾Œã®10ã‚¹ãƒ†ãƒƒãƒ—\nelse:\n    print(\"åˆ†æç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­...\")\n    _, analysis_history = generate_llada(\n        model, \"Explain how photosynthesis works in plants.\",\n        steps=24, gen_length=80, block_length=20,\n        temperature=0.0, cfg_scale=0.0, remasking='low_confidence'\n    )\n    analyze_confidence_over_steps(analysis_history)\n    visualize_block_progression(analysis_history[-8:])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒ\n",
    "\n",
    "æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã¨è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã®é•ã„ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ã€LLaDAã®ç‰¹å¾´ã‚’æ•´ç†ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_generation_strategies():\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆæˆ¦ç•¥ã®é•ã„ã‚’è¦–è¦šçš„ã«èª¬æ˜\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“Š ç”Ÿæˆæˆ¦ç•¥ã®æ¯”è¼ƒ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ï¼ˆGPTé¢¨ï¼‰ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "    print(\"ğŸ”„ è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ« (GPT/LLaMA):\")\n",
    "    autoregressive_steps = [\n",
    "        \"The\",\n",
    "        \"The capital\",\n",
    "        \"The capital of\",\n",
    "        \"The capital of Japan\",\n",
    "        \"The capital of Japan is\",\n",
    "        \"The capital of Japan is Tokyo\"\n",
    "    ]\n",
    "    \n",
    "    for i, step in enumerate(autoregressive_steps):\n",
    "        print(f\"  Step {i+1}: {step}\")\n",
    "    \n",
    "    print(\"\\n  ç‰¹å¾´:\")\n",
    "    print(\"  âœ“ å·¦ã‹ã‚‰å³ã¸é †æ¬¡ç”Ÿæˆ\")\n",
    "    print(\"  âœ“ å„ãƒˆãƒ¼ã‚¯ãƒ³ã¯éå»ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿å‚ç…§\")\n",
    "    print(\"  âœ“ é«˜é€Ÿï¼ˆKV-cacheãŒä½¿ãˆã‚‹ï¼‰\")\n",
    "    print(\"  âœ— å¾Œç¶šã®æ–‡è„ˆã‚’è€ƒæ…®ã§ããªã„\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ï¼ˆLLaDAï¼‰\n",
    "    print(\"ğŸŒ€ æ‹¡æ•£ãƒ¢ãƒ‡ãƒ« (LLaDA):\")\n",
    "    diffusion_steps = [\n",
    "        \"[MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\",\n",
    "        \"The [MASK] [MASK] Japan [MASK] [MASK]\",\n",
    "        \"The capital [MASK] Japan [MASK] Tokyo\",\n",
    "        \"The capital of Japan is Tokyo\"\n",
    "    ]\n",
    "    \n",
    "    for i, step in enumerate(diffusion_steps):\n",
    "        print(f\"  Step {i+1}: {step}\")\n",
    "    \n",
    "    print(\"\\n  ç‰¹å¾´:\")\n",
    "    print(\"  âœ“ å…¨ä½“æ§‹é€ ã‹ã‚‰è©³ç´°ã¸\")\n",
    "    print(\"  âœ“ åŒæ–¹å‘ã®æ–‡è„ˆã‚’è€ƒæ…®\")\n",
    "    print(\"  âœ“ åå¾©çš„ãªå“è³ªæ”¹å–„\")\n",
    "    print(\"  âœ— æ¯”è¼ƒçš„ä½é€Ÿï¼ˆKV-cacheä½¿ç”¨ä¸å¯ï¼‰\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"\\nğŸ’¡ ä¸»ãªé•ã„:\")\n",
    "    print(\"1. ç”Ÿæˆæ–¹å‘: è‡ªå·±å›å¸°ã¯ä¸€æ–¹å‘ã€æ‹¡æ•£ã¯åŒæ–¹å‘\")\n",
    "    print(\"2. å“è³ªå‘ä¸Š: æ‹¡æ•£ã¯è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—ã§åå¾©æ”¹å–„\")\n",
    "    print(\"3. è¨ˆç®—åŠ¹ç‡: è‡ªå·±å›å¸°ãŒé«˜é€Ÿã€æ‹¡æ•£ã¯é«˜å“è³ª\")\n",
    "    print(\"4. åˆ¶å¾¡æ€§: æ‹¡æ•£ã¯ã‚ˆã‚Šç´°ã‹ã„åˆ¶å¾¡ãŒå¯èƒ½\")\n",
    "\n",
    "compare_generation_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å®Ÿéš›ã®æ€§èƒ½æ¯”è¼ƒå®Ÿé¨“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_analysis(prompts, steps_list=[8, 16, 32]):\n",
    "    \"\"\"\n",
    "    ç•°ãªã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã‚¹ãƒ†ãƒƒãƒ—æ•°ã§ã®æ€§èƒ½åˆ†æ\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"\\nğŸ” åˆ†æä¸­: {prompt[:50]}...\")\n",
    "        \n",
    "        for steps in steps_list:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            result, history = simple_generate(\n",
    "                model, prompt,\n",
    "                steps=steps, gen_length=40, temperature=0.0\n",
    "            )\n",
    "            \n",
    "            generation_time = time.time() - start_time\n",
    "            avg_confidence = np.mean([h['avg_confidence'] for h in history])\n",
    "            \n",
    "            results.append({\n",
    "                'prompt': prompt,\n",
    "                'steps': steps,\n",
    "                'time': generation_time,\n",
    "                'avg_confidence': avg_confidence,\n",
    "                'result': result\n",
    "            })\n",
    "            \n",
    "            print(f\"  {steps:2d}ã‚¹ãƒ†ãƒƒãƒ—: {generation_time:.2f}ç§’, ä¿¡é ¼åº¦{avg_confidence:.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ç•°ãªã‚‹ã‚¿ã‚¤ãƒ—ã®è³ªå•ã§æ€§èƒ½ãƒ†ã‚¹ãƒˆ\n",
    "test_prompts = [\n",
    "    \"What is 15 + 27?\",  # ç°¡å˜ãªè¨ˆç®—\n",
    "    \"Explain the water cycle.\",  # èª¬æ˜æ–‡\n",
    "    \"Write a short poem about spring.\",  # å‰µä½œ\n",
    "]\n",
    "\n",
    "perf_results = performance_analysis(test_prompts, [8, 16, 24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# çµæœã®å¯è¦–åŒ–\n",
    "def visualize_performance_results(results):\n",
    "    # ãƒ‡ãƒ¼ã‚¿ã®æ•´ç†\n",
    "    steps_data = {}\n",
    "    for result in results:\n",
    "        steps = result['steps']\n",
    "        if steps not in steps_data:\n",
    "            steps_data[steps] = {'times': [], 'confidences': []}\n",
    "        steps_data[steps]['times'].append(result['time'])\n",
    "        steps_data[steps]['confidences'].append(result['avg_confidence'])\n",
    "    \n",
    "    # ã‚°ãƒ©ãƒ•ä½œæˆ\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    steps_list = sorted(steps_data.keys())\n",
    "    avg_times = [np.mean(steps_data[s]['times']) for s in steps_list]\n",
    "    avg_confidences = [np.mean(steps_data[s]['confidences']) for s in steps_list]\n",
    "    \n",
    "    # ç”Ÿæˆæ™‚é–“\n",
    "    ax1.bar(steps_list, avg_times, color='skyblue', alpha=0.7)\n",
    "    ax1.set_xlabel('Number of Steps')\n",
    "    ax1.set_ylabel('Average Generation Time (seconds)')\n",
    "    ax1.set_title('Generation Time vs Steps')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # å¹³å‡ä¿¡é ¼åº¦\n",
    "    ax2.bar(steps_list, avg_confidences, color='lightcoral', alpha=0.7)\n",
    "    ax2.set_xlabel('Number of Steps')\n",
    "    ax2.set_ylabel('Average Confidence Score')\n",
    "    ax2.set_title('Confidence vs Steps')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ æ€§èƒ½åˆ†æçµæœ:\")\n",
    "    for steps in steps_list:\n",
    "        avg_time = np.mean(steps_data[steps]['times'])\n",
    "        avg_conf = np.mean(steps_data[steps]['confidences'])\n",
    "        print(f\"  {steps:2d}ã‚¹ãƒ†ãƒƒãƒ—: å¹³å‡{avg_time:.2f}ç§’, ä¿¡é ¼åº¦{avg_conf:.3f}\")\n",
    "\n",
    "visualize_performance_results(perf_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ã¾ã¨ã‚ã¨ç™ºå±•çš„ãªå®Ÿé¨“\n",
    "\n",
    "### å­¦ã‚“ã ã“ã¨\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’é€šã˜ã¦ã€ä»¥ä¸‹ã®ã‚ˆã†ãªæ‹¡æ•£è¨€èªãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´ã‚’å­¦ã³ã¾ã—ãŸï¼š\n",
    "\n",
    "1. **æ‹¡æ•£ãƒ—ãƒ­ã‚»ã‚¹**: [MASK]ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰å§‹ã¾ã‚Šã€æ®µéšçš„ã«å®Ÿéš›ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ˜ã‹ã—ã¦ã„ã\n",
    "2. **ä¿¡é ¼åº¦ãƒ™ãƒ¼ã‚¹é¸æŠ**: ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ä¿¡é ¼åº¦ã«åŸºã¥ã„ã¦ã€ã©ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç¢ºå®šã™ã‚‹ã‹ã‚’æ±ºå®š\n",
    "3. **åŒæ–¹å‘æ–‡è„ˆ**: å¾“æ¥ã®è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã¨ç•°ãªã‚Šã€å…¨ä½“ã®æ–‡è„ˆã‚’è€ƒæ…®ã§ãã‚‹\n",
    "4. **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ¶å¾¡**: ã‚¹ãƒ†ãƒƒãƒ—æ•°ã€æ¸©åº¦ãªã©ã§ç”Ÿæˆã®å“è³ªã¨å¤šæ§˜æ€§ã‚’èª¿æ•´å¯èƒ½\n",
    "\n",
    "### ç™ºå±•çš„ãªå®Ÿé¨“\n",
    "\n",
    "ä»¥ä¸‹ã®ã‚ˆã†ãªå®Ÿé¨“ã‚’ã•ã‚‰ã«è©¦ã—ã¦ã¿ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_experiments():\n",
    "    \"\"\"\n",
    "    ç™ºå±•çš„ãªå®Ÿé¨“ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’æç¤º\n",
    "    \"\"\"\n",
    "    experiments = [\n",
    "        {\n",
    "            \"title\": \"ğŸ¯ åˆ¶ç´„ä»˜ãç”Ÿæˆ\",\n",
    "            \"description\": \"ç‰¹å®šã®å˜èªã‚’ç‰¹å®šã®ä½ç½®ã«é…ç½®ã—ãŸç”Ÿæˆ\",\n",
    "            \"example\": \"ä½ç½®3ã«'beautiful'ã€ä½ç½®7ã«'sunset'ã‚’æŒ‡å®š\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"ğŸ”„ å†ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥\",\n",
    "            \"description\": \"'low_confidence'ã¨'random'ã®é•ã„ã‚’æ¯”è¼ƒ\",\n",
    "            \"example\": \"åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§æˆ¦ç•¥ã‚’å¤‰ãˆã¦çµæœã‚’æ¯”è¼ƒ\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"ğŸ“Š é•·æ–‡ç”Ÿæˆ\",\n",
    "            \"description\": \"é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã§ã®æ‹¡æ•£ãƒ—ãƒ­ã‚»ã‚¹ã®è¦³å¯Ÿ\",\n",
    "            \"example\": \"200ãƒˆãƒ¼ã‚¯ãƒ³ä»¥ä¸Šã®ç‰©èªã‚„è¨˜äº‹ã®ç”Ÿæˆ\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"ğŸŒ¡ï¸ æ¸©åº¦ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°\",\n",
    "            \"description\": \"ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«æ¸©åº¦ã‚’å¤‰åŒ–ã•ã›ã‚‹\",\n",
    "            \"example\": \"åˆæœŸã¯é«˜æ¸©åº¦ã€å¾ŒæœŸã¯ä½æ¸©åº¦ã§å®‰å®šåŒ–\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"ğŸ¨ å‰µä½œæ€§è©•ä¾¡\",\n",
    "            \"description\": \"è©©ã‚„ç‰©èªã®å‰µä½œã§ã®æ‹¡æ•£ã®åŠ¹æœ\",\n",
    "            \"example\": \"åŒã˜ãƒ†ãƒ¼ãƒã§è¤‡æ•°å›ç”Ÿæˆã—ã¦å¤šæ§˜æ€§ã‚’æ¸¬å®š\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸš€ ç™ºå±•çš„ãªå®Ÿé¨“ã‚¢ã‚¤ãƒ‡ã‚¢\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, exp in enumerate(experiments, 1):\n",
    "        print(f\"\\n{i}. {exp['title']}\")\n",
    "        print(f\"   æ¦‚è¦: {exp['description']}\")\n",
    "        print(f\"   ä¾‹: {exp['example']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ğŸ’¡ å®Ÿé¨“ã®ãƒ’ãƒ³ãƒˆ:\")\n",
    "    print(\"- ç•°ãªã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆæ•°å­¦ã€ç§‘å­¦ã€æ–‡å­¦ï¼‰ã§ã®æ€§èƒ½ã‚’æ¯”è¼ƒ\")\n",
    "    print(\"- ç”Ÿæˆå“è³ªã®å®šé‡çš„è©•ä¾¡æŒ‡æ¨™ã‚’è€ƒæ¡ˆ\")\n",
    "    print(\"- ä»–ã®æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒç ”ç©¶\")\n",
    "    print(\"- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯è¦–åŒ–ãƒ„ãƒ¼ãƒ«ã®é–‹ç™º\")\n",
    "\n",
    "advanced_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç°¡å˜ãªç™ºå±•å®Ÿé¨“: å‰µä½œæ€§ã®æ¯”è¼ƒ"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def creativity_experiment():\n    \"\"\"\n    å‰µä½œçš„ãªã‚¿ã‚¹ã‚¯ã§ã®æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´ã‚’èª¿ã¹ã‚‹\n    \"\"\"\n    creative_prompt = \"Write a creative story about a robot discovering emotions.\"\n    \n    print(\"ğŸ¨ å‰µä½œæ€§å®Ÿé¨“\")\n    print(\"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:\", creative_prompt)\n    print(\"=\" * 60)\n    \n    # ç•°ãªã‚‹è¨­å®šã§ç”Ÿæˆ\n    settings = [\n        {\n            \"steps\": 16, \"block_length\": 40, \"temperature\": 0.0, \n            \"cfg_scale\": 0.0, \"remasking\": \"low_confidence\",\n            \"name\": \"æ±ºå®šè«–çš„ãƒ»é«˜é€Ÿ\"\n        },\n        {\n            \"steps\": 32, \"block_length\": 20, \"temperature\": 0.0, \n            \"cfg_scale\": 0.0, \"remasking\": \"low_confidence\",\n            \"name\": \"æ±ºå®šè«–çš„ãƒ»é«˜å“è³ª\"\n        },\n        {\n            \"steps\": 24, \"block_length\": 30, \"temperature\": 0.4, \n            \"cfg_scale\": 0.0, \"remasking\": \"low_confidence\",\n            \"name\": \"å‰µé€ çš„ãƒ»ä¸­å“è³ª\"\n        },\n        {\n            \"steps\": 20, \"block_length\": 25, \"temperature\": 0.2, \n            \"cfg_scale\": 1.5, \"remasking\": \"low_confidence\",\n            \"name\": \"CFGå¼·åŒ–ãƒ»å‰µé€ çš„\"\n        },\n        {\n            \"steps\": 18, \"block_length\": 35, \"temperature\": 0.0, \n            \"cfg_scale\": 0.0, \"remasking\": \"random\",\n            \"name\": \"ãƒ©ãƒ³ãƒ€ãƒ å†ãƒã‚¹ã‚¯\"\n        }\n    ]\n    \n    results = []\n    \n    for setting in settings:\n        print(f\"\\\\nğŸ“ {setting['name']}:\")\n        print(f\"  è¨­å®š: steps={setting['steps']}, block={setting['block_length']}, \"\n              f\"temp={setting['temperature']}, cfg={setting['cfg_scale']}, \"\n              f\"remasking={setting['remasking']}\")\n        \n        start_time = time.time()\n        result, history = generate_llada(\n            model, creative_prompt,\n            steps=setting['steps'],\n            gen_length=80,\n            block_length=setting['block_length'],\n            temperature=setting['temperature'],\n            cfg_scale=setting['cfg_scale'],\n            remasking=setting['remasking']\n        )\n        generation_time = time.time() - start_time\n        \n        avg_confidence = np.mean([h['avg_confidence'] for h in history])\n        \n        results.append({\n            'setting': setting['name'],\n            'text': result,\n            'time': generation_time,\n            'avg_confidence': avg_confidence,\n            'parameters': setting\n        })\n        \n        print(f\"  çµæœ: {result}\")\n        print(f\"  æ™‚é–“: {generation_time:.2f}ç§’, å¹³å‡ä¿¡é ¼åº¦: {avg_confidence:.3f}\")\n    \n    print(\"\\\\n\" + \"=\" * 60)\n    print(\"ğŸ” åˆ†æ:\")\n    print(\"- æ¸©åº¦0.0ã§ã¯ä¸€è²«ã—ãŸçµæœãŒå¾—ã‚‰ã‚Œã‚‹\")\n    print(\"- ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’å¢—ã‚„ã™ã“ã¨ã§è©³ç´°åº¦ãŒå‘ä¸Š\")\n    print(\"- é©åº¦ãªæ¸©åº¦ã¯å‰µé€ æ€§ã‚’é«˜ã‚ã‚‹å¯èƒ½æ€§\")\n    print(\"- CFGã¯æ¡ä»¶ä»˜ãç”Ÿæˆã®å“è³ªã‚’å‘ä¸Š\")\n    print(\"- ãƒ©ãƒ³ãƒ€ãƒ å†ãƒã‚¹ã‚¯ã¯äºˆæœŸã—ãªã„å‰µé€ æ€§ã‚’æä¾›\")\n    \n    return results\n\ndef compare_remasking_strategies():\n    \"\"\"\n    å†ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥ã®æ¯”è¼ƒå®Ÿé¨“\n    \"\"\"\n    prompt = \"Describe the process of making coffee step by step.\"\n    \n    print(\"\\\\nğŸ”„ å†ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥æ¯”è¼ƒ\")\n    print(\"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:\", prompt)\n    print(\"=\" * 60)\n    \n    strategies = ['low_confidence', 'random']\n    results = {}\n    \n    for strategy in strategies:\n        print(f\"\\\\nğŸ¯ æˆ¦ç•¥: {strategy}\")\n        \n        # åŒã˜è¨­å®šã§è¤‡æ•°å›å®Ÿè¡Œã—ã¦å®‰å®šæ€§ã‚’ç¢ºèª\n        runs = []\n        for run in range(3):\n            result, history = generate_llada(\n                model, prompt,\n                steps=20, gen_length=60, block_length=20,\n                temperature=0.0, cfg_scale=0.0, remasking=strategy\n            )\n            \n            avg_confidence = np.mean([h['avg_confidence'] for h in history])\n            runs.append({\n                'run': run + 1,\n                'result': result,\n                'avg_confidence': avg_confidence\n            })\n            \n            print(f\"  å®Ÿè¡Œ {run + 1}: ä¿¡é ¼åº¦ {avg_confidence:.3f}\")\n            print(f\"    çµæœ: {result[:60]}{'...' if len(result) > 60 else ''}\")\n        \n        results[strategy] = runs\n    \n    print(\"\\\\n\" + \"=\" * 60)\n    print(\"ğŸ“Š æˆ¦ç•¥æ¯”è¼ƒçµæœ:\")\n    for strategy, runs in results.items():\n        avg_conf = np.mean([r['avg_confidence'] for r in runs])\n        std_conf = np.std([r['avg_confidence'] for r in runs])\n        print(f\"{strategy}: å¹³å‡ä¿¡é ¼åº¦ {avg_conf:.3f} Â± {std_conf:.3f}\")\n    \n    return results\n\n# å‰µä½œå®Ÿé¨“ã‚’å®Ÿè¡Œ\ncreativity_results = creativity_experiment()\n\n# å†ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥æ¯”è¼ƒ\nremasking_results = compare_remasking_strategies()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æœ€çµ‚ãƒãƒ£ãƒ¬ãƒ³ã‚¸: ã‚ªãƒªã‚¸ãƒŠãƒ«å®Ÿé¨“"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# ã‚ãªãŸè‡ªèº«ã®å®Ÿé¨“ã‚’è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼\n\nprint(\"ğŸ¯ ãƒãƒ£ãƒ¬ãƒ³ã‚¸: ã‚ãªãŸè‡ªèº«ã®å®Ÿé¨“\")\nprint(\"=\" * 40)\nprint(\"å®Œå…¨ãªLLaDAã®æ©Ÿèƒ½ã‚’ä½¿ã£ã¦è‡ªç”±ã«å®Ÿé¨“ã—ã¦ã¿ã¦ãã ã•ã„:\")\nprint()\nprint(\"ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä¾‹:\")\nprint('- \"Explain quantum physics in simple terms.\"')\nprint('- \"Write a dialogue between two AIs.\"')\nprint('- \"Create a recipe for happiness.\"')\nprint('- \"Describe a day in the life of a photon.\"')\nprint()\nprint(\"âš™ï¸ åˆ©ç”¨å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:\")\nprint(\"- steps: 8~64 (æ¨å¥¨: 16, 24, 32)\")\nprint(\"- gen_length: 32~128 (æ¨å¥¨: 48, 64, 80)\")\nprint(\"- block_length: 8~gen_length (æ¨å¥¨: 16, 24, 32)\")\nprint(\"- temperature: 0.0~1.0 (æ¨å¥¨: 0.0, 0.3, 0.5)\")\nprint(\"- cfg_scale: 0.0~3.0 (æ¨å¥¨: 0.0, 1.0, 2.0)\")\nprint(\"- remasking: 'low_confidence' ã¾ãŸã¯ 'random'\")\nprint()\nprint(\"ğŸš€ æ–°æ©Ÿèƒ½:\")\nprint(\"âœ“ ã‚»ãƒŸè‡ªå·±å›å¸°ç”Ÿæˆï¼ˆãƒ–ãƒ­ãƒƒã‚¯å‡¦ç†ï¼‰\")\nprint(\"âœ“ åˆ†é¡å™¨ãƒ•ãƒªãƒ¼ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ï¼ˆCFGï¼‰\")\nprint(\"âœ“ å†ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥é¸æŠ\")\nprint(\"âœ“ è©³ç´°ãªç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹è¿½è·¡\")\nprint()\nprint(\"ä¾‹:\")\nprint('your_result, your_history = generate_llada(')\nprint('    model, \"ã‚ãªãŸã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\",')\nprint('    steps=24, gen_length=64, block_length=16,')\nprint('    temperature=0.2, cfg_scale=1.0, remasking=\"low_confidence\"')\nprint(')')\nprint('visualize_block_progression(your_history)')\nprint('analyze_confidence_over_steps(your_history)')\nprint()\nprint(\"ğŸ’¡ å®Ÿé¨“ã‚¢ã‚¤ãƒ‡ã‚¢:\")\nprint(\"1. CFGã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆ0.0, 1.0, 2.0ï¼‰ã®å½±éŸ¿ã‚’æ¯”è¼ƒ\")\nprint(\"2. ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºãŒç”Ÿæˆå“è³ªã«ä¸ãˆã‚‹å½±éŸ¿\")\nprint(\"3. å‰µä½œçš„ãªã‚¿ã‚¹ã‚¯ã§ã®æ¸©åº¦ã¨CFGã®çµ„ã¿åˆã‚ã›\")\nprint(\"4. ç§‘å­¦çš„èª¬æ˜vså‰µä½œçš„æ–‡ç« ã§ã®æœ€é©è¨­å®š\")\nprint(\"5. å†ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥ã®ä½¿ã„åˆ†ã‘\")\n\n# ã“ã“ã«ã‚ãªãŸã®å®Ÿé¨“ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ãã ã•ã„ï¼\n# ä¾‹:\n# my_prompt = \"Write a short poem about the beauty of mathematics.\"\n# my_result, my_history = generate_llada(\n#     model, my_prompt,\n#     steps=20, gen_length=60, block_length=20,\n#     temperature=0.3, cfg_scale=1.2, remasking=\"low_confidence\"\n# )\n# print(f\"çµæœ: {my_result}\")\n# visualize_block_progression(my_history[-5:])  # æœ€å¾Œã®5ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¡¨ç¤º"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ å­¦ç¿’ã®ã¾ã¨ã‚\n",
    "\n",
    "### LLaDAæ‹¡æ•£è¨€èªãƒ¢ãƒ‡ãƒ«ã§å­¦ã‚“ã é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ\n",
    "\n",
    "1. **æ‹¡æ•£ãƒ—ãƒ­ã‚»ã‚¹ã®ç†è§£**\n",
    "   - å…¨ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’[MASK]ã§åˆæœŸåŒ–\n",
    "   - ä¿¡é ¼åº¦ã«åŸºã¥ãæ®µéšçš„ãªæ˜ã‹ã—\n",
    "   - åŒæ–¹å‘ã®æ–‡è„ˆè€ƒæ…®\n",
    "\n",
    "2. **è‡ªå·±å›å¸°ã¨ã®é•ã„**\n",
    "   - ç”Ÿæˆæ–¹å‘: ä¸€æ–¹å‘ vs å…¨æ–¹å‘\n",
    "   - å“è³ªæ”¹å–„: ä¸€å› vs åå¾©çš„\n",
    "   - åˆ¶å¾¡æ€§: é™å®šçš„ vs æŸ”è»Ÿ\n",
    "\n",
    "3. **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿**\n",
    "   - ã‚¹ãƒ†ãƒƒãƒ—æ•°: å“è³ªã¨è¨ˆç®—æ™‚é–“ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•\n",
    "   - æ¸©åº¦: æ±ºå®šè«–çš„ vs å‰µé€ çš„\n",
    "   - ç”Ÿæˆé•·: å¿œç­”ã®è©³ç´°åº¦\n",
    "\n",
    "4. **å®Ÿç”¨çš„ãªæ´å¯Ÿ**\n",
    "   - æ•°å­¦çš„ãªå•é¡Œ: ä½æ¸©åº¦ã€å¤šã‚¹ãƒ†ãƒƒãƒ—ãŒåŠ¹æœçš„\n",
    "   - å‰µä½œçš„ãªã‚¿ã‚¹ã‚¯: ä¸­ç¨‹åº¦ã®æ¸©åº¦ã§å¤šæ§˜æ€§å‘ä¸Š\n",
    "   - çŸ­ã„å¿œç­”: å°‘ãªã„ã‚¹ãƒ†ãƒƒãƒ—ã§ã‚‚ååˆ†\n",
    "\n",
    "### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "\n",
    "- ä»–ã®æ‹¡æ•£è¨€èªãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒ\n",
    "- å°‚é–€åˆ†é‡ï¼ˆåŒ»ç™‚ã€æ³•å¾‹ã€æŠ€è¡“ï¼‰ã§ã®æ€§èƒ½è©•ä¾¡\n",
    "- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¿œç”¨ã®å¯èƒ½æ€§æ¢ç´¢\n",
    "- ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã®ç ”ç©¶\n",
    "\n",
    "**ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼** æ‹¡æ•£è¨€èªãƒ¢ãƒ‡ãƒ«ã®åŸºç¤ã‚’ç†è§£ã—ã€å®Ÿéš›ã«LLaDAã‚’ä½¿ã£ãŸå®Ÿé¨“ã‚’å®Œäº†ã—ã¾ã—ãŸã€‚ğŸ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}